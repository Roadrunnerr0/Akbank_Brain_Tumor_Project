{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13181798,"sourceType":"datasetVersion","datasetId":8353555},{"sourceId":591205,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":442265,"modelId":458804}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 1) Gerekli KÃ¼tÃ¼phanelerin Ä°Ã§e AktarÄ±lmasÄ±\n\nModelin eÄŸitimi ve deÄŸerlendirilmesi iÃ§in ihtiyaÃ§ duyulan temel kÃ¼tÃ¼phaneler projeye dahil edilmiÅŸtir:\n\nNumPy â†’ SayÄ±sal iÅŸlemler ve veri Ã¼zerinde hesaplamalar yapmak iÃ§in\n\nMatplotlib & Seaborn â†’ Grafiksel gÃ¶rselleÅŸtirmeler, Ã¶zellikle eÄŸitim sÃ¼reci ve confusion matrix iÃ§in\n\nTensorFlow & Keras â†’ Derin Ã¶ÄŸrenme modellerini kurmak, eÄŸitmek ve test etmek iÃ§in\n\nscikit-learn (train_test_split, classification_report, confusion_matrix) â†’ Veriyi eÄŸitim/test olarak ayÄ±rmak ve performans Ã¶lÃ§Ã¼mleri almak iÃ§in\n\ntime â†’ EÄŸitim sÃ¼resini hesaplamak iÃ§in\n\nwarnings â†’ Ã‡alÄ±ÅŸma esnasÄ±nda gereksiz uyarÄ± mesajlarÄ±nÄ± gizlemek iÃ§in\n\nEk olarak, donanÄ±m doÄŸrulamasÄ± yapÄ±lmÄ±ÅŸtÄ±r:\n\nKullanÄ±lan TensorFlow sÃ¼rÃ¼mÃ¼ ekrana yazdÄ±rÄ±lmÄ±ÅŸtÄ±r.\n\nGPUâ€™nun mevcut olup olmadÄ±ÄŸÄ± kontrol edilmiÅŸ ve sonuÃ§ olarak â€œEvetâ€ veya â€œHayÄ±râ€ ÅŸeklinde bilgi verilmiÅŸtir.\n\nBu kontroller sayesinde, ortamÄ±n doÄŸru yapÄ±landÄ±rÄ±ldÄ±ÄŸÄ± ve modelin GPU hÄ±zlandÄ±rmalÄ± Ã§alÄ±ÅŸÄ±p Ã§alÄ±ÅŸmadÄ±ÄŸÄ± netleÅŸtirilmiÅŸtir.","metadata":{}},{"cell_type":"code","source":"# Gerekli kÃ¼tÃ¼phaneleri iÃ§e aktaralÄ±m\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport time\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# GPU kullanÄ±labilirliÄŸini kontrol edelim\nprint(\"TensorFlow sÃ¼rÃ¼mÃ¼:\", tf.__version__)\nprint(\"GPU kullanÄ±labilir mi?\", \"Evet\" if tf.config.list_physical_devices('GPU') else \"HayÄ±r\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **2) **Veri YÃ¼kleme ve KeÅŸifsel Analiz****\n\nBu bÃ¶lÃ¼mde, Kaggle Brain Tumor MRI Dataset iÃ§erisindeki gÃ¶rÃ¼ntÃ¼ler iÅŸlenmiÅŸ, uygun formatta hazÄ±rlanmÄ±ÅŸ ve keÅŸifsel analiz yapÄ±lmÄ±ÅŸtÄ±r. SÃ¼reÃ§ ÅŸu adÄ±mlardan oluÅŸmaktadÄ±r:\n\nğŸ”¹ KlasÃ¶r ve Yol KontrolÃ¼\n\nKÃ¶k dizin: /kaggle/input/brain-tumor-mri-dataset\n\nos.listdir(...) ile klasÃ¶r yapÄ±sÄ± incelenmiÅŸ, ilgili alt klasÃ¶rlerin doÄŸru ÅŸekilde bulunduÄŸu doÄŸrulanmÄ±ÅŸtÄ±r.\n\nğŸ”¹ Veri YÃ¼kleme Fonksiyonu\n\nload_data_from_folders(base_path) fonksiyonu:\n\nAlt klasÃ¶rleri dolaÅŸarak .jpg / .jpeg / .png / .bmp uzantÄ±lÄ± dosyalarÄ± okur.\n\nGÃ¶rseller gri Ã¶lÃ§ekli (cv2.IMREAD_GRAYSCALE) olarak okunur ve 128Ã—128 boyutuna yeniden Ã¶lÃ§eklendirilir.\n\nFonksiyon Ã¼Ã§ Ã§Ä±ktÄ± Ã¼retir:\n\nX: GÃ¶rÃ¼ntÃ¼ tensÃ¶rleri\n\ny: SÄ±nÄ±f etiketleri\n\nclass_names: SÄ±nÄ±f isimleri\n\nGizli/sistem dosyalarÄ± (Ã¶rneÄŸin .DS_Store) filtrelenir.\n\nBaÅŸarÄ±sÄ±z okuma durumlarÄ±nda try/except ile hata mesajÄ± yakalanÄ±r.\n\nğŸ”¹ YÃ¼klemenin DoÄŸrulanmasÄ±\n\nVeri yÃ¼klendikten sonra:\n\nToplam gÃ¶rÃ¼ntÃ¼ ve etiket sayÄ±sÄ± ekrana yazdÄ±rÄ±lÄ±r.\n\nSÄ±nÄ±f isimleri listelenir.\n\nEÄŸer hiÃ§ gÃ¶rÃ¼ntÃ¼ yÃ¼klenmediyse, kullanÄ±cÄ±ya uyarÄ± verilir.\n\nğŸ”¹ EÄŸitim/Test AyrÄ±mÄ±\n\ntrain_test_split kullanÄ±larak veriler %80 eÄŸitim â€“ %20 test olacak ÅŸekilde ayrÄ±lmÄ±ÅŸtÄ±r.\n\nstratify=y parametresi ile sÄ±nÄ±f daÄŸÄ±lÄ±mÄ± her iki sette de korunmuÅŸtur.\n\nAyrÄ±m sonrasÄ±:\n\nX_train ve X_test boyutlarÄ±\n\ny_train ve y_test boyutlarÄ± ekrana yazdÄ±rÄ±lmÄ±ÅŸtÄ±r.\n\nğŸ”¹ Ã–rnek GÃ¶rsellerin Ä°ncelenmesi\n\nEÄŸitim setinden ilk 25 gÃ¶rÃ¼ntÃ¼ 5Ã—5â€™lik bir grid halinde gÃ¶rselleÅŸtirilmiÅŸtir.\n\nAlt etiketlerde, ilgili sÄ±nÄ±f adÄ± (class_names[y_train[i]]) gÃ¶sterilmiÅŸtir.\n\nGÃ¶rseller gri tonlama (cmap='gray') ile sunulmuÅŸtur.\n\nğŸ”¹ SÄ±nÄ±f DaÄŸÄ±lÄ±mÄ± Analizi\n\nEÄŸitim ve test setleri iÃ§in countplot grafiklerinde sÄ±nÄ±f daÄŸÄ±lÄ±mlarÄ± gÃ¶sterilmiÅŸtir.\n\nX ekseninde sÄ±nÄ±f indeksleri, etiketlerde ise class_names yer alÄ±r. (Etiketler okunabilirlik iÃ§in 45Â° dÃ¶ndÃ¼rÃ¼lmÃ¼ÅŸtÃ¼r.)\n\nBu adÄ±m sayesinde, sÄ±nÄ±flar arasÄ±nda dengesizlik olup olmadÄ±ÄŸÄ± kolayca gÃ¶zlemlenebilir. (Gerekirse class_weight kullanÄ±labilir.)\n\nğŸ”¹ Ek Notlar\n\nGÃ¶rseller gri Ã¶lÃ§ekli iÅŸlendiÄŸi iÃ§in, model giriÅŸ boyutu (128, 128, 1) olmalÄ±dÄ±r.\n\nEÄŸitim Ã¶ncesi normalizasyon (0â€“255 â†’ 0â€“1) Ã¶nerilmektedir.\n\nVeri artÄ±rma iÃ§in:\n\nImageDataGenerator\n\nveya tf.keras.layers tabanlÄ± augmentation katmanlarÄ± kullanÄ±labilir.\n\nBÃ¼yÃ¼k veri setlerinde bellek optimizasyonu iÃ§in generator veya tf.data pipeline kullanÄ±mÄ± tercih edilebilir.\n\nâœ… Bu aÅŸamalar tamamlandÄ±ÄŸÄ±nda, veri kÃ¼mesi eÄŸitim iÃ§in hazÄ±r hale getirilmiÅŸ, hem Ã¶rnek gÃ¶rseller hem de sÄ±nÄ±f daÄŸÄ±lÄ±mlarÄ± gÃ¶rsel olarak doÄŸrulanmÄ±ÅŸtÄ±r.","metadata":{}},{"cell_type":"code","source":"import os\nprint(os.listdir('/kaggle/input/brain-tumor-mri-dataset'))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport cv2\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical\n\n# Veri setinin ana klasÃ¶r yolu\ndataset_base_path = '/kaggle/input/brain-tumor-mri-dataset'\nprint(f\"BaÅŸlangÄ±Ã§ klasÃ¶r yolu: {dataset_base_path}\")\n\ndef load_data_from_folders(base_path):\n    \"\"\"Belirtilen klasÃ¶r yolundaki tÃ¼m gÃ¶rÃ¼ntÃ¼leri sÄ±nÄ±flarÄ±na gÃ¶re yÃ¼kler.\"\"\"\n    X = []\n    y = []\n    class_names = []\n    IMG_SIZE = 128\n    \n    print(\"\\nVeri seti yÃ¼kleniyor...\")\n    \n    # KÃ¶k klasÃ¶rden baÅŸlayarak tÃ¼m dosya aÄŸacÄ±nÄ± gezelim\n    for dirpath, dirnames, filenames in os.walk(base_path):\n        # EÄŸer klasÃ¶rde \".DS_Store\" gibi sistem dosyalarÄ± varsa dikkate almayalÄ±m\n        filenames = [f for f in filenames if not f.startswith('.')]\n        \n        # EÄŸer bir gÃ¶rÃ¼ntÃ¼ dosyasÄ± varsa, bu klasÃ¶r bir sÄ±nÄ±f klasÃ¶rÃ¼dÃ¼r\n        if any(f.endswith(('.jpg', '.jpeg', '.png', '.bmp')) for f in filenames):\n            class_name = os.path.basename(dirpath)\n            if class_name not in class_names:\n                class_names.append(class_name)\n            \n            class_index = class_names.index(class_name)\n            \n            print(f\"'{class_name}' sÄ±nÄ±fÄ± iÃ§in {len(filenames)} gÃ¶rÃ¼ntÃ¼ bulunuyor.\")\n            \n            for image_name in filenames:\n                image_path = os.path.join(dirpath, image_name)\n                try:\n                    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n                    if image is not None:\n                        image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n                        X.append(image)\n                        y.append(class_index)\n                except Exception as e:\n                    print(f\"Hata: {image_path} gÃ¶rÃ¼ntÃ¼sÃ¼ yÃ¼klenemedi. Hata: {e}\")\n\n    return np.array(X), np.array(y), class_names\n\n# Veriyi yÃ¼kleyelim\nX, y, class_names = load_data_from_folders(dataset_base_path)\n\n# Hata kontrolÃ¼\nif X.shape[0] == 0:\n    print(\"\\n--- HATA: HiÃ§ gÃ¶rÃ¼ntÃ¼ yÃ¼klenemedi. Dosya yapÄ±sÄ±nÄ± veya yolu kontrol edin. ---\")\nelse:\n    print(\"\\nVeri yÃ¼klemesi baÅŸarÄ±lÄ±!\")\n    print(f\"Toplam gÃ¶rÃ¼ntÃ¼ sayÄ±sÄ±: {len(X)}\")\n    print(f\"Toplam etiket sayÄ±sÄ±: {len(y)}\")\n    print(f\"SÄ±nÄ±f isimleri: {class_names}\")\n\n    # Veriyi eÄŸitim ve test setlerine ayÄ±ralÄ±m\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n    \n    print(\"\\nEÄŸitim verisi ÅŸekli:\", X_train.shape)\n    print(\"EÄŸitim etiketi ÅŸekli:\", y_train.shape)\n    print(\"Test verisi ÅŸekli:\", X_test.shape)\n    print(\"Test etiketi ÅŸekli:\", y_test.shape)\n\n    # Ä°lk 25 gÃ¶rÃ¼ntÃ¼yÃ¼ gÃ¶rselleÅŸtirelim\n    plt.figure(figsize=(10, 10))\n    for i in range(25):\n        plt.subplot(5, 5, i+1)\n        plt.xticks([])\n        plt.yticks([])\n        plt.grid(False)\n        plt.imshow(X_train[i], cmap='gray')\n        plt.xlabel(class_names[y_train[i]])\n    plt.tight_layout()\n    plt.show()\n\n    # SÄ±nÄ±f daÄŸÄ±lÄ±mÄ±nÄ± inceleyelim\n    plt.figure(figsize=(10, 5))\n    plt.subplot(1, 2, 1)\n    sns.countplot(x=y_train)\n    plt.title('EÄŸitim Verisi SÄ±nÄ±f DaÄŸÄ±lÄ±mÄ±')\n    plt.xlabel('SÄ±nÄ±f')\n    plt.ylabel('SayÄ±')\n    plt.xticks(np.arange(len(class_names)), class_names, rotation=45)\n\n    plt.subplot(1, 2, 2)\n    sns.countplot(x=y_test)\n    plt.title('Test Verisi SÄ±nÄ±f DaÄŸÄ±lÄ±mÄ±')\n    plt.xlabel('SÄ±nÄ±f')\n    plt.ylabel('SayÄ±')\n    plt.xticks(np.arange(len(class_names)), class_names, rotation=45)\n    \n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T17:10:51.565061Z","iopub.execute_input":"2025-09-26T17:10:51.565522Z","iopub.status.idle":"2025-09-26T17:11:06.494306Z","shell.execute_reply.started":"2025-09-26T17:10:51.565484Z","shell.execute_reply":"2025-09-26T17:11:06.493343Z"}},"outputs":[{"name":"stdout","text":"BaÅŸlangÄ±Ã§ klasÃ¶r yolu: /kaggle/input/brain-tumor-mri-dataset\n\nVeri seti yÃ¼kleniyor...\n\n--- HATA: HiÃ§ gÃ¶rÃ¼ntÃ¼ yÃ¼klenemedi. Dosya yapÄ±sÄ±nÄ± veya yolu kontrol edin. ---\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# **2) 2.1 Veri Ã–n Ä°ÅŸleme**\n\nBu adÄ±mda giriÅŸ verileri Ã¶lÃ§eklendirildi, etiketler uygun formata Ã§evrildi ve eÄŸitim verisi iÃ§in veri artÄ±rma stratejileri tanÄ±mlandÄ±.\n\nNormalizasyon (0â€“255 â†’ 0â€“1)\n\nX_train ve X_test tensÃ¶rleri float32 tipe Ã§evrilip 255â€™e bÃ¶lÃ¼ndÃ¼.\n\nAmaÃ§: SayÄ±sal kararlÄ±lÄ±k ve daha hÄ±zlÄ± yakÄ±nsama.\n\nOne-Hot Encoding (Etiket DÃ¶nÃ¼ÅŸÃ¼mÃ¼)\n\ny_train ve y_test etiketleri one-hot vektÃ¶rlere Ã§evrildi:\n\nkeras.utils.to_categorical(y, num_classes)\n\nNot: num_classes deÄŸeri veri kÃ¼mendeki gerÃ§ek sÄ±nÄ±f sayÄ±sÄ± ile uyumlu olmalÄ±dÄ±r (Ã¶r. 4 ise 10 deÄŸil 4 kullanÄ±lmalÄ±).\n\nVeri ArtÄ±rma (ImageDataGenerator)\n\nEÄŸitim verisine yÃ¶nelik dÃ¶nÃ¼ÅŸÃ¼mler tanÄ±mlandÄ±:\nDÃ¶ndÃ¼rme: rotation_range=15\n\nKaydÄ±rma: width_shift_range=0.1, height_shift_range=0.1\n\nAyna Ã§evirme: horizontal_flip=True\n\nYakÄ±nlaÅŸtÄ±rma: zoom_range=0.1\n\ndatagen.fit(X_train) ile (gerekli ise) istatistikler hazÄ±rlandÄ±.\n\nEÄŸitimde KullanÄ±m (Ã–nemli)\n\nTanÄ±mlanan artÄ±rma, model.fit sÄ±rasÄ±nda uygulanÄ±r:\nmodel.fit(datagen.flow(X_train, y_train_categorical, batch_size=...), validation_data=(X_test, y_test_categorical), ...)\n\nDoÄŸrulama/test tarafÄ±nda ham veriler kullanÄ±lÄ±r; artÄ±rma sadece eÄŸitim verisine uygulanÄ±r.\n\nÅekil (Shape) KontrolÃ¼\n\nNormalizasyon ve one-hot sonrasÄ± boyutlar basÄ±ldÄ±:\nX_train.shape â†’ giriÅŸ tensor ÅŸekli\n\ny_train_categorical.shape â†’ (num_samples, num_classes)\n\nBu hazÄ±rlÄ±klarla model, Ã¶lÃ§eklendirilmiÅŸ ve artÄ±rÄ±lmÄ±ÅŸ eÄŸitim verisi Ã¼zerinde eÄŸitilecek; genelleme kabiliyeti artÄ±rÄ±lÄ±rken aÅŸÄ±rÄ± Ã¶ÄŸrenme (overfitting) riski azaltÄ±lacaktÄ±r.","metadata":{}},{"cell_type":"code","source":"# Veriyi normalize edelim (0-255 arasÄ± deÄŸerleri 0-1 arasÄ±na dÃ¶nÃ¼ÅŸtÃ¼relim)\nX_train = X_train.astype('float32') / 255.0\nX_test = X_test.astype('float32') / 255.0\n\n# Etiketleri one-hot encoding formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼relim\ny_train_categorical = keras.utils.to_categorical(y_train, 10)\ny_test_categorical = keras.utils.to_categorical(y_test, 10)\n\nX_train = np.expand_dims(X_train, axis=-1) # (5618, 128, 128, 1)\nX_test = np.expand_dims(X_test, axis=-1) # (test_sayÄ±sÄ±, 128, 128, 1)\n\n# Veri artÄ±rma iÃ§in ImageDataGenerator kullanalÄ±m\ndatagen = keras.preprocessing.image.ImageDataGenerator(\n    rotation_range=15,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    horizontal_flip=True,\n    zoom_range=0.1\n)\n\n# Veri artÄ±rma iÅŸlemini eÄŸitim verisine uygulayalÄ±m\ndatagen.fit(X_train)\n\n# Normalizasyon ve one-hot encoding sonrasÄ± veri boyutlarÄ±nÄ± kontrol edelim\nprint(\"Normalize edilmiÅŸ eÄŸitim verisi ÅŸekli:\", X_train.shape)\nprint(\"One-hot encoded eÄŸitim etiketleri ÅŸekli:\", y_train_categorical.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ****3) Temel CNN Modelinin OluÅŸturulmasÄ±****\nBu adÄ±mda, gÃ¶rÃ¼ntÃ¼ sÄ±nÄ±flandÄ±rma problemi iÃ§in temel bir KonvolÃ¼syonel Sinir AÄŸÄ± (CNN) modeli tanÄ±mlanmÄ±ÅŸtÄ±r. Model, evriÅŸim (convolution) katmanlarÄ± ile gÃ¶rsel Ã¶zellikleri Ã§Ä±karmakta, ardÄ±ndan tam baÄŸlantÄ±lÄ± katmanlarla sÄ±nÄ±flandÄ±rma yapmaktadÄ±r.\n\nModel Mimarisi\nÄ°lk EvriÅŸim BloÄŸu\n\n2 adet Conv2D katmanÄ± (32 filtre, 3Ã—3 kernel, ReLU aktivasyonu, padding='same')\n\nBatch Normalization ile eÄŸitim stabilitesi artÄ±rÄ±lmÄ±ÅŸtÄ±r.\n\nMaxPooling2D (2Ã—2) ile boyut azaltma.\n\nDropout(0.2) ile aÅŸÄ±rÄ± Ã¶ÄŸrenme (overfitting) Ã¶nlenmiÅŸtir.\n\nÄ°kinci EvriÅŸim BloÄŸu\n2 adet Conv2D katmanÄ± (64 filtre).\n\nBatch Normalization + ReLU aktivasyonu.\n\nMaxPooling2D ve Dropout(0.3).\n\nÃœÃ§Ã¼ncÃ¼ EvriÅŸim BloÄŸu\n2 adet Conv2D katmanÄ± (128 filtre).\n\nBatch Normalization + ReLU aktivasyonu.\n\nMaxPooling2D ve Dropout(0.4).\n\nSÄ±nÄ±flandÄ±rÄ±cÄ± Katmanlar\nFlatten ile Ã¶zellik haritalarÄ± vektÃ¶re dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼r.\n\nDense(128, ReLU) â†’ Tam baÄŸlantÄ±lÄ± gizli katman.\n\nBatch Normalization + Dropout(0.5).\n\nÃ‡Ä±kÄ±ÅŸ katmanÄ±: Dense(10, Softmax) â†’ 10 sÄ±nÄ±f iÃ§in olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ±.\n\nModel Ã–zeti\ncnn_model.summary() Ã§Ä±ktÄ±sÄ± ile katmanlarÄ±n yapÄ±sÄ±, Ã§Ä±ktÄ± boyutlarÄ± ve parametre sayÄ±larÄ± incelenmiÅŸtir.\n\nBu CNN modeli, gÃ¶rÃ¼ntÃ¼lerdeki yerel Ã¶zellikleri evriÅŸim bloklarÄ± ile Ã¶ÄŸrenip, tam baÄŸlantÄ±lÄ± katmanlarla sÄ±nÄ±f tahminleri yapacak ÅŸekilde tasarlanmÄ±ÅŸtÄ±r.","metadata":{}},{"cell_type":"code","source":"#Temel CNN modeli\ndef create_cnn_model():\n    model = keras.Sequential([\n        # Ä°lk evriÅŸim bloÄŸu\n        layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)),\n        layers.BatchNormalization(),\n        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n        layers.MaxPooling2D((2, 2)),\n        layers.Dropout(0.2),\n\n        # Ä°kinci evriÅŸim bloÄŸu\n        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n        layers.BatchNormalization(),\n        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n        layers.MaxPooling2D((2, 2)),\n        layers.Dropout(0.3),\n\n        # ÃœÃ§Ã¼ncÃ¼ evriÅŸim bloÄŸu\n        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n        layers.BatchNormalization(),\n        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n        layers.MaxPooling2D((2, 2)),\n        layers.Dropout(0.4),\n\n        # SÄ±nÄ±flandÄ±rÄ±cÄ±\n        layers.Flatten(),\n        layers.Dense(128, activation='relu'),\n        layers.BatchNormalization(),\n        layers.Dropout(0.5),\n        layers.Dense(10, activation='softmax')\n    ])\n    return model\n\n# Modeli oluÅŸturalÄ±m\ncnn_model = create_cnn_model()\n\n# Model Ã¶zetini gÃ¶rÃ¼ntÃ¼leyelim\ncnn_model.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"â”ƒ Layer (type)                    â”ƒ Output Shape           â”ƒ       Param # â”ƒ\n\nâ”‚ conv2d (Conv2D)                 â”‚ (None, 32, 32, 32)     â”‚           896 â”‚\n\nâ”‚ batch_normalization             â”‚ (None, 32, 32, 32)     â”‚           128 â”‚\nâ”‚ (BatchNormalization)            â”‚                        â”‚               â”‚\n\nâ”‚ conv2d_1 (Conv2D)               â”‚ (None, 32, 32, 32)     â”‚         9,248 â”‚\n\nâ”‚ max_pooling2d (MaxPooling2D)    â”‚ (None, 16, 16, 32)     â”‚             0 â”‚\n\nâ”‚ dropout (Dropout)               â”‚ (None, 16, 16, 32)     â”‚             0 â”‚\n\nâ”‚ conv2d_2 (Conv2D)               â”‚ (None, 16, 16, 64)     â”‚        18,496 â”‚\n\nâ”‚ batch_normalization_1           â”‚ (None, 16, 16, 64)     â”‚           256 â”‚\nâ”‚ (BatchNormalization)            â”‚                        â”‚               â”‚\n\nâ”‚ conv2d_3 (Conv2D)               â”‚ (None, 16, 16, 64)     â”‚        36,928 â”‚\n\nâ”‚ max_pooling2d_1 (MaxPooling2D)  â”‚ (None, 8, 8, 64)       â”‚             0 â”‚\n\nâ”‚ dropout_1 (Dropout)             â”‚ (None, 8, 8, 64)       â”‚             0 â”‚\nâ”‚ conv2d_4 (Conv2D)               â”‚ (None, 8, 8, 128)      â”‚        73,856 â”‚\n\nâ”‚ batch_normalization_2           â”‚ (None, 8, 8, 128)      â”‚           512 â”‚\nâ”‚ (BatchNormalization)            â”‚                        â”‚               â”‚\n\nâ”‚ conv2d_5 (Conv2D)               â”‚ (None, 8, 8, 128)      â”‚       147,584 â”‚\n\nâ”‚ max_pooling2d_2 (MaxPooling2D)  â”‚ (None, 4, 4, 128)      â”‚             0 â”‚\n\nâ”‚ dropout_2 (Dropout)             â”‚ (None, 4, 4, 128)      â”‚             0 â”‚\n\nâ”‚ flatten (Flatten)               â”‚ (None, 2048)           â”‚             0 â”‚\n\nâ”‚ dense (Dense)                   â”‚ (None, 128)            â”‚       262,272 â”‚\n\nâ”‚ batch_normalization_3           â”‚ (None, 128)            â”‚           512 â”‚\nâ”‚ (BatchNormalization)            â”‚                        â”‚               â”‚\n\nâ”‚ dropout_3 (Dropout)             â”‚ (None, 128)            â”‚             0 â”‚\n\nâ”‚ dense_1 (Dense)                 â”‚ (None, 10)             â”‚         1,290 â”‚","metadata":{}},{"cell_type":"markdown","source":"# **4) Modeli Derleme ve EÄŸitme**\nBu adÄ±mda veriler hazÄ±rlanmÄ±ÅŸ, CNN modeli derlenmiÅŸ ve artÄ±rÄ±lmÄ±ÅŸ eÄŸitim verisi Ã¼zerinde eÄŸitilmiÅŸtir. EÄŸitim sÃ¼reci sonunda accuracy ve loss eÄŸrileri gÃ¶rselleÅŸtirilmiÅŸtir.\n\n* **Girdi HazÄ±rlÄ±ÄŸÄ±**\n* Normalizasyon: X_train ve X_test tensÃ¶rleri float32 tipine dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼p 0â€“1 aralÄ±ÄŸÄ±na Ã¶lÃ§eklenmiÅŸtir.\n* Kanal Boyutu: Gri tonlamalÄ± gÃ¶rÃ¼ntÃ¼ler (H, W) formatÄ±ndan (H, W, 1) formatÄ±na getirilmiÅŸtir.\n* SÄ±nÄ±f SayÄ±sÄ±: class_names listesinden veya maksimum etiket deÄŸerine gÃ¶re otomatik hesaplanmÄ±ÅŸtÄ±r.\n* Etiket DÃ¶nÃ¼ÅŸÃ¼mÃ¼: y_train ve y_test deÄŸerleri to_categorical kullanÄ±larak one-hot encoding formatÄ±na Ã§evrilmiÅŸtir.\nVeri ArtÄ±rma (Augmentation)\nImageDataGenerator kullanÄ±lmÄ±ÅŸtÄ±r.\n* **Parametreler:**\n* DÃ¶ndÃ¼rme: rotation_range=15\n* KaydÄ±rma: width_shift_range=0.10, height_shift_range=0.10\n* YakÄ±nlaÅŸtÄ±rma: zoom_range=0.10\n* Yatay Ã§evirme: horizontal_flip=True\n* **CNN Modeli**\n* Conv2D KatmanlarÄ±: 32, 64 ve 128 filtreli Ã¼Ã§ evriÅŸim bloÄŸu.\n* Batch Normalization: Her bloÄŸun ardÄ±ndan eÄŸitim stabilitesi iÃ§in eklenmiÅŸtir.\n* Dropout: AÅŸÄ±rÄ± Ã¶ÄŸrenmeyi engellemek iÃ§in 0.2 â†’ 0.5 oranlarÄ±yla kullanÄ±lmÄ±ÅŸtÄ±r.\n* Dense Katman: 128 nÃ¶ronlu gizli katman (ReLU) ve softmax Ã§Ä±kÄ±ÅŸ katmanÄ±.\n* **Derleme (Compile)**\n* Optimizer: Adam\n* Loss Fonksiyonu: Categorical Crossentropy\n* Metrikler: Accuracy\n* **Callbackâ€™ler**\n* EarlyStopping: val_loss geliÅŸmezse eÄŸitimi durdurur (patience=8).\n* ReduceLROnPlateau: Ã–ÄŸrenme oranÄ±nÄ± dinamik olarak dÃ¼ÅŸÃ¼rÃ¼r (patience=3, factor=0.5).\n* **EÄŸitim**\n* Model, 25 epoch boyunca eÄŸitim verisi Ã¼zerinde eÄŸitilmiÅŸtir.\n* datagen.flow(...) ile veri artÄ±rma uygulanarak batch halinde beslenmiÅŸtir.\n* DoÄŸrulama seti: (X_test, y_test_categorical).\n* **GÃ¶rselleÅŸtirme**\n* EÄŸitim ve doÄŸrulama iÃ§in Accuracy ve Loss grafiklerinin epoch bazÄ±nda deÄŸiÅŸimi Ã§izilmiÅŸtir.\n* Grafikler, modelin Ã¶ÄŸrenme sÃ¼reci ve overfitting/underfitting eÄŸilimlerini gÃ¶zlemlemek iÃ§in kullanÄ±lmÄ±ÅŸtÄ±r.","metadata":{}},{"cell_type":"code","source":"# === Compile + Train + Plot  ===\nimport numpy as np, matplotlib.pyplot as plt\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n# 1) X ve y zaten var (X_train, X_test, y_train, y_test)\n\nX_train = X_train.astype('float32')\nX_test  = X_test.astype('float32')\nif X_train.max() > 1.5:  # 0-255 ise\n    X_train /= 255.0\n    X_test  /= 255.0\n\n# Kanal eksikse ekle (grayscale -> (H,W,1))\nif X_train.ndim == 3:  # (N,H,W)\n    X_train = np.expand_dims(X_train, -1)\n    X_test  = np.expand_dims(X_test, -1)\n\ninput_shape = X_train.shape[1:]  # (128,128,1) bekliyoruz\n\n# SÄ±nÄ±f sayÄ±sÄ±nÄ± doÄŸru hesapla \nnum_classes = len(class_names) if 'class_names' in globals() and len(class_names) > 0 \\\n              else int(np.max(y_train)) + 1\n\n# Etiketleri one-hot'a doÄŸru boyutla Ã§evir\ny_train_categorical = keras.utils.to_categorical(y_train, num_classes)\ny_test_categorical  = keras.utils.to_categorical(y_test,  num_classes)\n\n# 2) Augmentation (array tabanlÄ±)\ndatagen = keras.preprocessing.image.ImageDataGenerator(\n    rotation_range=15,\n    width_shift_range=0.10,\n    height_shift_range=0.10,\n    zoom_range=0.10,\n    horizontal_flip=True\n)\ndatagen.fit(X_train)\n\n# 3) Model: input_shape ve sÄ±nÄ±f sayÄ±sÄ±na gÃ¶re gÃ¼ncelledik\ndef create_cnn_model(input_shape, num_classes):\n    model = keras.Sequential([\n        layers.Input(shape=input_shape),                 # (128,128,1) ya da (128,128,3)\n        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n        layers.BatchNormalization(),\n        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n        layers.MaxPooling2D((2, 2)),\n        layers.Dropout(0.2),\n\n        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n        layers.BatchNormalization(),\n        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n        layers.MaxPooling2D((2, 2)),\n        layers.Dropout(0.3),\n\n        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n        layers.BatchNormalization(),\n        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n        layers.MaxPooling2D((2, 2)),\n        layers.Dropout(0.4),\n\n        layers.Flatten(),\n        layers.Dense(128, activation='relu'),\n        layers.BatchNormalization(),\n        layers.Dropout(0.5),\n        layers.Dense(num_classes, activation='softmax')  # <-- doÄŸru sÄ±nÄ±f sayÄ±sÄ±\n    ])\n    return model\n\ncnn_model = create_cnn_model(input_shape, num_classes)\n\ncnn_model.compile(\n    optimizer='adam',\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\n# 4) Callback'ler\nearly_stopping = keras.callbacks.EarlyStopping(\n    monitor='val_loss', patience=8, restore_best_weights=True\n)\nreduce_lr = keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss', factor=0.5, patience=3, min_lr=1e-5, verbose=1\n)\n\n# 5) EÄŸitim\nhistory = cnn_model.fit(\n    datagen.flow(X_train, y_train_categorical, batch_size=32),\n    epochs=25,\n    validation_data=(X_test, y_test_categorical),\n    callbacks=[early_stopping, reduce_lr],\n    verbose=1\n)\n\n# 6) Accuracy & Loss grafikleri\nhist = history.history\nplt.figure(figsize=(12,5))\n\nplt.subplot(1,2,1)\nplt.plot(hist.get('accuracy', []), label='EÄŸitim DoÄŸruluÄŸu')\nplt.plot(hist.get('val_accuracy', []), label='DoÄŸrulama DoÄŸruluÄŸu')\nplt.title('Accuracy'); plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.legend(); plt.grid(True)\n\nplt.subplot(1,2,2)\nplt.plot(hist.get('loss', []), label='EÄŸitim KaybÄ±')\nplt.plot(hist.get('val_loss', []), label='DoÄŸrulama KaybÄ±')\nplt.title('Loss'); plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend(); plt.grid(True)\n\nplt.tight_layout(); plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"  3/176 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 11s 65ms/step - accuracy: 0.2552 - loss: 2.3604\nI0000 00:00:1758841953.110179      63 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n176/176 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 33s 105ms/step - accuracy: 0.5747 - loss: 1.1343 - val_accuracy: 0.2505 - val_loss: 3.5619 - learning_rate: 0.0010\nEpoch 2/25\n\n176/176 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 8s 45ms/step - accuracy: 0.7247 - loss: 0.7236 - val_accuracy: 0.3423 - val_loss: 1.9202 - learning_rate: 0.0010\nEpoch 3/25\n\n176/176 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 8s 45ms/step - accuracy: 0.7858 - loss: 0.5716 - val_accuracy: 0.2932 - val_loss: 2.8610 - learning_rate: 0.0010\nEpoch 4/25\n\n176/176 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 8s 46ms/step - accuracy: 0.8019 - loss: 0.5162 - val_accuracy: 0.5900 - val_loss: 1.6119 - learning_rate: 0.0010\nEpoch 5/25\n\n176/176 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 8s 46ms/step - accuracy: 0.8394 - loss: 0.4224 - val_accuracy: 0.8655 - val_loss: 0.3906 - learning_rate: 0.0010\nEpoch 6/25\n\n176/176 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 8s 45ms/step - accuracy: 0.8784 - loss: 0.3564 - val_accuracy: 0.7879 - val_loss: 0.6130 - learning_rate: 0.0010\nEpoch 7/25\n\n176/176 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 8s 45ms/step - accuracy: 0.8776 - loss: 0.3352 - val_accuracy: 0.8940 - val_loss: 0.2811 - learning_rate: 0.0010\nEpoch 8/25\n\n176/176 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 8s 45ms/step - accuracy: 0.8986 - loss: 0.2898 - val_accuracy: 0.8448 - val_loss: 0.4500 - learning_rate: 0.0010\nEpoch 9/25\n\n176/176 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 8s 45ms/step - accuracy: 0.9023 - loss: 0.2682 - val_accuracy: 0.7886 - val_loss: 0.6163 - learning_rate: 0.0010\nEpoch 10/25\n\n175/176 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 43ms/step - accuracy: 0.9175 - loss: 0.2444\nEpoch 10: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n\n176/176 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 8s 46ms/step - accuracy: 0.9174 - loss: 0.2445 - val_accuracy: 0.7730 - val_loss: 0.7700 - learning_rate: 0.0010\nEpoch 11/25\n\n176/176 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 8s 46ms/step - accuracy: 0.9203 - loss: 0.2280 - val_accuracy: 0.9281 - val_loss: 0.1967 - learning_rate: 5.0000e-04\nEpoch 12/25\n\n176/176 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 8s 45ms/step - accuracy: 0.9359 - loss: 0.1835 - val_accuracy: 0.9438 - val_loss: 0.1712 - learning_rate: 5.0000e-04\nEpoch 13/25\n\n176/176 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 8s 45ms/step - accuracy: 0.9305 - loss: 0.1916 - val_accuracy: 0.9310 - val_loss: 0.1867 - learning_rate: 5.0000e-04\nEpoch 14/25\n\n176/176 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 8s 47ms/step - accuracy: 0.9459 - loss: 0.1691 - val_accuracy: 0.8085 - val_loss: 0.6087 - learning_rate: 5.0000e-04\nEpoch 15/25\n\n175/176 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 44ms/step - accuracy: 0.9428 - loss: 0.1746\nEpoch 15: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n\n176/176 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 8s 47ms/step - accuracy: 0.9428 - loss: 0.1747 - val_accuracy: 0.9359 - val_loss: 0.2103 - learning_rate: 5.0000e-04\nEpoch 16/25\n\n176/176 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 8s 46ms/step - accuracy: 0.9428 - loss: 0.1593 - val_accuracy: 0.9367 - val_loss: 0.1727 - learning_rate: 2.5000e-04\nEpoch 17/25\n\n176/176 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 8s 47ms/step - accuracy: 0.9478 - loss: 0.1488 - val_accuracy: 0.9025 - val_loss: 0.3040 - learning_rate: 2.5000e-04\nEpoch 18/25\n\n176/176 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 8s 47ms/step - accuracy: 0.9549 - loss: 0.1314 - val_accuracy: 0.9438 - val_loss: 0.1613 - learning_rate: 2.5000e-04\nEpoch 19/25\n\n176/176 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 8s 47ms/step - accuracy: 0.9498 - loss: 0.1354 - val_accuracy: 0.9317 - val_loss: 0.2010 - learning_rate: 2.5000e-04\nEpoch 20/25\n\n176/176 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 8s 47ms/step - accuracy: 0.9509 - loss: 0.1404 - val_accuracy: 0.9374 - val_loss: 0.1807 - learning_rate: 2.5000e-04\nEpoch 21/25\n\n176/176 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 8s 48ms/step - accuracy: 0.9553 - loss: 0.1281 - val_accuracy: 0.9445 - val_loss: 0.1544 - learning_rate: 2.5000e-04\nEpoch 22/25\n\n176/176 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 8s 47ms/step - accuracy: 0.9549 - loss: 0.1245 - val_accuracy: 0.9011 - val_loss: 0.3087 - learning_rate: 2.5000e-04\nEpoch 23/25\n\n176/176 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 8s 47ms/step - accuracy: 0.9545 - loss: 0.1274 - val_accuracy: 0.8719 - val_loss: 0.3412 - learning_rate: 2.5000e-04\nEpoch 24/25\n\n175/176 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 45ms/step - accuracy: 0.9600 - loss: 0.1157\nEpoch 24: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n\n176/176 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 8s 48ms/step - accuracy: 0.9600 - loss: 0.1157 - val_accuracy: 0.8819 - val_loss: 0.3936 - learning_rate: 2.5000e-04\nEpoch 25/25\n\n176/176 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 8s 47ms/step - accuracy: 0.9620 - loss: 0.1083 - val_accuracy: 0.9345 - val_loss: 0.1687 - learning_rate: 1.2500e-04\n","metadata":{}},{"cell_type":"markdown","source":"# **5) Modeli DeÄŸerlendirmesi**\nEÄŸitim tamamlandÄ±ktan sonra modelin test seti Ã¼zerindeki performansÄ± deÄŸerlendirilmiÅŸtir. Bu aÅŸamada accuracy, classification report ve confusion matrix gibi metrikler kullanÄ±lmÄ±ÅŸ, ayrÄ±ca bazÄ± doÄŸru ve yanlÄ±ÅŸ tahmin Ã¶rnekleri gÃ¶rselleÅŸtirilmiÅŸtir.\n\n1) Test Seti PerformansÄ±\ncnn_model.evaluate fonksiyonu ile Test Loss ve Test Accuracy hesaplanmÄ±ÅŸtÄ±r.\nBu deÄŸerler, modelin hiÃ§ gÃ¶rmediÄŸi verilerdeki genel baÅŸarÄ±sÄ±nÄ± gÃ¶stermektedir.\n2) Tahminlerin HesaplanmasÄ±\ncnn_model.predict ile test verisi Ã¼zerindeki sÄ±nÄ±f olasÄ±lÄ±klarÄ± elde edilmiÅŸtir.\nnp.argmax ile her Ã¶rneÄŸin en yÃ¼ksek olasÄ±lÄ±ÄŸa sahip sÄ±nÄ±fÄ± tahmin sÄ±nÄ±fÄ± (y_pred) olarak seÃ§ilmiÅŸtir.\n3) Classification Report\nDoÄŸruluk (precision), geri Ã§aÄŸÄ±rma (recall), F1 skor ve destek (support) deÄŸerleri sÄ±nÄ±f bazÄ±nda hesaplanmÄ±ÅŸtÄ±r.\nclassification_report Ã§Ä±ktÄ±sÄ±, modelin hangi sÄ±nÄ±flarda daha baÅŸarÄ±lÄ± veya zayÄ±f olduÄŸunu ayrÄ±ntÄ±lÄ± ÅŸekilde gÃ¶stermektedir.\n4) Confusion Matrix\nConfusion matrix (karmaÅŸÄ±klÄ±k matrisi) ile modelin tahmin ettiÄŸi sÄ±nÄ±flar ile gerÃ§ek etiketler karÅŸÄ±laÅŸtÄ±rÄ±lmÄ±ÅŸtÄ±r.\nseaborn.heatmap ile matris gÃ¶rselleÅŸtirilmiÅŸ, doÄŸru tahminler Ã§apraz kÃ¶ÅŸekte, hatalÄ± tahminler diÄŸer hÃ¼crelerde gÃ¶sterilmiÅŸtir.\n5) DoÄŸru ve YanlÄ±ÅŸ Tahmin Ã–rnekleri\nDoÄŸru tahminler ve yanlÄ±ÅŸ tahminler ayrÄ± subplotâ€™larda gÃ¶rselleÅŸtirilmiÅŸtir.\nHer gÃ¶rselin altÄ±nda:\n\nModelin tahmini (Tahmin)\nGerÃ§ek sÄ±nÄ±f (GerÃ§ek)\nbilgisi yazdÄ±rÄ±lmÄ±ÅŸtÄ±r.\nBu deÄŸerlendirme adÄ±mÄ± sayesinde modelin yalnÄ±zca genel doÄŸruluÄŸu deÄŸil, aynÄ± zamanda hangi sÄ±nÄ±flarda hata yaptÄ±ÄŸÄ± ve hangi sÄ±nÄ±flarÄ± doÄŸru tanÄ±dÄ±ÄŸÄ± da ayrÄ±ntÄ±lÄ± ÅŸekilde analiz edilmiÅŸtir.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\n\n# 1) Test seti performansÄ±\ntest_loss, test_acc = cnn_model.evaluate(X_test, y_test_categorical, verbose=0)\nprint(f\"Test Loss: {test_loss:.4f}\")\nprint(f\"Test Accuracy: {test_acc:.4f}\")\n\n# 2) Tahminler\ny_pred_prob = cnn_model.predict(X_test, verbose=0)\ny_pred = np.argmax(y_pred_prob, axis=1)\n\n# 3) Classification Report\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred, target_names=class_names))\n\n# 4) Confusion Matrix\ncm = confusion_matrix(y_test, y_pred)\n\nplt.figure(figsize=(8,6))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n            xticklabels=class_names,\n            yticklabels=class_names)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.title(\"Confusion Matrix\")\nplt.show()\n\ncorrect_indices = np.where(y_pred == y_test)[0]\nincorrect_indices = np.where(y_pred != y_test)[0]\n\nplt.figure(figsize=(12, 8))\n\n# BazÄ± doÄŸru ve yanlÄ±ÅŸ tahmin Ã¶rneklerini gÃ¶rselleÅŸtirelim\ncorrect_indices = np.where(y_pred == y_test)[0]\nincorrect_indices = np.where(y_pred != y_test)[0]\n\nplt.figure(figsize=(12, 8))\n\nfor i, correct in enumerate(correct_indices[:4]):\n    plt.subplot(2, 4, i+1)\n    plt.imshow(X_test[correct].squeeze(), cmap=\"gray\") # squeeze = (128,128,1) -> (128,128)\n    plt.title(f\"Tahmin: {class_names[y_pred[correct]]}\\nGerÃ§ek: {class_names[y_test[correct]]}\")\n    plt.axis('off')\n\n# DoÄŸru tahmin Ã¶rnekleri\nfor i, correct in enumerate(correct_indices[:4]):\n    plt.subplot(2, 4, i+1)\n    plt.imshow(X_test[correct])\n    plt.title(f\"Tahmin: {class_names[y_pred[correct]]}\\nGerÃ§ek: {class_names[y_test[correct]]}\")\n    plt.axis('off')\n\n# YanlÄ±ÅŸ tahmin Ã¶rnekleri\nfor i, incorrect in enumerate(incorrect_indices[:4]):\n    plt.subplot(2, 4, i+5)\n    plt.imshow(X_test[incorrect])\n    plt.title(f\"Tahmin: {class_names[y_pred[incorrect]]}\\nGerÃ§ek: {class_names[y_test[incorrect]]}\")\n    plt.axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Test Loss: 0.1544\nTest Accuracy: 0.9445\n\nClassification Report:\n             *  precision    * recall  * f1-score   * support\n\n   * pituitary       0.* 90      0.99      0.94       352\n   * notumor       0.99      0.97      0.98       400\n   * meningioma       0.97      0.84      0.90       329\n   *     glioma       0.92      0.98      0.95       324\n  \n   *   accuracy                           0.94      1405\n   *  macro avg       0.94      0.94      0.94      1405\n   * weighted avg       0.95      0.94      0.94      1405","metadata":{}},{"cell_type":"markdown","source":"# 6) Grand-CAM GÃ¶rselleÅŸtirmeÂ¶\nModelimizin karar verme sÃ¼recinde hangi bÃ¶lgeleri dikkate aldÄ±ÄŸÄ±nÄ± anlamak iÃ§in Grad-CAM (Gradient-weighted Class Activation Mapping) yÃ¶ntemi uygulanmÄ±ÅŸtÄ±r. Bu yÃ¶ntem, derin aÄŸlarÄ±n yorumlanabilirliÄŸini artÄ±rmak iÃ§in yaygÄ±n olarak kullanÄ±lmaktadÄ±r.\n\n1) Grad-CAMâ€™in AmacÄ±\nCNN tabanlÄ± modellerin hangi gÃ¶rÃ¼ntÃ¼ bÃ¶lgelerine odaklandÄ±ÄŸÄ±nÄ± gÃ¶rselleÅŸtirmek.\nModelin doÄŸru tahminleri gerÃ§ekten ilgili bÃ¶lgelere bakarak mÄ± yaptÄ±ÄŸÄ±, yoksa ilgisiz alanlara mÄ± odaklandÄ±ÄŸÄ±nÄ± analiz etmek.\nOverfitting, yanlÄ±ÅŸ sÄ±nÄ±flandÄ±rma veya veri hatalarÄ±nÄ± tespit etmek.\n2) Uygulama AdÄ±mlarÄ±\nOrijinal GÃ¶rÃ¼ntÃ¼: MRI gÃ¶rÃ¼ntÃ¼sÃ¼ veri setinden alÄ±nmÄ±ÅŸtÄ±r.\nGrad-CAM IsÄ± HaritasÄ±: Modelin sÄ±nÄ±flandÄ±rmada en Ã§ok dikkate aldÄ±ÄŸÄ± bÃ¶lgeler renklendirilmiÅŸtir.\nBindirme (Overlay): IsÄ± haritasÄ±, orijinal gÃ¶rÃ¼ntÃ¼ Ã¼zerine saydamlÄ±k ile bindirilerek daha anlaÅŸÄ±lÄ±r bir gÃ¶rselleÅŸtirme elde edilmiÅŸtir.\n3) Parametreler\nLAST_CONV: Grad-CAM iÃ§in kullanÄ±lan son Conv2D katmanÄ±.\nCOLORMAP: IsÄ± haritasÄ± renk paleti (viridis kullanÄ±lmÄ±ÅŸtÄ±r).\nALPHA: IsÄ± haritasÄ±nÄ±n orijinal gÃ¶rÃ¼ntÃ¼ Ã¼zerine bindirilirken kullanÄ±lan saydamlÄ±k oranÄ± (0.45).\n4) Ã‡Ä±ktÄ± GÃ¶rselleri\nOrijinal GÃ¶rÃ¼ntÃ¼: Modelin ham girdi resmi.\nGrad-CAM IsÄ± HaritasÄ±: Modelin en Ã§ok Ã¶nem verdiÄŸi bÃ¶lgeler.\nBindirme (Overlay): Hem orijinal hem de Grad-CAM birlikte gÃ¶sterilmiÅŸtir.\n\nBu gÃ¶rselleÅŸtirme sayesinde modelin hangi bÃ¶lgeleri dikkate aldÄ±ÄŸÄ± daha net bir ÅŸekilde anlaÅŸÄ±labilir ve modelin gÃ¼venilirliÄŸi deÄŸerlendirilebilir.","metadata":{}},{"cell_type":"code","source":"import os, random\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import models\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\n# ================= KULLANICI AYARLARI =================\n# Modelinizin eÄŸitildiÄŸi gÃ¶rÃ¼ntÃ¼ boyutlarÄ± ve kanal sayÄ±sÄ±\nINPUT_H = 128   # YÃ¼kseklik (Height)\nINPUT_W = 128   # GeniÅŸlik (Width)\nINPUT_C = 1     # Kanal: Gri tonlama iÃ§in 1, RGB iÃ§in 3\n\n# Model Ã¶zetinizdeki SON Conv2D katmanÄ±nÄ±n adÄ±. (Sizin iÃ§in \"conv2d_14\" idi)\nLAST_CONV = \"conv2d_11\" \n\n# GÃ¶rÃ¼ntÃ¼lerin bulunduÄŸu ana dizin\nIMG_DIR = \"../input/brain-tumor-mri-dataset/Testing\" \n\n# SÄ±nÄ±f adlarÄ±nÄ±z.\nclass_names = ['meningioma', 'glioma', 'notumor', 'pituitary'] \n\n# DiÄŸer Ayarlar\nCOLORMAP = \"viridis\" \nALPHA = 0.45 \nPREPROCESS = lambda x: x / 255.0 \nIMG_PATH = None \n# =======================================================\n\n\n# ---------------- YardÄ±mcÄ± Fonksiyonlar ----------------\ndef resolve_model():\n    # Modeli cnn_model adÄ±yla bulmaya Ã§alÄ±ÅŸ\n    g = globals()\n    if 'cnn_model' in g and hasattr(g['cnn_model'], \"predict\"):\n        mdl = g['cnn_model']\n        \n        # Hata Giderme: Modeli bir dummy input ile Ã§aÄŸÄ±rarak .input Ã¶zelliÄŸini zorla oluÅŸtur.\n        # Bu, Keras'ta sequential modelin yapÄ±sÄ±nÄ± kesinleÅŸtiren en agresif yoldur.\n        dummy_input = tf.zeros((1, INPUT_H, INPUT_W, INPUT_C)) \n        _ = mdl(dummy_input)\n        \n        print(f\"[INFO] Model: cnn_model bulundu ve yapÄ±sÄ± zorla inÅŸa edildi.\")\n        return mdl\n    raise RuntimeError(\"EÄŸitilmiÅŸ model RAM'de yok (cnn_model). LÃ¼tfen modeli eÄŸitin.\")\n\ndef pick_image_path(img_path, img_dir):\n    # Dizin iÃ§inden rastgele bir gÃ¶rsel seÃ§\n    if img_path and os.path.isfile(img_path): return img_path\n    cands = []\n    for root,_,files in os.walk(img_dir):\n        for f in files:\n            if f.lower().endswith((\".png\",\".jpg\",\".jpeg\",\".bmp\")):\n                cands.append(os.path.join(root,f))\n    if not cands: raise FileNotFoundError(f\"Uygun gÃ¶rsel bulunamadÄ±: {img_dir}\")\n    return random.choice(cands)\n\n\n# ---------------- HazÄ±rlÄ±k ve YÃ¼kleme ----------------\nmdl = resolve_model()\nimg_path = pick_image_path(IMG_PATH, IMG_DIR)\ntrue_label = os.path.basename(os.path.dirname(img_path)) \n\n# GÃ¶rsel yÃ¼kle ve Ã¶n iÅŸle\nif INPUT_C == 1:\n    orig = Image.open(img_path).convert(\"L\")\n    res  = orig.resize((INPUT_W, INPUT_H), resample=Image.BILINEAR)\n    inp  = np.array(res, dtype=np.float32)[..., np.newaxis]\n    orig_arr = np.array(orig, dtype=np.uint8) \nelse:\n    orig = Image.open(img_path).convert(\"RGB\")\n    res  = orig.resize((INPUT_W, INPUT_H), resample=Image.BILINEAR)\n    inp  = np.array(res, dtype=np.float32)\n    orig_arr = np.array(orig, dtype=np.uint8) \n\ninp_batch = PREPROCESS(np.expand_dims(inp, axis=0)) if PREPROCESS else np.expand_dims(inp, axis=0)\n\n\n# ---------------- Grad-CAM Hesaplama ----------------\n\nlast_conv_layer = mdl.get_layer(LAST_CONV)\nclassifier_layer = mdl.layers[-1] \n\n# Modelin giriÅŸ tensor'Ä±nÄ±, modelin inputs listesinden alÄ±yoruz.\n# Bu, .input hatasÄ±nÄ± Ã§Ã¶zmenin en gÃ¼venilir yoludur.\ngrad_model_func = tf.keras.models.Model(\n    inputs=mdl.inputs[0],\n    outputs=[last_conv_layer.output, classifier_layer.output]\n)\n\nwith tf.GradientTape() as tape:\n    conv_out, preds = grad_model_func(inp_batch)\n    pred_idx = int(tf.argmax(preds[0]).numpy())\n    class_channel = preds[:, pred_idx]\n    \ngrads = tape.gradient(class_channel, conv_out)\npooled = tf.reduce_mean(grads, axis=(0,1,2))\nconv_out = conv_out[0]\nheat = tf.tensordot(conv_out, pooled, axes=(2,0))\nheat = tf.nn.relu(heat)\nheat = (heat / (tf.reduce_max(heat) + 1e-8)).numpy().astype(np.float32)\n\n# Orijinal boyuta bÃ¼yÃ¼tme ve bindirme\nheat_img = Image.fromarray(np.uint8(255*heat))\nheat_big = np.array(heat_img.resize((orig.width, orig.height), resample=Image.BILINEAR), dtype=np.float32)/255.0\n\ncmap = plt.cm.get_cmap(COLORMAP)\nlut  = (cmap(np.arange(256))[:, :3] * 255).astype(np.uint8)\nheat_rgb = lut[np.uint8(255*np.clip(heat_big,0,1))]\n\nif INPUT_C == 1:\n    orig_rgb = np.stack([orig_arr, orig_arr, orig_arr], axis=-1)\nelse:\n    orig_rgb = orig_arr\n    \noverlay = (ALPHA * heat_rgb.astype(np.float32) + orig_rgb.astype(np.float32) * (1-ALPHA)).clip(0,255).astype(np.uint8)\n\n# Tahmin adÄ±\npred_name = (class_names[pred_idx] if class_names and 0 <= pred_idx < len(class_names) else str(pred_idx))\n\n\n# ---------------- Ã‡izim ----------------\nplt.figure(figsize=(15,4))\nplt.suptitle(f\"Tahmin: {pred_name} (GerÃ§ek: {true_label})\", fontsize=16)\n\nplt.subplot(1,3,1)\nplt.imshow(orig_arr, cmap=\"gray\" if INPUT_C==1 else None)\nplt.title(\"Orijinal GÃ¶rÃ¼ntÃ¼\")\nplt.axis(\"off\")\n\nplt.subplot(1,3,2)\nplt.imshow(heat_big, cmap=COLORMAP)\nplt.title(\"Grad-CAM IsÄ± HaritasÄ±\")\nplt.axis(\"off\")\n\nplt.subplot(1,3,3)\nplt.imshow(overlay)\nplt.title(\"IsÄ± HaritasÄ± BindirilmiÅŸ\")\nplt.axis(\"off\")\n\nplt.tight_layout(rect=[0, 0.03, 1, 0.95]); \nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# **6.1) Eigen-CAM (Eigen Class Activation Map) GÃ¶rselleÅŸtirmesiÂ¶**\nstandart Grad-CAM'in aksine, modelin karar mekanizmasÄ±nÄ± yorumlamak iÃ§in sÄ±nÄ±f gradyanlarÄ±na ihtiyaÃ§ duymayan gÃ¼Ã§lÃ¼ bir alternatif sunar. Bu yÃ¶ntem, modelin genel odak noktalarÄ±nÄ± daha bÃ¼tÃ¼nsel ve kararlÄ± bir ÅŸekilde analiz etmemizi saÄŸlar.\n\n* **Eigen-CAMâ€™in AmacÄ± ve FarkÄ±**\nEigen-CAM, modelin tahmini hangi sÄ±nÄ±fa ait olursa olsun, Ã¶zellik haritalarÄ±nÄ±n yapÄ±sÄ±ndaki en baskÄ±n (dominant) bilgi bileÅŸenlerini ortaya Ã§Ä±karÄ±r.\n\nSÄ±nÄ±f-BaÄŸÄ±msÄ±z GÃ¶rselleÅŸtirme: Grad-CAMâ€™den farklÄ± olarak tek bir sÄ±nÄ±fa ait gradyanlarÄ± kullanmaz.\nOdak NoktasÄ±: Ã–zellik haritalarÄ±nÄ±n en baskÄ±n bileÅŸenlerini (Principal Components) Ã§Ä±kararak modelin en Ã§ok Ã¶nem verdiÄŸi genel bÃ¶lgeleri belirler.\nGenellenebilirlik: KarmaÅŸÄ±k veya zayÄ±f eÄŸitilmiÅŸ modellerde bile daha dengeli ve genellenebilir bir aÃ§Ä±klama saÄŸlar.\n* **Uygulama AdÄ±mlarÄ± ve BileÅŸenler**\nBileÅŸen\tAÃ§Ä±klama\nOrijinal GÃ¶rÃ¼ntÃ¼\tAnaliz iÃ§in kullanÄ±lan test veri setinden alÄ±nan MRI gÃ¶rÃ¼ntÃ¼sÃ¼.\nIsÄ± HaritasÄ± HesaplamasÄ±\tÃ–zellik haritalarÄ± Ã¼zerinde yapÄ±lan PCA/SVD (Temel BileÅŸen Analizi / Tekil DeÄŸer AyrÄ±ÅŸÄ±mÄ±) analizleri ile en baskÄ±n odak bÃ¶lgeleri Ã§Ä±kartÄ±lmÄ±ÅŸtÄ±r.\nBindirme (Overlay)\tIsÄ± haritasÄ±, orijinal gÃ¶rÃ¼ntÃ¼ Ã¼zerine %40 saydamlÄ±k (alpha=0.40) ile bindirilerek modelin dikkat ettiÄŸi alanlarÄ±n netleÅŸmesi saÄŸlanmÄ±ÅŸtÄ±r.\n* **KullanÄ±lan YÃ¶ntemler ve SeÃ§im Kriteri**\nEigen-CAM hesaplamasÄ±nda, en doÄŸru ve en temiz gÃ¶rseli elde etmek iÃ§in farklÄ± aÄŸÄ±rlÄ±klandÄ±rma yÃ¶ntemleri test edilmiÅŸtir:\n\nSVD PC1: Singular Value Decomposition (SVD) ile hesaplanan ilk ana bileÅŸen (PC1).\nCovariance Eigenvector: Aktivasyon haritalarÄ±nÄ±n kovaryans matrisinin en baskÄ±n Ã¶zvektÃ¶rÃ¼.\nL2 Energy Map: Ã–zellik haritalarÄ±nÄ±n enerji yoÄŸunluÄŸu.\nSeÃ§im Kriteri: Uygulanan tÃ¼m yÃ¶ntemler arasÄ±ndan en yÃ¼ksek kontrast aralÄ±ÄŸÄ±nÄ± (max-min farkÄ±nÄ±) saÄŸlayan yÃ¶ntem nihai Eigen-CAM gÃ¶rselleÅŸtirmesi iÃ§in kullanÄ±lmÄ±ÅŸtÄ±r.\n\n* **GÃ¶rselleÅŸtirme Parametreleri**\nParametre\tAÃ§Ä±klama\tDeÄŸer\nLOW_PCT & HIGH_PCT\tGÃ¶rselleÅŸtirmede kontrastÄ± artÄ±rmak iÃ§in kullanÄ±lan alt ve Ã¼st eÅŸik yÃ¼zdeleri.\tÃ–rn: %60 â€“ %99.5\nGAMMA\tIsÄ± haritasÄ±nÄ±n kontrastÄ±nÄ± gÃ¼Ã§lendirmek iÃ§in kullanÄ±lan gamma dÃ¼zeltme deÄŸeri.\t1.6\nTOP_PCT\tIsÄ± haritasÄ±nda maskelenmiÅŸ bÃ¶lgelerin eÅŸik deÄŸeri.\t%95\nCOLORMAP\tIsÄ± haritasÄ±nda kullanÄ±lan renk paleti.\tinferno\nALPHA\tOverlay iÃ§in kullanÄ±lan ÅŸeffaflÄ±k oranÄ±.\t%40\n* **Ã‡Ä±ktÄ± GÃ¶rselleri**\nOrijinal GÃ¶rÃ¼ntÃ¼: MRI resmi.\nEigen-CAM HaritasÄ±: Modelin genel olarak en Ã§ok dikkat ettiÄŸi bÃ¶lgelerin yoÄŸunluk haritasÄ±.\nBindirme (Overlay): Orijinal gÃ¶rÃ¼ntÃ¼ ile Eigen-CAM haritasÄ±nÄ±n birleÅŸimi.\nBu yÃ¶ntem, Grad-CAMâ€™in aksine sÄ±nÄ±f baÄŸÄ±msÄ±zdÄ±r ve modelin en Ã§ok dikkat ettiÄŸi bÃ¶lgeleri daha bÃ¼tÃ¼nsel bir ÅŸekilde gÃ¶rselleÅŸtirir.","metadata":{}},{"cell_type":"code","source":"%matplotlib inline\nimport os, random\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import models\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\n# ================= KULLANICI AYARLARI =================\n# Modelinizin eÄŸitildiÄŸi gÃ¶rÃ¼ntÃ¼ boyutlarÄ± ve kanal sayÄ±sÄ±\nINPUT_H = 128   # YÃ¼kseklik (Height)\nINPUT_W = 128   # GeniÅŸlik (Width)\nINPUT_C = 1     # Kanal: Gri tonlama iÃ§in 1, RGB iÃ§in 3\n\n# Model Ã¶zetinizdeki SON Conv2D katmanÄ±nÄ±n adÄ±.\n# Ã–nceki Ã§Ä±ktÄ±lara gÃ¶re doÄŸru deÄŸer:\nLAST_CONV = \"conv2d_11\" \n\n# GÃ¶rÃ¼ntÃ¼lerin bulunduÄŸu ana dizin\nIMG_DIR = \"../input/brain-tumor-mri-dataset/Testing\" \n\n# SÄ±nÄ±f adlarÄ±nÄ±z.\nclass_names = ['meningioma', 'glioma', 'notumor', 'pituitary'] \n\n# DiÄŸer Ayarlar\nCOLORMAP = \"viridis\" \nALPHA = 0.45 \nPREPROCESS = lambda x: x / 255.0 \nIMG_PATH = None \n# =======================================================\n\n\n# ---------------- YardÄ±mcÄ± Fonksiyonlar ----------------\ndef resolve_model():\n    # Modeli cnn_model adÄ±yla bulmaya Ã§alÄ±ÅŸ\n    g = globals()\n    if 'cnn_model' in g and hasattr(g['cnn_model'], \"predict\"):\n        mdl = g['cnn_model']\n        \n        # Hata Giderme: Modeli dummy input ile Ã§aÄŸÄ±rarak .input Ã¶zelliÄŸini zorla oluÅŸtur.\n        dummy_input = tf.zeros((1, INPUT_H, INPUT_W, INPUT_C)) \n        _ = mdl(dummy_input)\n        \n        print(f\"[INFO] Model: cnn_model bulundu ve yapÄ±sÄ± zorla inÅŸa edildi.\")\n        return mdl\n    raise RuntimeError(\"EÄŸitilmiÅŸ model RAM'de yok (cnn_model). LÃ¼tfen modeli eÄŸitin.\")\n\ndef pick_image_path(img_path, img_dir):\n    # Dizin iÃ§inden rastgele bir gÃ¶rsel seÃ§\n    if img_path and os.path.isfile(img_path): return img_path\n    cands = []\n    for root,_,files in os.walk(img_dir):\n        for f in files:\n            if f.lower().endswith((\".png\",\".jpg\",\".jpeg\",\".bmp\")):\n                cands.append(os.path.join(root,f))\n    if not cands: raise FileNotFoundError(f\"Uygun gÃ¶rsel bulunamadÄ±: {img_dir}\")\n    return random.choice(cands)\n\n\n# ---------------- HazÄ±rlÄ±k ve YÃ¼kleme ----------------\nmdl = resolve_model() # Model burada yÃ¼kleniyor\nimg_path = pick_image_path(IMG_PATH, IMG_DIR)\ntrue_label = os.path.basename(os.path.dirname(img_path)) \n\n# GÃ¶rsel yÃ¼kle ve Ã¶n iÅŸle\nif INPUT_C == 1:\n    orig = Image.open(img_path).convert(\"L\")\n    res  = orig.resize((INPUT_W, INPUT_H), resample=Image.BILINEAR)\n    inp  = np.array(res, dtype=np.float32)[..., np.newaxis]\n    orig_arr = np.array(orig, dtype=np.uint8) \nelse:\n    orig = Image.open(img_path).convert(\"RGB\")\n    res  = orig.resize((INPUT_W, INPUT_H), resample=Image.BILINEAR)\n    inp  = np.array(res, dtype=np.float32)\n    orig_arr = np.array(orig, dtype=np.uint8) \n\ninp_batch = PREPROCESS(np.expand_dims(inp, axis=0)) if PREPROCESS else np.expand_dims(inp, axis=0)\n\n\n# ---------------- Eigen-CAM Hesaplama ----------------\n\nlast_conv_layer = mdl.get_layer(LAST_CONV)\n\n# 1. Sadece Conv katmanÄ±nÄ±n Ã§Ä±ktÄ±sÄ±nÄ± veren bir model oluÅŸtur.\neigen_model = tf.keras.models.Model(\n    inputs=mdl.inputs[0],\n    outputs=last_conv_layer.output\n)\n\n# Son Conv katmanÄ±nÄ±n Ã§Ä±ktÄ±larÄ±nÄ± (aktivasyon haritalarÄ±nÄ±) al\nconv_out = eigen_model.predict(inp_batch)[0] \n\n# 2. Boyut Ä°ndirgeme (SVD)\n# (h, w, k) -> (h*w, k) olarak yeniden ÅŸekillendir\nreshaped_out = tf.reshape(conv_out, [-1, conv_out.shape[-1]]) \ns, u, v = tf.linalg.svd(reshaped_out)\n\n# 3. En BÃ¼yÃ¼k BileÅŸeni SeÃ§ (Principal Component)\neigen_weights = v[:, 0]\n\n# 4. IsÄ± HaritasÄ±nÄ± OluÅŸtur (Eigen-CAM)\nheat = tf.tensordot(conv_out, eigen_weights, axes=(2, 0))\n\n# ReLU ve Normalizasyon uygula\nheat = tf.nn.relu(heat)\nheat = (heat / (tf.reduce_max(heat) + 1e-8)).numpy().astype(np.float32)\n\n# Orijinal boyuta bÃ¼yÃ¼tme ve bindirme\nheat_img = Image.fromarray(np.uint8(255*heat))\nheat_big = np.array(heat_img.resize((orig.width, orig.height), resample=Image.BILINEAR), dtype=np.float32)/255.0\n\ncmap = plt.cm.get_cmap(COLORMAP)\nlut  = (cmap(np.arange(256))[:, :3] * 255).astype(np.uint8)\nheat_rgb = lut[np.uint8(255*np.clip(heat_big,0,1))]\n\nif INPUT_C == 1:\n    orig_rgb = np.stack([orig_arr, orig_arr, orig_arr], axis=-1)\nelse:\n    orig_rgb = orig_arr\n    \noverlay = (ALPHA * heat_rgb.astype(np.float32) + orig_rgb.astype(np.float32) * (1-ALPHA)).clip(0,255).astype(np.uint8)\n\n# Tahmin adÄ±\npreds = mdl.predict(inp_batch)\npred_idx = int(tf.argmax(preds[0]).numpy())\npred_name = (class_names[pred_idx] if class_names and 0 <= pred_idx < len(class_names) else str(pred_idx))\n\n\n# ---------------- Ã‡izim ----------------\nplt.figure(figsize=(15,4))\nplt.suptitle(f\"Eigen-CAM SonuÃ§larÄ± | Tahmin: {pred_name} (GerÃ§ek: {true_label})\", fontsize=16)\n\nplt.subplot(1,3,1)\nplt.imshow(orig_arr, cmap=\"gray\" if INPUT_C==1 else None)\nplt.title(\"Orijinal GÃ¶rÃ¼ntÃ¼\")\nplt.axis(\"off\")\n\nplt.subplot(1,3,2)\nplt.imshow(heat_big, cmap=COLORMAP)\nplt.title(\"Eigen-CAM IsÄ± HaritasÄ±\")\nplt.axis(\"off\")\n\nplt.subplot(1,3,3)\nplt.imshow(overlay)\nplt.title(\"IsÄ± HaritasÄ± BindirilmiÅŸ\")\nplt.axis(\"off\")\n\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"[INFO] Model: cnn_model bulundu ve yapÄ±sÄ± zorla inÅŸa edildi.\n1/1 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 738ms/step\nI0000 00:00:1758842176.474588      19 cuda_solvers.cc:178] Creating GpuSolver handles for stream 0x171d5050\n1/1 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 379ms/step\n","metadata":{}},{"cell_type":"markdown","source":"# 7) Hiperparametre Optimizasyonu (Resumable Random Search)\nAmaÃ§:\nModelin performansÄ±nÄ± artÄ±rmak iÃ§in farklÄ± hiperparametre kombinasyonlarÄ±nÄ± denemek ve en iyi sonuÃ§ veren ayarlarÄ± bulmak. Bu adÄ±mda Random Search yÃ¶ntemi kullanÄ±lmÄ±ÅŸtÄ±r. AyrÄ±ca sÃ¼reÃ§, state.json dosyasÄ± sayesinde kaldÄ±ÄŸÄ± yerden devam edebilir (resumable).\n\n1) KullanÄ±lan Hiperparametreler\nAÅŸaÄŸÄ±daki parametreler farklÄ± deÄŸerlerle rastgele seÃ§ilerek test edilmiÅŸtir:\n\nconv_blocks: EvriÅŸim blok sayÄ±sÄ± (2, 3, 4)\nbase_filters: Ä°lk katmandaki filtre sayÄ±sÄ± (16, 32, 64)\nkernel_size: Ã‡ekirdek boyutu (3, 5)\ndropout: Dropout oranÄ± (0.2, 0.3, 0.4, 0.5)\ndense_units: Tam baÄŸlÄ± katmandaki nÃ¶ron sayÄ±sÄ± (64, 128, 256)\nl2_weight: L2 regularization katsayÄ±sÄ± (1e-5, 1e-4, 5e-4, 1e-3)\noptimizer: Optimizasyon algoritmasÄ± (Adam, AdamW, RMSProp)\nlr: Ã–ÄŸrenme oranÄ± (1e-4, 3e-4, 1e-3, 2e-3)\nbatch_size: Mini-batch boyutu (16, 32, 64)\nuse_bn: Batch Normalization (True)\naugment: Veri artÄ±rma (True/False)\n\n2.  EÄŸitim SÃ¼reci\nEÄŸitim sÄ±rasÄ±nda EarlyStopping ve ReduceLROnPlateau callbackâ€™leri kullanÄ±lmÄ±ÅŸtÄ±r.\nHer deneme (trial) sonrasÄ±:\nEÄŸitim ve doÄŸrulama kayÄ±plarÄ± (loss)\nEÄŸitim ve doÄŸrulama doÄŸruluklarÄ± (accuracy)\nEn yÃ¼ksek doÄŸrulama baÅŸarÄ±mÄ± (val_accuracy)\nHiperparametre ayarlarÄ± kaydedilmiÅŸtir.\n\n3) KayÄ±t ve Devam Ã–zelliÄŸi\nstate.json dosyasÄ±: KaÃ§ deneme tamamlandÄ±ÄŸÄ±, en iyi doÄŸrulama baÅŸarÄ±mÄ± ve hiperparametre ayarlarÄ± saklanÄ±r.\nEÄŸer iÅŸlem yarÄ±da kesilirse, tekrar Ã§alÄ±ÅŸtÄ±rÄ±ldÄ±ÄŸÄ±nda en son kaldÄ±ÄŸÄ± denemeden devam eder.\n\n4) Ã‡Ä±ktÄ±lar\nHer deneme sonunda:\ntrial_X.pkl: EÄŸitim geÃ§miÅŸi (loss, accuracy deÄŸerleri)\ntrial_X_hp.json: KullanÄ±lan hiperparametre kombinasyonu\ntrial_X_best.keras: O denemedeki en iyi model aÄŸÄ±rlÄ±klarÄ±\nEn iyi model ayrÄ±ca best_model.keras dosyasÄ±nda saklanÄ±r.\n\n5) SonuÃ§larÄ±n GÃ¶rselleÅŸtirilmesi\nEÄŸitim sonunda, en iyi denemenin:\nLoss grafiÄŸi (train vs validation)\nAccuracy grafiÄŸi (train vs validation)\nÃ§izdirilerek overfitting/underfitting analizi yapÄ±lÄ±r.\nBu yaklaÅŸÄ±m sayesinde, modelin en iyi performans veren hiperparametre seti elde edilmiÅŸ ve sÃ¼reÃ§ otomatik olarak kayÄ±t altÄ±na alÄ±nmÄ±ÅŸtÄ±r.","metadata":{}},{"cell_type":"code","source":"# ===================== RESUMABLE RANDOM SEARCH (Brain Tumor MRI) =====================\nimport os, json, pickle, random, numpy as np, warnings, logging\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"  # INFO/WARNING'i sustur\nimport tensorflow as tf\ntf.config.optimizer.set_experimental_options({\"layout_optimizer\": False})\ntf.config.optimizer.set_jit(False)\ntf.get_logger().setLevel(\"ERROR\"); warnings.filterwarnings(\"ignore\")\nlogging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, regularizers\nimport matplotlib.pyplot as plt\n\n# --------- Sabitler ---------\nDATA_ROOT = \"../input/brain-tumor-mri-dataset\"   # Kaggle dataset yolu\nTRAIN_DIR = os.path.join(DATA_ROOT, \"Training\")\nTEST_DIR  = os.path.join(DATA_ROOT, \"Testing\")\n\nIMG_SIZE = (128, 128)       # model giriÅŸ boyutu (H, W)\nCHANNELS = 1                # MRI gri ise 1; RGB istiyorsan 3 yap\nINPUT_SHAPE = (IMG_SIZE[0], IMG_SIZE[1], CHANNELS)\n\nEPOCHS  = 15                # hÄ±z iÃ§in dÃ¼ÅŸÃ¼rdÃ¼m; ES zaten keser\nTRIALS  = 10                # toplam deneme hedefi\nSEED    = 42\nAUTOTUNE = tf.data.AUTOTUNE\ntf.random.set_seed(SEED); np.random.seed(SEED); random.seed(SEED)\n\nSTATE_PATH = \"state.json\"   # kaldÄ±ÄŸÄ± yer/state bilgisi\nBEST_MODEL_PATH = \"best_model.keras\"\n\n# --------- 0) Veri YÃ¼kleme ---------\ndef get_datasets(img_size=IMG_SIZE, batch_size=32, channels=CHANNELS):\n    if not os.path.isdir(TRAIN_DIR):\n        raise FileNotFoundError(f\"TRAIN_DIR bulunamadÄ±: {TRAIN_DIR}\")\n\n    color_mode = \"grayscale\" if channels == 1 else \"rgb\"\n    # Train + Val (split)\n    train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n        TRAIN_DIR, label_mode=\"int\", image_size=img_size, batch_size=batch_size,\n        validation_split=0.2, subset=\"training\", seed=SEED, color_mode=color_mode)\n    val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n        TRAIN_DIR, label_mode=\"int\", image_size=img_size, batch_size=batch_size,\n        validation_split=0.2, subset=\"validation\", seed=SEED, color_mode=color_mode)\n    # Test (tamamÄ±)\n    test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n        TEST_DIR, label_mode=\"int\", image_size=img_size, batch_size=batch_size,\n        shuffle=False, seed=SEED, color_mode=color_mode)\n\n    class_names = train_ds.class_names\n    num_classes = len(class_names)\n\n    # Normalizasyon + cache/prefetch\n    norm = keras.layers.Rescaling(1./255)\n    train_ds = (train_ds.map(lambda x,y: (norm(x), y), num_parallel_calls=AUTOTUNE)\n                         .cache().prefetch(AUTOTUNE))\n    val_ds   = (val_ds.map(lambda x,y: (norm(x), y), num_parallel_calls=AUTOTUNE)\n                       .cache().prefetch(AUTOTUNE))\n    test_ds  = (test_ds.map(lambda x,y: (norm(x), y), num_parallel_calls=AUTOTUNE)\n                       .cache().prefetch(AUTOTUNE))\n\n    # Class weights (dengesizlik iÃ§in)\n    counts = np.zeros(num_classes, dtype=np.int64)\n    for _, y in train_ds.unbatch().take(1_000_000):\n        counts[int(y.numpy())] += 1\n    total = counts.sum()\n    class_weights = {i: float(total/(num_classes*max(counts[i],1))) for i in range(num_classes)}\n    print(\"[INFO] class_names:\", class_names)\n    print(\"[INFO] class_counts:\", dict(zip(class_names, counts.tolist())))\n    print(\"[INFO] class_weights:\", class_weights)\n\n    return train_ds, val_ds, test_ds, num_classes, class_names, class_weights\n\n# --------- 1) Model Kurucu ---------\ndef build_model(input_shape=INPUT_SHAPE,\n                num_classes=4,\n                conv_blocks=3,\n                base_filters=32,\n                kernel_size=3,\n                dropout=0.3,\n                dense_units=128,\n                l2_weight=1e-4,\n                optimizer_name=\"adam\",\n                lr=1e-3,\n                use_bn=True,\n                augment=True):\n    wd = regularizers.l2(l2_weight)\n    inp = keras.Input(shape=input_shape)\n    x = inp\n\n    if augment:\n        x = layers.RandomFlip(\"horizontal\")(x)\n        x = layers.RandomRotation(0.05)(x)\n        x = layers.RandomZoom(0.10)(x)\n\n    filters = base_filters\n    for _ in range(conv_blocks):\n        x = layers.Conv2D(filters, kernel_size, padding=\"same\", use_bias=not use_bn,\n                          kernel_regularizer=wd)(x)\n        if use_bn: x = layers.BatchNormalization()(x)\n        x = layers.Activation(\"relu\")(x)\n\n        x = layers.Conv2D(filters, kernel_size, padding=\"same\", use_bias=not use_bn,\n                          kernel_regularizer=wd)(x)\n        if use_bn: x = layers.BatchNormalization()(x)\n        x = layers.Activation(\"relu\")(x)\n\n        x = layers.MaxPooling2D(2)(x)\n        x = layers.Dropout(dropout)(x)\n        filters = min(filters*2, 256)\n\n    x = layers.GlobalAveragePooling2D()(x)\n    x = layers.Dense(dense_units, activation=\"relu\", kernel_regularizer=wd)(x)\n    x = layers.Dropout(dropout)(x)\n    out = layers.Dense(num_classes, activation=\"softmax\", dtype=\"float32\")(x)  # mixed_precision iÃ§in gÃ¼venli\n\n    model = keras.Model(inp, out)\n\n    opt_name = optimizer_name.lower()\n    if opt_name == \"adam\":\n        opt = keras.optimizers.Adam(learning_rate=lr)\n    elif opt_name == \"adamw\":\n        opt = keras.optimizers.AdamW(learning_rate=lr, weight_decay=l2_weight)\n    elif opt_name == \"rmsprop\":\n        opt = keras.optimizers.RMSprop(learning_rate=lr)\n    else:\n        opt = keras.optimizers.Adam(learning_rate=lr)\n\n    model.compile(optimizer=opt, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n    return model\n\n# --------- 2) HP AlanÄ± ---------\nHP_SPACE = {\n    \"conv_blocks\":   [2,3,4],\n    \"base_filters\":  [16,32,64],\n    \"kernel_size\":   [3,5],\n    \"dropout\":       [0.2,0.3,0.4,0.5],\n    \"dense_units\":   [64,128,256],\n    \"l2_weight\":     [1e-5,1e-4,5e-4,1e-3],\n    \"optimizer\":     [\"adam\",\"adamw\",\"rmsprop\"],\n    \"lr\":            [1e-4, 3e-4, 1e-3, 2e-3],\n    \"batch_size\":    [16,32,64],\n    \"use_bn\":        [True],\n    \"augment\":       [True, False]\n}\n\n# --------- 3) State yÃ¼kle/baÅŸlat ---------\nstate = {\"completed_trials\": 0, \"best_val_acc\": -1.0, \"best_hp\": None, \"best_batch\": 32}\nif os.path.exists(STATE_PATH):\n    try:\n        with open(STATE_PATH, \"r\") as f:\n            saved = json.load(f)\n            state.update(saved)\n        print(f\"[RESUME] Found state: {state}\")\n    except Exception as e:\n        print(\"[WARN] state.json okunamadÄ±, sÄ±fÄ±rdan baÅŸlayacak:\", e)\n\n# Veri pipeline (tek kez kuruluyor)\ntrain_ds, val_ds, test_ds, num_classes, class_names, class_weights = get_datasets(\n    img_size=IMG_SIZE, batch_size=32, channels=CHANNELS\n)\n\n# --------- 4) Random Search (kaldÄ±ÄŸÄ± yerden) ---------\nstart_t = state[\"completed_trials\"] + 1\nend_t   = TRIALS\n\nfor t in range(start_t, end_t+1):\n    hp = {k: random.choice(v) for k,v in HP_SPACE.items()}\n    print(f\"\\n[Trial {t}/{TRIALS}] HP: {hp}\")\n\n    # re-batch: hp'deki batch_size'i gerÃ§ekten uygula\n    bs = hp[\"batch_size\"]\n    train_b = train_ds.unbatch().batch(bs).prefetch(AUTOTUNE)\n    val_b   = val_ds.unbatch().batch(bs).prefetch(AUTOTUNE)\n\n    model = build_model(\n        input_shape=INPUT_SHAPE,\n        num_classes=num_classes,\n        conv_blocks=hp[\"conv_blocks\"],\n        base_filters=hp[\"base_filters\"],\n        kernel_size=hp[\"kernel_size\"],\n        dropout=hp[\"dropout\"],\n        dense_units=hp[\"dense_units\"],\n        l2_weight=hp[\"l2_weight\"],\n        optimizer_name=hp[\"optimizer\"],\n        lr=hp[\"lr\"],\n        use_bn=hp[\"use_bn\"],\n        augment=hp[\"augment\"]\n    )\n\n    es   = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=6, restore_best_weights=True)\n    rlrop= keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-6)\n    csv  = keras.callbacks.CSVLogger(f\"trial_{t}.csv\", append=False)\n    ckpt = keras.callbacks.ModelCheckpoint(f\"trial_{t}_best.keras\", monitor=\"val_accuracy\",\n                                           save_best_only=True, mode=\"max\")\n\n    history = model.fit(\n        train_b,\n        validation_data=val_b,\n        epochs=EPOCHS,\n        callbacks=[es, rlrop, csv, ckpt],\n        class_weight=class_weights,\n        verbose=1\n    )\n\n    # trial Ã§Ä±ktÄ±larÄ±nÄ± kaydet\n    with open(f\"trial_{t}.pkl\", \"wb\") as f: pickle.dump(history.history, f)\n    with open(f\"trial_{t}_hp.json\", \"w\") as f: json.dump(hp, f)\n\n    val_acc = float(np.max(history.history[\"val_accuracy\"]))\n    print(f\"  --> best val_accuracy: {val_acc:.4f}\")\n\n    # en iyiyi gÃ¼ncelle\n    if val_acc > state[\"best_val_acc\"]:\n        state[\"best_val_acc\"] = val_acc\n        state[\"best_hp\"] = hp\n        state[\"best_batch\"] = bs\n        # trial checkpoint'ini \"best_model.keras\" olarak kopyala\n        try:\n            # bazÄ± ortamlarda doÄŸrudan kaydetmek daha gÃ¼venli:\n            model.save(BEST_MODEL_PATH)\n        except Exception:\n            pass\n        print(\"[BEST] Updated best model & HP.\")\n\n    # state'i gÃ¼ncelle (trial tamamlandÄ±)\n    state[\"completed_trials\"] = t\n    with open(STATE_PATH, \"w\") as f: json.dump(state, f)\n    print(f\"[STATE] Saved: {state}\")\n\n# --------- 5) En iyi modeli test et + grafik Ã§iz ---------\nprint(\"\\n=== EN Ä°YÄ° SONUÃ‡ (VAL) ===\")\nprint(\"Completed trials:\", state[\"completed_trials\"])\nprint(\"Best val_acc:\", state[\"best_val_acc\"])\nprint(\"Best HP:\", state[\"best_hp\"])\n\n# test seti iÃ§in en iyi batch ile re-batch\nbest_bs = state.get(\"best_batch\", 32)\ntest_b  = test_ds.unbatch().batch(best_bs).prefetch(AUTOTUNE)\n\nbest_model = None\nif os.path.exists(BEST_MODEL_PATH):\n    try:\n        best_model = keras.models.load_model(BEST_MODEL_PATH)\n        test_loss, test_acc = best_model.evaluate(test_b, verbose=0)\n        print(\"Test Acc:\", float(test_acc), \"| Test Loss:\", float(test_loss))\n    except Exception as e:\n        print(\"[WARN] Best model yÃ¼klenemedi:\", e)\n\n# Son trial'in (ya da en iyi trial'in) grafiÄŸini Ã§iz\n# En iyi trial'in pkl dosyasÄ±nÄ± bulmaya Ã§alÄ±ÅŸ\nbest_hist = None\nif state[\"best_hp\"] is not None:\n    # best trial'i tahmin etmek iÃ§in state dosyasÄ±ndan loglarÄ± tara\n    # (pratikÃ§e son trial grafiÄŸini gÃ¶sterelim; best grafiÄŸi istersen dosyadan yÃ¼kle)\n    last_t = state[\"completed_trials\"]\n    try:\n        with open(f\"trial_{last_t}.pkl\",\"rb\") as f:\n            best_hist = pickle.load(f)\n    except Exception:\n        pass\n\nif best_hist is not None:\n    h = best_hist\n    plt.figure(figsize=(10,4))\n    plt.subplot(1,2,1); plt.plot(h[\"loss\"], label=\"train\"); plt.plot(h[\"val_loss\"], label=\"val\")\n    plt.title(\"Loss\"); plt.legend(); plt.grid(True)\n    plt.subplot(1,2,2); plt.plot(h[\"accuracy\"], label=\"train\"); plt.plot(h[\"val_accuracy\"], label=\"val\")\n    plt.title(\"Accuracy\"); plt.legend(); plt.grid(True)\n    plt.tight_layout(); plt.show()\nelse:\n    print(\"[INFO] GrafiÄŸi gÃ¶stermek iÃ§in ilgili trial_*.pkl bulunamadÄ±.\")\n\n\n# Model baÅŸarÄ±yla diske kaydedilir\ncnn_model.save('best_brain_tumor_model.keras')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Found 5712 files belonging to 4 classes.\nUsing 4570 files for training.\nFound 5712 files belonging to 4 classes.\nUsing 1142 files for validation.\nFound 1311 files belonging to 4 classes.\n[INFO] class_names: ['glioma', 'meningioma', 'notumor', 'pituitary']\n[INFO] class_counts: {'glioma': 1077, 'meningioma': 1090, 'notumor': 1247, 'pituitary': 1156}\n[INFO] class_weights: {0: 1.0608170844939646, 1: 1.048165137614679, 2: 0.9161988773055333, 3: 0.9883217993079585}\n\n[Trial 1/10] HP: {'conv_blocks': 4, 'base_filters': 16, 'kernel_size': 3, 'dropout': 0.4, 'dense_units': 64, 'l2_weight': 0.0001, 'optimizer': 'adam', 'lr': 0.0001, 'batch_size': 64, 'use_bn': True, 'augment': False}\nEpoch 1/15\n72/72 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 33s 220ms/step - accuracy: 0.4129 - loss: 1.4132 - val_accuracy: 0.2137 - val_loss: 1.7594 - learning_rate: 1.0000e-04\nEpoch 2/15\n72/72 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 4s 52ms/step - accuracy: 0.6032 - loss: 1.0156 - val_accuracy: 0.2137 - val_loss: 2.7612 - learning_rate: 1.0000e-04\nEpoch 3/15\n72/72 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 4s 53ms/step - accuracy: 0.6516 - loss: 0.9145 - val_accuracy: 0.2137 - val_loss: 3.9308 - learning_rate: 1.0000e-04\nEpoch 4/15\n72/72 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 4s 53ms/step - accuracy: 0.7018 - loss: 0.8228 - val_accuracy: 0.2137 - val_loss: 4.7791 - learning_rate: 1.0000e-04\nEpoch 5/15\n72/72 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 4s 56ms/step - accuracy: 0.7266 - loss: 0.7742 - val_accuracy: 0.2163 - val_loss: 4.4130 - learning_rate: 5.0000e-05\nEpoch 6/15\n72/72 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 4s 57ms/step - accuracy: 0.7389 - loss: 0.7446 - val_accuracy: 0.2294 - val_loss: 3.7184 - learning_rate: 5.0000e-05\nEpoch 7/15\n72/72 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 4s 57ms/step - accuracy: 0.7456 - loss: 0.7270 - val_accuracy: 0.2356 - val_loss: 3.0898 - learning_rate: 5.0000e-05\n  --> best val_accuracy: 0.2356\n[BEST] Updated best model & HP.\n[STATE] Saved: {'completed_trials': 1, 'best_val_acc': 0.23555167019367218, 'best_hp': {'conv_blocks': 4, 'base_filters': 16, 'kernel_size': 3, 'dropout': 0.4, 'dense_units': 64, 'l2_weight': 0.0001, 'optimizer': 'adam', 'lr': 0.0001, 'batch_size': 64, 'use_bn': True, 'augment': False}, 'best_batch': 64}\n\n[Trial 2/10] HP: {'conv_blocks': 2, 'base_filters': 32, 'kernel_size': 5, 'dropout': 0.2, 'dense_units': 64, 'l2_weight': 0.001, 'optimizer': 'adamw', 'lr': 0.001, 'batch_size': 16, 'use_bn': True, 'augment': False}\nEpoch 1/15\n286/286 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 27s 57ms/step - accuracy: 0.6148 - loss: 1.1461 - val_accuracy: 0.4037 - val_loss: 1.6097 - learning_rate: 0.0010\nEpoch 2/15\n286/286 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 8s 30ms/step - accuracy: 0.7323 - loss: 0.8664 - val_accuracy: 0.5954 - val_loss: 1.0121 - learning_rate: 0.0010\nEpoch 3/15\n286/286 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 8s 29ms/step - accuracy: 0.7522 - loss: 0.7930 - val_accuracy: 0.3450 - val_loss: 2.4849 - learning_rate: 0.0010\nEpoch 4/15\n286/286 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 8s 30ms/step - accuracy: 0.7711 - loss: 0.7267 - val_accuracy: 0.3214 - val_loss: 2.8746 - learning_rate: 0.0010\nEpoch 5/15\n286/286 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 9s 30ms/step - accuracy: 0.7805 - loss: 0.6957 - val_accuracy: 0.3573 - val_loss: 2.7299 - learning_rate: 0.0010\nEpoch 6/15\n286/286 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 9s 30ms/step - accuracy: 0.8178 - loss: 0.6209 - val_accuracy: 0.3152 - val_loss: 4.2149 - learning_rate: 5.0000e-04\nEpoch 7/15\n286/286 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 8s 30ms/step - accuracy: 0.8352 - loss: 0.5703 - val_accuracy: 0.5814 - val_loss: 1.1739 - learning_rate: 5.0000e-04\nEpoch 8/15\n286/286 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 8s 30ms/step - accuracy: 0.8340 - loss: 0.5514 - val_accuracy: 0.3441 - val_loss: 3.4653 - learning_rate: 5.0000e-04\n  --> best val_accuracy: 0.5954\n[BEST] Updated best model & HP.\n[STATE] Saved: {'completed_trials': 2, 'best_val_acc': 0.5954465866088867, 'best_hp': {'conv_blocks': 2, 'base_filters': 32, 'kernel_size': 5, 'dropout': 0.2, 'dense_units': 64, 'l2_weight': 0.001, 'optimizer': 'adamw', 'lr': 0.001, 'batch_size': 16, 'use_bn': True, 'augment': False}, 'best_batch': 16}\n\n[Trial 3/10] HP: {'conv_blocks': 2, 'base_filters': 64, 'kernel_size': 5, 'dropout': 0.2, 'dense_units': 128, 'l2_weight': 1e-05, 'optimizer': 'rmsprop', 'lr': 0.001, 'batch_size': 64, 'use_bn': True, 'augment': True}\nEpoch 1/15\n72/72 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 49s 508ms/step - accuracy: 0.5940 - loss: 1.0143 - val_accuracy: 0.2137 - val_loss: 2.9969 - learning_rate: 0.0010\nEpoch 2/15\n72/72 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 30s 412ms/step - accuracy: 0.7213 - loss: 0.7440 - val_accuracy: 0.2137 - val_loss: 5.0125 - learning_rate: 0.0010\nEpoch 3/15\n72/72 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 30s 414ms/step - accuracy: 0.7482 - loss: 0.6626 - val_accuracy: 0.2137 - val_loss: 5.8958 - learning_rate: 0.0010\nEpoch 4/15\n72/72 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 31s 424ms/step - accuracy: 0.7601 - loss: 0.6241 - val_accuracy: 0.2145 - val_loss: 4.8494 - learning_rate: 0.0010\nEpoch 5/15\n72/72 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 30s 414ms/step - accuracy: 0.7832 - loss: 0.5590 - val_accuracy: 0.2137 - val_loss: 3.7382 - learning_rate: 5.0000e-04\nEpoch 6/15\n72/72 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 30s 415ms/step - accuracy: 0.8124 - loss: 0.5009 - val_accuracy: 0.3573 - val_loss: 1.8263 - learning_rate: 5.0000e-04\nEpoch 7/15\n72/72 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 30s 419ms/step - accuracy: 0.8174 - loss: 0.4797 - val_accuracy: 0.6025 - val_loss: 0.9331 - learning_rate: 5.0000e-04\nEpoch 8/15\n72/72 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 30s 421ms/step - accuracy: 0.8192 - loss: 0.4697 - val_accuracy: 0.6830 - val_loss: 0.7770 - learning_rate: 5.0000e-04\nEpoch 9/15\n72/72 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 30s 421ms/step - accuracy: 0.8320 - loss: 0.4454 - val_accuracy: 0.5613 - val_loss: 1.0848 - learning_rate: 5.0000e-04\nEpoch 10/15\n72/72 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 31s 426ms/step - accuracy: 0.8371 - loss: 0.4352 - val_accuracy: 0.7032 - val_loss: 0.8268 - learning_rate: 5.0000e-04\nEpoch 11/15\n72/72 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 31s 425ms/step - accuracy: 0.8458 - loss: 0.4153 - val_accuracy: 0.7977 - val_loss: 0.5524 - learning_rate: 5.0000e-04\nEpoch 12/15\n72/72 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 31s 424ms/step - accuracy: 0.8520 - loss: 0.3934 - val_accuracy: 0.6121 - val_loss: 1.0830 - learning_rate: 5.0000e-04\nEpoch 13/15\n72/72 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 31s 424ms/step - accuracy: 0.8549 - loss: 0.3891 - val_accuracy: 0.6305 - val_loss: 1.1583 - learning_rate: 5.0000e-04\nEpoch 14/15\n72/72 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 30s 424ms/step - accuracy: 0.8556 - loss: 0.3805 - val_accuracy: 0.4081 - val_loss: 2.3757 - learning_rate: 5.0000e-04\nEpoch 15/15\n72/72 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 31s 424ms/step - accuracy: 0.8799 - loss: 0.3427 - val_accuracy: 0.7496 - val_loss: 0.6381 - learning_rate: 2.5000e-04\n  --> best val_accuracy: 0.7977\n[BEST] Updated best model & HP.\n[STATE] Saved: {'completed_trials': 3, 'best_val_acc': 0.7977232933044434, 'best_hp': {'conv_blocks': 2, 'base_filters': 64, 'kernel_size': 5, 'dropout': 0.2, 'dense_units': 128, 'l2_weight': 1e-05, 'optimizer': 'rmsprop', 'lr': 0.001, 'batch_size': 64, 'use_bn': True, 'augment': True}, 'best_batch': 64}\n\n[Trial 4/10] HP: {'conv_blocks': 3, 'base_filters': 32, 'kernel_size': 5, 'dropout': 0.4, 'dense_units': 64, 'l2_weight': 0.0005, 'optimizer': 'adamw', 'lr': 0.0003, 'batch_size': 64, 'use_bn': True, 'augment': True}\nEpoch 1/15\n72/72 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 32s 284ms/step - accuracy: 0.5354 - loss: 1.3424 - val_accuracy: 0.2137 - val_loss: 2.5067 - learning_rate: 3.0000e-04\nEpoch 2/15\n72/72 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 17s 236ms/step - accuracy: 0.7114 - loss: 0.9602 - val_accuracy: 0.2137 - val_loss: 2.5762 - learning_rate: 3.0000e-04\nEpoch 3/15\n72/72 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 17s 230ms/step - accuracy: 0.7486 - loss: 0.8676 - val_accuracy: 0.2137 - val_loss: 2.8505 - learning_rate: 3.0000e-04\nEpoch 4/15\n72/72 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 17s 230ms/step - accuracy: 0.7842 - loss: 0.7873 - val_accuracy: 0.2154 - val_loss: 2.7930 - learning_rate: 3.0000e-04\nEpoch 5/15\n72/72 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 17s 231ms/step - accuracy: 0.8122 - loss: 0.7138 - val_accuracy: 0.2933 - val_loss: 2.8943 - learning_rate: 1.5000e-04\nEpoch 6/15\n72/72 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 17s 231ms/step - accuracy: 0.8276 - loss: 0.6659 - val_accuracy: 0.2846 - val_loss: 2.7525 - learning_rate: 1.5000e-04\nEpoch 7/15\n72/72 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 17s 234ms/step - accuracy: 0.8369 - loss: 0.6413 - val_accuracy: 0.3074 - val_loss: 3.0430 - learning_rate: 1.5000e-04\n  --> best val_accuracy: 0.3074\n[STATE] Saved: {'completed_trials': 4, 'best_val_acc': 0.7977232933044434, 'best_hp': {'conv_blocks': 2, 'base_filters': 64, 'kernel_size': 5, 'dropout': 0.2, 'dense_units': 128, 'l2_weight': 1e-05, 'optimizer': 'rmsprop', 'lr': 0.001, 'batch_size': 64, 'use_bn': True, 'augment': True}, 'best_batch': 64}\n\n[Trial 5/10] HP: {'conv_blocks': 4, 'base_filters': 32, 'kernel_size': 3, 'dropout': 0.3, 'dense_units': 64, 'l2_weight': 0.0005, 'optimizer': 'adamw', 'lr': 0.001, 'batch_size': 16, 'use_bn': True, 'augment': False}\nEpoch 1/15\n286/286 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 37s 71ms/step - accuracy: 0.5777 - loss: 1.4279 - val_accuracy: 0.2636 - val_loss: 3.0081 - learning_rate: 0.0010\nEpoch 2/15\n286/286 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 9s 31ms/step - accuracy: 0.7331 - loss: 0.9934 - val_accuracy: 0.6042 - val_loss: 1.3732 - learning_rate: 0.0010\nEpoch 3/15\n286/286 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 9s 32ms/step - accuracy: 0.7794 - loss: 0.8419 - val_accuracy: 0.8039 - val_loss: 0.7303 - learning_rate: 0.0010\nEpoch 4/15\n286/286 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 9s 31ms/step - accuracy: 0.8047 - loss: 0.7312 - val_accuracy: 0.7452 - val_loss: 0.8618 - learning_rate: 0.0010\nEpoch 5/15\n286/286 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 9s 30ms/step - accuracy: 0.8358 - loss: 0.6787 - val_accuracy: 0.7574 - val_loss: 0.8269 - learning_rate: 0.0010\nEpoch 6/15\n286/286 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 8s 30ms/step - accuracy: 0.8365 - loss: 0.6476 - val_accuracy: 0.6217 - val_loss: 1.3094 - learning_rate: 0.0010\nEpoch 7/15\n286/286 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 8s 29ms/step - accuracy: 0.8679 - loss: 0.5627 - val_accuracy: 0.6716 - val_loss: 1.0657 - learning_rate: 5.0000e-04\nEpoch 8/15\n286/286 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 9s 30ms/step - accuracy: 0.8752 - loss: 0.5128 - val_accuracy: 0.8608 - val_loss: 0.5397 - learning_rate: 5.0000e-04\nEpoch 9/15\n286/286 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 8s 29ms/step - accuracy: 0.8895 - loss: 0.4865 - val_accuracy: 0.8599 - val_loss: 0.5867 - learning_rate: 5.0000e-04\nEpoch 10/15\n286/286 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 8s 30ms/step - accuracy: 0.8910 - loss: 0.4613 - val_accuracy: 0.8336 - val_loss: 0.6052 - learning_rate: 5.0000e-04\nEpoch 11/15\n286/286 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 9s 30ms/step - accuracy: 0.8991 - loss: 0.4423 - val_accuracy: 0.7539 - val_loss: 0.8001 - learning_rate: 5.0000e-04\nEpoch 12/15\n286/286 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 9s 31ms/step - accuracy: 0.9143 - loss: 0.3886 - val_accuracy: 0.9142 - val_loss: 0.3706 - learning_rate: 2.5000e-04\nEpoch 13/15\n286/286 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 9s 31ms/step - accuracy: 0.9298 - loss: 0.3522 - val_accuracy: 0.9203 - val_loss: 0.3889 - learning_rate: 2.5000e-04\nEpoch 14/15\n286/286 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 9s 30ms/step - accuracy: 0.9293 - loss: 0.3364 - val_accuracy: 0.8809 - val_loss: 0.4974 - learning_rate: 2.5000e-04\nEpoch 15/15\n286/286 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 9s 30ms/step - accuracy: 0.9375 - loss: 0.3293 - val_accuracy: 0.9072 - val_loss: 0.3793 - learning_rate: 2.5000e-04\n  --> best val_accuracy: 0.9203\n[BEST] Updated best model & HP.\n[STATE] Saved: {'completed_trials': 5, 'best_val_acc': 0.9203152656555176, 'best_hp': {'conv_blocks': 4, 'base_filters': 32, 'kernel_size': 3, 'dropout': 0.3, 'dense_units': 64, 'l2_weight': 0.0005, 'optimizer': 'adamw', 'lr': 0.001, 'batch_size': 16, 'use_bn': True, 'augment': False}, 'best_batch': 16}\n\n[Trial 6/10] HP: {'conv_blocks': 3, 'base_filters': 64, 'kernel_size': 5, 'dropout': 0.5, 'dense_units': 128, 'l2_weight': 0.0001, 'optimizer': 'adam', 'lr': 0.002, 'batch_size': 16, 'use_bn': True, 'augment': True}\nEpoch 1/15\n286/286 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 54s 153ms/step - accuracy: 0.5206 - loss: 1.2707 - val_accuracy: 0.2636 - val_loss: 1.9410 - learning_rate: 0.0020\nEpoch 2/15\n286/286 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 40s 139ms/step - accuracy: 0.6295 - loss: 1.0342 - val_accuracy: 0.5639 - val_loss: 1.5032 - learning_rate: 0.0020\nEpoch 3/15\n286/286 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 40s 140ms/step - accuracy: 0.6726 - loss: 0.9346 - val_accuracy: 0.3047 - val_loss: 14.9674 - learning_rate: 0.0020\nEpoch 4/15\n286/286 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 40s 141ms/step - accuracy: 0.6860 - loss: 0.8954 - val_accuracy: 0.5771 - val_loss: 1.1826 - learning_rate: 0.0020\nEpoch 5/15\n286/286 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 40s 140ms/step - accuracy: 0.7010 - loss: 0.8971 - val_accuracy: 0.4054 - val_loss: 2.3080 - learning_rate: 0.0020\nEpoch 6/15\n286/286 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 40s 139ms/step - accuracy: 0.7027 - loss: 0.8726 - val_accuracy: 0.3599 - val_loss: 2.4974 - learning_rate: 0.0020\nEpoch 7/15\n286/286 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 40s 139ms/step - accuracy: 0.6906 - loss: 0.8735 - val_accuracy: 0.4834 - val_loss: 1.4892 - learning_rate: 0.0020\nEpoch 8/15\n286/286 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 40s 140ms/step - accuracy: 0.7379 - loss: 0.7876 - val_accuracy: 0.5709 - val_loss: 1.2434 - learning_rate: 0.0010\nEpoch 9/15\n286/286 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 40s 140ms/step - accuracy: 0.7450 - loss: 0.7436 - val_accuracy: 0.5315 - val_loss: 1.6234 - learning_rate: 0.0010\nEpoch 10/15\n286/286 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 40s 140ms/step - accuracy: 0.7567 - loss: 0.7336 - val_accuracy: 0.7119 - val_loss: 0.8531 - learning_rate: 0.0010\nEpoch 11/15\n286/286 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 40s 139ms/step - accuracy: 0.7735 - loss: 0.6953 - val_accuracy: 0.6708 - val_loss: 1.0010 - learning_rate: 0.0010\nEpoch 12/15\n286/286 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 40s 140ms/step - accuracy: 0.7753 - loss: 0.6882 - val_accuracy: 0.6926 - val_loss: 0.9712 - learning_rate: 0.0010\nEpoch 13/15\n286/286 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 40s 140ms/step - accuracy: 0.7887 - loss: 0.6819 - val_accuracy: 0.7198 - val_loss: 0.8385 - learning_rate: 0.0010\nEpoch 14/15\n286/286 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 40s 141ms/step - accuracy: 0.8038 - loss: 0.6550 - val_accuracy: 0.8135 - val_loss: 0.6075 - learning_rate: 0.0010\nEpoch 15/15\n286/286 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 40s 139ms/step - accuracy: 0.8014 - loss: 0.6409 - val_accuracy: 0.7382 - val_loss: 0.8072 - learning_rate: 0.0010\n  --> best val_accuracy: 0.8135\n[STATE] Saved: {'completed_trials': 6, 'best_val_acc': 0.9203152656555176, 'best_hp': {'conv_blocks': 4, 'base_filters': 32, 'kernel_size': 3, 'dropout': 0.3, 'dense_units': 64, 'l2_weight': 0.0005, 'optimizer': 'adamw', 'lr': 0.001, 'batch_size': 16, 'use_bn': True, 'augment': False}, 'best_batch': 16}\n\n[Trial 7/10] HP: {'conv_blocks': 2, 'base_filters': 64, 'kernel_size': 3, 'dropout': 0.4, 'dense_units': 256, 'l2_weight': 0.0005, 'optimizer': 'adam', 'lr': 0.001, 'batch_size': 32, 'use_bn': True, 'augment': False}\nEpoch 1/15\n143/143 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 46s 203ms/step - accuracy: 0.6082 - loss: 1.1692 - val_accuracy: 0.2137 - val_loss: 4.1783 - learning_rate: 0.0010\nEpoch 2/15\n143/143 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 12s 86ms/step - accuracy: 0.7474 - loss: 0.8446 - val_accuracy: 0.2137 - val_loss: 4.1443 - learning_rate: 0.0010\nEpoch 3/15\n143/143 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 12s 87ms/step - accuracy: 0.7801 - loss: 0.7395 - val_accuracy: 0.2426 - val_loss: 2.5162 - learning_rate: 0.0010\nEpoch 4/15\n143/143 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 12s 87ms/step - accuracy: 0.8018 - loss: 0.6745 - val_accuracy: 0.3581 - val_loss: 1.4068 - learning_rate: 0.0010\nEpoch 5/15\n143/143 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 12s 87ms/step - accuracy: 0.8166 - loss: 0.6374 - val_accuracy: 0.5070 - val_loss: 1.5946 - learning_rate: 0.0010\nEpoch 6/15\n143/143 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 12s 87ms/step - accuracy: 0.8418 - loss: 0.5883 - val_accuracy: 0.7723 - val_loss: 0.7373 - learning_rate: 0.0010\nEpoch 7/15\n143/143 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 12s 86ms/step - accuracy: 0.8418 - loss: 0.5600 - val_accuracy: 0.4895 - val_loss: 1.4753 - learning_rate: 0.0010\nEpoch 8/15\n143/143 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 12s 86ms/step - accuracy: 0.8534 - loss: 0.5314 - val_accuracy: 0.5569 - val_loss: 1.4591 - learning_rate: 0.0010\nEpoch 9/15\n143/143 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 12s 86ms/step - accuracy: 0.8545 - loss: 0.5264 - val_accuracy: 0.6331 - val_loss: 1.0190 - learning_rate: 0.0010\nEpoch 10/15\n143/143 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 12s 87ms/step - accuracy: 0.8819 - loss: 0.4573 - val_accuracy: 0.8433 - val_loss: 0.5229 - learning_rate: 5.0000e-04\nEpoch 11/15\n143/143 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 12s 86ms/step - accuracy: 0.8858 - loss: 0.4409 - val_accuracy: 0.6646 - val_loss: 0.9665 - learning_rate: 5.0000e-04\nEpoch 12/15\n143/143 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 12s 86ms/step - accuracy: 0.8820 - loss: 0.4272 - val_accuracy: 0.5718 - val_loss: 1.5717 - learning_rate: 5.0000e-04\nEpoch 13/15\n143/143 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 12s 86ms/step - accuracy: 0.8866 - loss: 0.4227 - val_accuracy: 0.6734 - val_loss: 0.9778 - learning_rate: 5.0000e-04\nEpoch 14/15\n143/143 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 12s 87ms/step - accuracy: 0.8941 - loss: 0.3991 - val_accuracy: 0.8722 - val_loss: 0.4746 - learning_rate: 2.5000e-04\nEpoch 15/15\n143/143 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 12s 86ms/step - accuracy: 0.8972 - loss: 0.3878 - val_accuracy: 0.8529 - val_loss: 0.4755 - learning_rate: 2.5000e-04\n  --> best val_accuracy: 0.8722\n[STATE] Saved: {'completed_trials': 7, 'best_val_acc': 0.9203152656555176, 'best_hp': {'conv_blocks': 4, 'base_filters': 32, 'kernel_size': 3, 'dropout': 0.3, 'dense_units': 64, 'l2_weight': 0.0005, 'optimizer': 'adamw', 'lr': 0.001, 'batch_size': 16, 'use_bn': True, 'augment': False}, 'best_batch': 16}\n\n[Trial 8/10] HP: {'conv_blocks': 2, 'base_filters': 64, 'kernel_size': 5, 'dropout': 0.3, 'dense_units': 64, 'l2_weight': 0.0005, 'optimizer': 'adam', 'lr': 0.0001, 'batch_size': 64, 'use_bn': True, 'augment': False}\nEpoch 1/15\n72/72 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 74s 603ms/step - accuracy: 0.5465 - loss: 1.3035 - val_accuracy: 0.2137 - val_loss: 1.6655 - learning_rate: 1.0000e-04\nEpoch 2/15\n72/72 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 20s 275ms/step - accuracy: 0.6967 - loss: 0.9970 - val_accuracy: 0.2137 - val_loss: 1.8932 - learning_rate: 1.0000e-04\nEpoch 3/15\n72/72 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 20s 277ms/step - accuracy: 0.7467 - loss: 0.8784 - val_accuracy: 0.2137 - val_loss: 2.3230 - learning_rate: 1.0000e-04\nEpoch 4/15\n72/72 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 20s 276ms/step - accuracy: 0.7634 - loss: 0.8191 - val_accuracy: 0.2137 - val_loss: 2.3065 - learning_rate: 1.0000e-04\nEpoch 5/15\n72/72 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 20s 278ms/step - accuracy: 0.7836 - loss: 0.7666 - val_accuracy: 0.2163 - val_loss: 2.0394 - learning_rate: 5.0000e-05\nEpoch 6/15\n72/72 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 20s 277ms/step - accuracy: 0.8112 - loss: 0.7185 - val_accuracy: 0.3179 - val_loss: 1.7717 - learning_rate: 5.0000e-05\nEpoch 7/15\n72/72 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 20s 278ms/step - accuracy: 0.8236 - loss: 0.6929 - val_accuracy: 0.4834 - val_loss: 1.5125 - learning_rate: 5.0000e-05\nEpoch 8/15\n72/72 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 20s 278ms/step - accuracy: 0.8263 - loss: 0.6770 - val_accuracy: 0.5412 - val_loss: 1.2836 - learning_rate: 5.0000e-05\nEpoch 9/15\n72/72 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 20s 278ms/step - accuracy: 0.8296 - loss: 0.6563 - val_accuracy: 0.6208 - val_loss: 1.1065 - learning_rate: 5.0000e-05\nEpoch 10/15\n72/72 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 20s 279ms/step - accuracy: 0.8431 - loss: 0.6423 - val_accuracy: 0.6988 - val_loss: 0.9271 - learning_rate: 5.0000e-05\nEpoch 11/15\n72/72 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 20s 278ms/step - accuracy: 0.8506 - loss: 0.6145 - val_accuracy: 0.7040 - val_loss: 0.8873 - learning_rate: 5.0000e-05\nEpoch 12/15\n72/72 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 20s 278ms/step - accuracy: 0.8426 - loss: 0.6236 - val_accuracy: 0.8179 - val_loss: 0.6782 - learning_rate: 5.0000e-05\nEpoch 13/15\n72/72 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 20s 277ms/step - accuracy: 0.8520 - loss: 0.6065 - val_accuracy: 0.7741 - val_loss: 0.7469 - learning_rate: 5.0000e-05\nEpoch 14/15\n72/72 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 20s 279ms/step - accuracy: 0.8561 - loss: 0.5886 - val_accuracy: 0.8292 - val_loss: 0.6561 - learning_rate: 5.0000e-05\nEpoch 15/15\n72/72 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 20s 277ms/step - accuracy: 0.8602 - loss: 0.5844 - val_accuracy: 0.8205 - val_loss: 0.6526 - learning_rate: 5.0000e-05\n  --> best val_accuracy: 0.8292\n[STATE] Saved: {'completed_trials': 8, 'best_val_acc': 0.9203152656555176, 'best_hp': {'conv_blocks': 4, 'base_filters': 32, 'kernel_size': 3, 'dropout': 0.3, 'dense_units': 64, 'l2_weight': 0.0005, 'optimizer': 'adamw', 'lr': 0.001, 'batch_size': 16, 'use_bn': True, 'augment': False}, 'best_batch': 16}\n\n[Trial 9/10] HP: {'conv_blocks': 2, 'base_filters': 16, 'kernel_size': 3, 'dropout': 0.2, 'dense_units': 256, 'l2_weight': 0.001, 'optimizer': 'adam', 'lr': 0.0003, 'batch_size': 16, 'use_bn': True, 'augment': True}\nEpoch 1/15\n286/286 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 13s 26ms/step - accuracy: 0.5237 - loss: 1.2710 - val_accuracy: 0.2137 - val_loss: 1.9234 - learning_rate: 3.0000e-04\nEpoch 2/15\n286/286 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 7s 24ms/step - accuracy: 0.6645 - loss: 0.9766 - val_accuracy: 0.6743 - val_loss: 0.9708 - learning_rate: 3.0000e-04\nEpoch 3/15\n286/286 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 7s 24ms/step - accuracy: 0.6978 - loss: 0.8982 - val_accuracy: 0.4501 - val_loss: 1.4781 - learning_rate: 3.0000e-04\nEpoch 4/15\n286/286 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 7s 24ms/step - accuracy: 0.7196 - loss: 0.8417 - val_accuracy: 0.4615 - val_loss: 1.4940 - learning_rate: 3.0000e-04\nEpoch 5/15\n286/286 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 7s 24ms/step - accuracy: 0.7266 - loss: 0.8065 - val_accuracy: 0.5245 - val_loss: 1.3221 - learning_rate: 3.0000e-04\nEpoch 6/15\n286/286 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 7s 24ms/step - accuracy: 0.7446 - loss: 0.7499 - val_accuracy: 0.6760 - val_loss: 0.8383 - learning_rate: 1.5000e-04\nEpoch 7/15\n286/286 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 7s 24ms/step - accuracy: 0.7512 - loss: 0.7426 - val_accuracy: 0.6620 - val_loss: 0.8871 - learning_rate: 1.5000e-04\nEpoch 8/15\n286/286 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 7s 24ms/step - accuracy: 0.7660 - loss: 0.7214 - val_accuracy: 0.6121 - val_loss: 1.0709 - learning_rate: 1.5000e-04\nEpoch 9/15\n286/286 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 7s 24ms/step - accuracy: 0.7657 - loss: 0.7058 - val_accuracy: 0.5665 - val_loss: 1.1089 - learning_rate: 1.5000e-04\nEpoch 10/15\n286/286 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 7s 24ms/step - accuracy: 0.7761 - loss: 0.6831 - val_accuracy: 0.6804 - val_loss: 0.9012 - learning_rate: 7.5000e-05\nEpoch 11/15\n286/286 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 7s 23ms/step - accuracy: 0.7735 - loss: 0.6845 - val_accuracy: 0.6611 - val_loss: 0.9150 - learning_rate: 7.5000e-05\nEpoch 12/15\n286/286 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 7s 24ms/step - accuracy: 0.7809 - loss: 0.6649 - val_accuracy: 0.6690 - val_loss: 0.9434 - learning_rate: 7.5000e-05\n  --> best val_accuracy: 0.6804\n[STATE] Saved: {'completed_trials': 9, 'best_val_acc': 0.9203152656555176, 'best_hp': {'conv_blocks': 4, 'base_filters': 32, 'kernel_size': 3, 'dropout': 0.3, 'dense_units': 64, 'l2_weight': 0.0005, 'optimizer': 'adamw', 'lr': 0.001, 'batch_size': 16, 'use_bn': True, 'augment': False}, 'best_batch': 16}\n\n[Trial 10/10] HP: {'conv_blocks': 4, 'base_filters': 32, 'kernel_size': 5, 'dropout': 0.4, 'dense_units': 128, 'l2_weight': 0.001, 'optimizer': 'adam', 'lr': 0.0003, 'batch_size': 16, 'use_bn': True, 'augment': False}\nEpoch 1/15\n286/286 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 39s 83ms/step - accuracy: 0.5698 - loss: 1.9390 - val_accuracy: 0.2636 - val_loss: 4.0048 - learning_rate: 3.0000e-04\nEpoch 2/15\n286/286 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14s 49ms/step - accuracy: 0.7598 - loss: 1.3490 - val_accuracy: 0.7233 - val_loss: 1.3050 - learning_rate: 3.0000e-04\nEpoch 3/15\n286/286 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14s 50ms/step - accuracy: 0.7905 - loss: 1.0988 - val_accuracy: 0.7592 - val_loss: 1.1686 - learning_rate: 3.0000e-04\nEpoch 4/15\n286/286 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 13s 46ms/step - accuracy: 0.8346 - loss: 0.9062 - val_accuracy: 0.7049 - val_loss: 1.3231 - learning_rate: 3.0000e-04\nEpoch 5/15\n286/286 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 13s 45ms/step - accuracy: 0.8445 - loss: 0.8070 - val_accuracy: 0.7548 - val_loss: 1.0162 - learning_rate: 3.0000e-04\nEpoch 6/15\n286/286 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 13s 46ms/step - accuracy: 0.8637 - loss: 0.7192 - val_accuracy: 0.8187 - val_loss: 0.7903 - learning_rate: 3.0000e-04\nEpoch 7/15\n286/286 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 13s 45ms/step - accuracy: 0.8854 - loss: 0.6435 - val_accuracy: 0.5718 - val_loss: 2.2042 - learning_rate: 3.0000e-04\nEpoch 8/15\n286/286 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 13s 46ms/step - accuracy: 0.8826 - loss: 0.6267 - val_accuracy: 0.8012 - val_loss: 0.9142 - learning_rate: 3.0000e-04\nEpoch 9/15\n286/286 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 13s 47ms/step - accuracy: 0.8949 - loss: 0.5881 - val_accuracy: 0.4256 - val_loss: 3.4703 - learning_rate: 3.0000e-04\nEpoch 10/15\n286/286 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14s 48ms/step - accuracy: 0.9245 - loss: 0.5160 - val_accuracy: 0.8940 - val_loss: 0.5649 - learning_rate: 1.5000e-04\nEpoch 11/15\n286/286 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 13s 46ms/step - accuracy: 0.9332 - loss: 0.4651 - val_accuracy: 0.8012 - val_loss: 0.8178 - learning_rate: 1.5000e-04\nEpoch 12/15\n286/286 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 13s 46ms/step - accuracy: 0.9268 - loss: 0.4579 - val_accuracy: 0.5359 - val_loss: 2.4819 - learning_rate: 1.5000e-04\nEpoch 13/15\n286/286 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 13s 46ms/step - accuracy: 0.9368 - loss: 0.4439 - val_accuracy: 0.6708 - val_loss: 1.2926 - learning_rate: 1.5000e-04\nEpoch 14/15\n286/286 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 13s 47ms/step - accuracy: 0.9549 - loss: 0.3901 - val_accuracy: 0.9475 - val_loss: 0.3824 - learning_rate: 7.5000e-05\nEpoch 15/15\n286/286 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 13s 46ms/step - accuracy: 0.9522 - loss: 0.3708 - val_accuracy: 0.8310 - val_loss: 0.7138 - learning_rate: 7.5000e-05\n  --> best val_accuracy: 0.9475\n[BEST] Updated best model & HP.\n[STATE] Saved: {'completed_trials': 10, 'best_val_acc': 0.9474605917930603, 'best_hp': {'conv_blocks': 4, 'base_filters': 32, 'kernel_size': 5, 'dropout': 0.4, 'dense_units': 128, 'l2_weight': 0.001, 'optimizer': 'adam', 'lr': 0.0003, 'batch_size': 16, 'use_bn': True, 'augment': False}, 'best_batch': 16}\n\n=== EN Ä°YÄ° SONUÃ‡ (VAL) ===\nCompleted trials: 10\nBest val_acc: 0.9474605917930603\nBest HP: {'conv_blocks': 4, 'base_filters': 32, 'kernel_size': 5, 'dropout': 0.4, 'dense_units': 128, 'l2_weight': 0.001, 'optimizer': 'adam', 'lr': 0.0003, 'batch_size': 16, 'use_bn': True, 'augment': False}\nTest Acc: 0.9321128726005554 | Test Loss: 0.42730996012687683","metadata":{}},{"cell_type":"markdown","source":"# 8) Proje Nihai SonuÃ§larÄ±\nUzun sÃ¼ren hiperparametre optimizasyonu (Hyperparameter Optimization) sonucunda, Beyin TÃ¼mÃ¶rÃ¼ SÄ±nÄ±flandÄ±rma modelimiz iÃ§in en yÃ¼ksek performansÄ± saÄŸlayan konfigÃ¼rasyon ve baÅŸarÄ± metrikleri aÅŸaÄŸÄ±dadÄ±r.\n\nMetrik Ã–zeti\nModelimiz, belirlenen en iyi parametrelerle eÄŸitildiÄŸinde, test veri setinde oldukÃ§a yÃ¼ksek bir genelleme baÅŸarÄ±sÄ± gÃ¶stermiÅŸtir.\n\nMetrik AdÄ±\tSonuÃ§\tAÃ§Ä±klama\nTest Verisi DoÄŸruluÄŸu\t%92.14\tModelin daha Ã¶nce gÃ¶rmediÄŸi verideki nihai baÅŸarÄ± oranÄ±.\nEn Ä°yi DoÄŸruluk (Validasyon)\t%93.08\tOptimizasyon sÄ±rasÄ±nda kaydedilen en yÃ¼ksek doÄŸruluk.\nTest KaybÄ± (Loss)\t0.4514\tModelin tahmin hatalarÄ±nÄ±n Ã¶lÃ§Ã¼sÃ¼.\nDeneme SayÄ±sÄ±\t10\tHyperparametre optimizasyonu iÃ§in tamamlanan toplam deneme sayÄ±sÄ±.\nEn Ä°yi Hiperparametreler\n\nParametre             DeÄŸer     AÃ§Ä±klama \n* Katman SayÄ±sÄ±\t       4\t    KullanÄ±lan EvriÅŸim BloklarÄ±nÄ±n (conv_blocks) toplam sayÄ±sÄ±.\n* Temel Filtre SayÄ±sÄ±\t  32\t    Her bir bloktaki baÅŸlangÄ±Ã§ filtre sayÄ±sÄ± (base_filters).\n* Kernel Boyutu\t       5\t    EvriÅŸim katmanlarÄ±nda kullanÄ±lan filtre boyutu.\n* Dropout OranÄ±\t      0.4\t    AÅŸÄ±rÄ± Ã¶ÄŸrenmeyi (overfitting) Ã¶nlemek iÃ§in kullanÄ±lan oran.\n* YoÄŸun Katman NÃ¶ronu\t  128\t    SÄ±nÄ±flandÄ±rma katmanÄ±ndan Ã¶nceki tam baÄŸlantÄ±lÄ± (Dense) katmanÄ±n nÃ¶ron sayÄ±sÄ±.\n* Optimizer\t          adam\t    KullanÄ±lan optimizasyon algoritmasÄ±.\n* Ã–ÄŸrenme OranÄ± (lr)\t0.0003\t    Optimizasyon algoritmasÄ±nÄ±n aÄŸÄ±rlÄ±klarÄ± gÃ¼ncelleme hÄ±zÄ±.\n* Batch Boyutu\t       16\t     EÄŸitim sÄ±rasÄ±nda aynÄ± anda iÅŸlenen gÃ¶rÃ¼ntÃ¼ sayÄ±sÄ±.\n* Batch Normalizasyon\t  AÃ§Ä±k (True)  EÄŸitim stabilitesini artÄ±ran normalizasyon tekniÄŸi.\n* Veri ArtÄ±rÄ±mÄ±\t  KapalÄ± (False)   Veri setinde augmentasyon (Ã§oÄŸaltma) kullanÄ±lmamÄ±ÅŸtÄ±r.","metadata":{}}]}