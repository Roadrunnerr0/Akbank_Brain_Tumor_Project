{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13181798,"sourceType":"datasetVersion","datasetId":8353555},{"sourceId":591205,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":442265,"modelId":458804}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 1) Gerekli Kütüphanelerin İçe Aktarılması\n\nModelin eğitimi ve değerlendirilmesi için ihtiyaç duyulan temel kütüphaneler projeye dahil edilmiştir:\n\nNumPy → Sayısal işlemler ve veri üzerinde hesaplamalar yapmak için\n\nMatplotlib & Seaborn → Grafiksel görselleştirmeler, özellikle eğitim süreci ve confusion matrix için\n\nTensorFlow & Keras → Derin öğrenme modellerini kurmak, eğitmek ve test etmek için\n\nscikit-learn (train_test_split, classification_report, confusion_matrix) → Veriyi eğitim/test olarak ayırmak ve performans ölçümleri almak için\n\ntime → Eğitim süresini hesaplamak için\n\nwarnings → Çalışma esnasında gereksiz uyarı mesajlarını gizlemek için\n\nEk olarak, donanım doğrulaması yapılmıştır:\n\nKullanılan TensorFlow sürümü ekrana yazdırılmıştır.\n\nGPU’nun mevcut olup olmadığı kontrol edilmiş ve sonuç olarak “Evet” veya “Hayır” şeklinde bilgi verilmiştir.\n\nBu kontroller sayesinde, ortamın doğru yapılandırıldığı ve modelin GPU hızlandırmalı çalışıp çalışmadığı netleştirilmiştir.","metadata":{}},{"cell_type":"code","source":"# Gerekli kütüphaneleri içe aktaralım\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport time\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# GPU kullanılabilirliğini kontrol edelim\nprint(\"TensorFlow sürümü:\", tf.__version__)\nprint(\"GPU kullanılabilir mi?\", \"Evet\" if tf.config.list_physical_devices('GPU') else \"Hayır\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **2) **Veri Yükleme ve Keşifsel Analiz****\n\nBu bölümde, Kaggle Brain Tumor MRI Dataset içerisindeki görüntüler işlenmiş, uygun formatta hazırlanmış ve keşifsel analiz yapılmıştır. Süreç şu adımlardan oluşmaktadır:\n\n🔹 Klasör ve Yol Kontrolü\n\nKök dizin: /kaggle/input/brain-tumor-mri-dataset\n\nos.listdir(...) ile klasör yapısı incelenmiş, ilgili alt klasörlerin doğru şekilde bulunduğu doğrulanmıştır.\n\n🔹 Veri Yükleme Fonksiyonu\n\nload_data_from_folders(base_path) fonksiyonu:\n\nAlt klasörleri dolaşarak .jpg / .jpeg / .png / .bmp uzantılı dosyaları okur.\n\nGörseller gri ölçekli (cv2.IMREAD_GRAYSCALE) olarak okunur ve 128×128 boyutuna yeniden ölçeklendirilir.\n\nFonksiyon üç çıktı üretir:\n\nX: Görüntü tensörleri\n\ny: Sınıf etiketleri\n\nclass_names: Sınıf isimleri\n\nGizli/sistem dosyaları (örneğin .DS_Store) filtrelenir.\n\nBaşarısız okuma durumlarında try/except ile hata mesajı yakalanır.\n\n🔹 Yüklemenin Doğrulanması\n\nVeri yüklendikten sonra:\n\nToplam görüntü ve etiket sayısı ekrana yazdırılır.\n\nSınıf isimleri listelenir.\n\nEğer hiç görüntü yüklenmediyse, kullanıcıya uyarı verilir.\n\n🔹 Eğitim/Test Ayrımı\n\ntrain_test_split kullanılarak veriler %80 eğitim – %20 test olacak şekilde ayrılmıştır.\n\nstratify=y parametresi ile sınıf dağılımı her iki sette de korunmuştur.\n\nAyrım sonrası:\n\nX_train ve X_test boyutları\n\ny_train ve y_test boyutları ekrana yazdırılmıştır.\n\n🔹 Örnek Görsellerin İncelenmesi\n\nEğitim setinden ilk 25 görüntü 5×5’lik bir grid halinde görselleştirilmiştir.\n\nAlt etiketlerde, ilgili sınıf adı (class_names[y_train[i]]) gösterilmiştir.\n\nGörseller gri tonlama (cmap='gray') ile sunulmuştur.\n\n🔹 Sınıf Dağılımı Analizi\n\nEğitim ve test setleri için countplot grafiklerinde sınıf dağılımları gösterilmiştir.\n\nX ekseninde sınıf indeksleri, etiketlerde ise class_names yer alır. (Etiketler okunabilirlik için 45° döndürülmüştür.)\n\nBu adım sayesinde, sınıflar arasında dengesizlik olup olmadığı kolayca gözlemlenebilir. (Gerekirse class_weight kullanılabilir.)\n\n🔹 Ek Notlar\n\nGörseller gri ölçekli işlendiği için, model giriş boyutu (128, 128, 1) olmalıdır.\n\nEğitim öncesi normalizasyon (0–255 → 0–1) önerilmektedir.\n\nVeri artırma için:\n\nImageDataGenerator\n\nveya tf.keras.layers tabanlı augmentation katmanları kullanılabilir.\n\nBüyük veri setlerinde bellek optimizasyonu için generator veya tf.data pipeline kullanımı tercih edilebilir.\n\n✅ Bu aşamalar tamamlandığında, veri kümesi eğitim için hazır hale getirilmiş, hem örnek görseller hem de sınıf dağılımları görsel olarak doğrulanmıştır.","metadata":{}},{"cell_type":"code","source":"import os\nprint(os.listdir('/kaggle/input/brain-tumor-mri-dataset'))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport cv2\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical\n\n# Veri setinin ana klasör yolu\ndataset_base_path = '/kaggle/input/brain-tumor-mri-dataset'\nprint(f\"Başlangıç klasör yolu: {dataset_base_path}\")\n\ndef load_data_from_folders(base_path):\n    \"\"\"Belirtilen klasör yolundaki tüm görüntüleri sınıflarına göre yükler.\"\"\"\n    X = []\n    y = []\n    class_names = []\n    IMG_SIZE = 128\n    \n    print(\"\\nVeri seti yükleniyor...\")\n    \n    # Kök klasörden başlayarak tüm dosya ağacını gezelim\n    for dirpath, dirnames, filenames in os.walk(base_path):\n        # Eğer klasörde \".DS_Store\" gibi sistem dosyaları varsa dikkate almayalım\n        filenames = [f for f in filenames if not f.startswith('.')]\n        \n        # Eğer bir görüntü dosyası varsa, bu klasör bir sınıf klasörüdür\n        if any(f.endswith(('.jpg', '.jpeg', '.png', '.bmp')) for f in filenames):\n            class_name = os.path.basename(dirpath)\n            if class_name not in class_names:\n                class_names.append(class_name)\n            \n            class_index = class_names.index(class_name)\n            \n            print(f\"'{class_name}' sınıfı için {len(filenames)} görüntü bulunuyor.\")\n            \n            for image_name in filenames:\n                image_path = os.path.join(dirpath, image_name)\n                try:\n                    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n                    if image is not None:\n                        image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n                        X.append(image)\n                        y.append(class_index)\n                except Exception as e:\n                    print(f\"Hata: {image_path} görüntüsü yüklenemedi. Hata: {e}\")\n\n    return np.array(X), np.array(y), class_names\n\n# Veriyi yükleyelim\nX, y, class_names = load_data_from_folders(dataset_base_path)\n\n# Hata kontrolü\nif X.shape[0] == 0:\n    print(\"\\n--- HATA: Hiç görüntü yüklenemedi. Dosya yapısını veya yolu kontrol edin. ---\")\nelse:\n    print(\"\\nVeri yüklemesi başarılı!\")\n    print(f\"Toplam görüntü sayısı: {len(X)}\")\n    print(f\"Toplam etiket sayısı: {len(y)}\")\n    print(f\"Sınıf isimleri: {class_names}\")\n\n    # Veriyi eğitim ve test setlerine ayıralım\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n    \n    print(\"\\nEğitim verisi şekli:\", X_train.shape)\n    print(\"Eğitim etiketi şekli:\", y_train.shape)\n    print(\"Test verisi şekli:\", X_test.shape)\n    print(\"Test etiketi şekli:\", y_test.shape)\n\n    # İlk 25 görüntüyü görselleştirelim\n    plt.figure(figsize=(10, 10))\n    for i in range(25):\n        plt.subplot(5, 5, i+1)\n        plt.xticks([])\n        plt.yticks([])\n        plt.grid(False)\n        plt.imshow(X_train[i], cmap='gray')\n        plt.xlabel(class_names[y_train[i]])\n    plt.tight_layout()\n    plt.show()\n\n    # Sınıf dağılımını inceleyelim\n    plt.figure(figsize=(10, 5))\n    plt.subplot(1, 2, 1)\n    sns.countplot(x=y_train)\n    plt.title('Eğitim Verisi Sınıf Dağılımı')\n    plt.xlabel('Sınıf')\n    plt.ylabel('Sayı')\n    plt.xticks(np.arange(len(class_names)), class_names, rotation=45)\n\n    plt.subplot(1, 2, 2)\n    sns.countplot(x=y_test)\n    plt.title('Test Verisi Sınıf Dağılımı')\n    plt.xlabel('Sınıf')\n    plt.ylabel('Sayı')\n    plt.xticks(np.arange(len(class_names)), class_names, rotation=45)\n    \n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T17:10:51.565061Z","iopub.execute_input":"2025-09-26T17:10:51.565522Z","iopub.status.idle":"2025-09-26T17:11:06.494306Z","shell.execute_reply.started":"2025-09-26T17:10:51.565484Z","shell.execute_reply":"2025-09-26T17:11:06.493343Z"}},"outputs":[{"name":"stdout","text":"Başlangıç klasör yolu: /kaggle/input/brain-tumor-mri-dataset\n\nVeri seti yükleniyor...\n\n--- HATA: Hiç görüntü yüklenemedi. Dosya yapısını veya yolu kontrol edin. ---\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# **2) 2.1 Veri Ön İşleme**\n\nBu adımda giriş verileri ölçeklendirildi, etiketler uygun formata çevrildi ve eğitim verisi için veri artırma stratejileri tanımlandı.\n\nNormalizasyon (0–255 → 0–1)\n\nX_train ve X_test tensörleri float32 tipe çevrilip 255’e bölündü.\n\nAmaç: Sayısal kararlılık ve daha hızlı yakınsama.\n\nOne-Hot Encoding (Etiket Dönüşümü)\n\ny_train ve y_test etiketleri one-hot vektörlere çevrildi:\n\nkeras.utils.to_categorical(y, num_classes)\n\nNot: num_classes değeri veri kümendeki gerçek sınıf sayısı ile uyumlu olmalıdır (ör. 4 ise 10 değil 4 kullanılmalı).\n\nVeri Artırma (ImageDataGenerator)\n\nEğitim verisine yönelik dönüşümler tanımlandı:\nDöndürme: rotation_range=15\n\nKaydırma: width_shift_range=0.1, height_shift_range=0.1\n\nAyna çevirme: horizontal_flip=True\n\nYakınlaştırma: zoom_range=0.1\n\ndatagen.fit(X_train) ile (gerekli ise) istatistikler hazırlandı.\n\nEğitimde Kullanım (Önemli)\n\nTanımlanan artırma, model.fit sırasında uygulanır:\nmodel.fit(datagen.flow(X_train, y_train_categorical, batch_size=...), validation_data=(X_test, y_test_categorical), ...)\n\nDoğrulama/test tarafında ham veriler kullanılır; artırma sadece eğitim verisine uygulanır.\n\nŞekil (Shape) Kontrolü\n\nNormalizasyon ve one-hot sonrası boyutlar basıldı:\nX_train.shape → giriş tensor şekli\n\ny_train_categorical.shape → (num_samples, num_classes)\n\nBu hazırlıklarla model, ölçeklendirilmiş ve artırılmış eğitim verisi üzerinde eğitilecek; genelleme kabiliyeti artırılırken aşırı öğrenme (overfitting) riski azaltılacaktır.","metadata":{}},{"cell_type":"code","source":"# Veriyi normalize edelim (0-255 arası değerleri 0-1 arasına dönüştürelim)\nX_train = X_train.astype('float32') / 255.0\nX_test = X_test.astype('float32') / 255.0\n\n# Etiketleri one-hot encoding formatına dönüştürelim\ny_train_categorical = keras.utils.to_categorical(y_train, 10)\ny_test_categorical = keras.utils.to_categorical(y_test, 10)\n\nX_train = np.expand_dims(X_train, axis=-1) # (5618, 128, 128, 1)\nX_test = np.expand_dims(X_test, axis=-1) # (test_sayısı, 128, 128, 1)\n\n# Veri artırma için ImageDataGenerator kullanalım\ndatagen = keras.preprocessing.image.ImageDataGenerator(\n    rotation_range=15,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    horizontal_flip=True,\n    zoom_range=0.1\n)\n\n# Veri artırma işlemini eğitim verisine uygulayalım\ndatagen.fit(X_train)\n\n# Normalizasyon ve one-hot encoding sonrası veri boyutlarını kontrol edelim\nprint(\"Normalize edilmiş eğitim verisi şekli:\", X_train.shape)\nprint(\"One-hot encoded eğitim etiketleri şekli:\", y_train_categorical.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ****3) Temel CNN Modelinin Oluşturulması****\nBu adımda, görüntü sınıflandırma problemi için temel bir Konvolüsyonel Sinir Ağı (CNN) modeli tanımlanmıştır. Model, evrişim (convolution) katmanları ile görsel özellikleri çıkarmakta, ardından tam bağlantılı katmanlarla sınıflandırma yapmaktadır.\n\nModel Mimarisi\nİlk Evrişim Bloğu\n\n2 adet Conv2D katmanı (32 filtre, 3×3 kernel, ReLU aktivasyonu, padding='same')\n\nBatch Normalization ile eğitim stabilitesi artırılmıştır.\n\nMaxPooling2D (2×2) ile boyut azaltma.\n\nDropout(0.2) ile aşırı öğrenme (overfitting) önlenmiştir.\n\nİkinci Evrişim Bloğu\n2 adet Conv2D katmanı (64 filtre).\n\nBatch Normalization + ReLU aktivasyonu.\n\nMaxPooling2D ve Dropout(0.3).\n\nÜçüncü Evrişim Bloğu\n2 adet Conv2D katmanı (128 filtre).\n\nBatch Normalization + ReLU aktivasyonu.\n\nMaxPooling2D ve Dropout(0.4).\n\nSınıflandırıcı Katmanlar\nFlatten ile özellik haritaları vektöre dönüştürülür.\n\nDense(128, ReLU) → Tam bağlantılı gizli katman.\n\nBatch Normalization + Dropout(0.5).\n\nÇıkış katmanı: Dense(10, Softmax) → 10 sınıf için olasılık dağılımı.\n\nModel Özeti\ncnn_model.summary() çıktısı ile katmanların yapısı, çıktı boyutları ve parametre sayıları incelenmiştir.\n\nBu CNN modeli, görüntülerdeki yerel özellikleri evrişim blokları ile öğrenip, tam bağlantılı katmanlarla sınıf tahminleri yapacak şekilde tasarlanmıştır.","metadata":{}},{"cell_type":"code","source":"#Temel CNN modeli\ndef create_cnn_model():\n    model = keras.Sequential([\n        # İlk evrişim bloğu\n        layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)),\n        layers.BatchNormalization(),\n        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n        layers.MaxPooling2D((2, 2)),\n        layers.Dropout(0.2),\n\n        # İkinci evrişim bloğu\n        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n        layers.BatchNormalization(),\n        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n        layers.MaxPooling2D((2, 2)),\n        layers.Dropout(0.3),\n\n        # Üçüncü evrişim bloğu\n        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n        layers.BatchNormalization(),\n        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n        layers.MaxPooling2D((2, 2)),\n        layers.Dropout(0.4),\n\n        # Sınıflandırıcı\n        layers.Flatten(),\n        layers.Dense(128, activation='relu'),\n        layers.BatchNormalization(),\n        layers.Dropout(0.5),\n        layers.Dense(10, activation='softmax')\n    ])\n    return model\n\n# Modeli oluşturalım\ncnn_model = create_cnn_model()\n\n# Model özetini görüntüleyelim\ncnn_model.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n\n│ conv2d (Conv2D)                 │ (None, 32, 32, 32)     │           896 │\n\n│ batch_normalization             │ (None, 32, 32, 32)     │           128 │\n│ (BatchNormalization)            │                        │               │\n\n│ conv2d_1 (Conv2D)               │ (None, 32, 32, 32)     │         9,248 │\n\n│ max_pooling2d (MaxPooling2D)    │ (None, 16, 16, 32)     │             0 │\n\n│ dropout (Dropout)               │ (None, 16, 16, 32)     │             0 │\n\n│ conv2d_2 (Conv2D)               │ (None, 16, 16, 64)     │        18,496 │\n\n│ batch_normalization_1           │ (None, 16, 16, 64)     │           256 │\n│ (BatchNormalization)            │                        │               │\n\n│ conv2d_3 (Conv2D)               │ (None, 16, 16, 64)     │        36,928 │\n\n│ max_pooling2d_1 (MaxPooling2D)  │ (None, 8, 8, 64)       │             0 │\n\n│ dropout_1 (Dropout)             │ (None, 8, 8, 64)       │             0 │\n│ conv2d_4 (Conv2D)               │ (None, 8, 8, 128)      │        73,856 │\n\n│ batch_normalization_2           │ (None, 8, 8, 128)      │           512 │\n│ (BatchNormalization)            │                        │               │\n\n│ conv2d_5 (Conv2D)               │ (None, 8, 8, 128)      │       147,584 │\n\n│ max_pooling2d_2 (MaxPooling2D)  │ (None, 4, 4, 128)      │             0 │\n\n│ dropout_2 (Dropout)             │ (None, 4, 4, 128)      │             0 │\n\n│ flatten (Flatten)               │ (None, 2048)           │             0 │\n\n│ dense (Dense)                   │ (None, 128)            │       262,272 │\n\n│ batch_normalization_3           │ (None, 128)            │           512 │\n│ (BatchNormalization)            │                        │               │\n\n│ dropout_3 (Dropout)             │ (None, 128)            │             0 │\n\n│ dense_1 (Dense)                 │ (None, 10)             │         1,290 │","metadata":{}},{"cell_type":"markdown","source":"# **4) Modeli Derleme ve Eğitme**\nBu adımda veriler hazırlanmış, CNN modeli derlenmiş ve artırılmış eğitim verisi üzerinde eğitilmiştir. Eğitim süreci sonunda accuracy ve loss eğrileri görselleştirilmiştir.\n\n* **Girdi Hazırlığı**\n* Normalizasyon: X_train ve X_test tensörleri float32 tipine dönüştürülüp 0–1 aralığına ölçeklenmiştir.\n* Kanal Boyutu: Gri tonlamalı görüntüler (H, W) formatından (H, W, 1) formatına getirilmiştir.\n* Sınıf Sayısı: class_names listesinden veya maksimum etiket değerine göre otomatik hesaplanmıştır.\n* Etiket Dönüşümü: y_train ve y_test değerleri to_categorical kullanılarak one-hot encoding formatına çevrilmiştir.\nVeri Artırma (Augmentation)\nImageDataGenerator kullanılmıştır.\n* **Parametreler:**\n* Döndürme: rotation_range=15\n* Kaydırma: width_shift_range=0.10, height_shift_range=0.10\n* Yakınlaştırma: zoom_range=0.10\n* Yatay çevirme: horizontal_flip=True\n* **CNN Modeli**\n* Conv2D Katmanları: 32, 64 ve 128 filtreli üç evrişim bloğu.\n* Batch Normalization: Her bloğun ardından eğitim stabilitesi için eklenmiştir.\n* Dropout: Aşırı öğrenmeyi engellemek için 0.2 → 0.5 oranlarıyla kullanılmıştır.\n* Dense Katman: 128 nöronlu gizli katman (ReLU) ve softmax çıkış katmanı.\n* **Derleme (Compile)**\n* Optimizer: Adam\n* Loss Fonksiyonu: Categorical Crossentropy\n* Metrikler: Accuracy\n* **Callback’ler**\n* EarlyStopping: val_loss gelişmezse eğitimi durdurur (patience=8).\n* ReduceLROnPlateau: Öğrenme oranını dinamik olarak düşürür (patience=3, factor=0.5).\n* **Eğitim**\n* Model, 25 epoch boyunca eğitim verisi üzerinde eğitilmiştir.\n* datagen.flow(...) ile veri artırma uygulanarak batch halinde beslenmiştir.\n* Doğrulama seti: (X_test, y_test_categorical).\n* **Görselleştirme**\n* Eğitim ve doğrulama için Accuracy ve Loss grafiklerinin epoch bazında değişimi çizilmiştir.\n* Grafikler, modelin öğrenme süreci ve overfitting/underfitting eğilimlerini gözlemlemek için kullanılmıştır.","metadata":{}},{"cell_type":"code","source":"# === Compile + Train + Plot  ===\nimport numpy as np, matplotlib.pyplot as plt\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n# 1) X ve y zaten var (X_train, X_test, y_train, y_test)\n\nX_train = X_train.astype('float32')\nX_test  = X_test.astype('float32')\nif X_train.max() > 1.5:  # 0-255 ise\n    X_train /= 255.0\n    X_test  /= 255.0\n\n# Kanal eksikse ekle (grayscale -> (H,W,1))\nif X_train.ndim == 3:  # (N,H,W)\n    X_train = np.expand_dims(X_train, -1)\n    X_test  = np.expand_dims(X_test, -1)\n\ninput_shape = X_train.shape[1:]  # (128,128,1) bekliyoruz\n\n# Sınıf sayısını doğru hesapla \nnum_classes = len(class_names) if 'class_names' in globals() and len(class_names) > 0 \\\n              else int(np.max(y_train)) + 1\n\n# Etiketleri one-hot'a doğru boyutla çevir\ny_train_categorical = keras.utils.to_categorical(y_train, num_classes)\ny_test_categorical  = keras.utils.to_categorical(y_test,  num_classes)\n\n# 2) Augmentation (array tabanlı)\ndatagen = keras.preprocessing.image.ImageDataGenerator(\n    rotation_range=15,\n    width_shift_range=0.10,\n    height_shift_range=0.10,\n    zoom_range=0.10,\n    horizontal_flip=True\n)\ndatagen.fit(X_train)\n\n# 3) Model: input_shape ve sınıf sayısına göre güncelledik\ndef create_cnn_model(input_shape, num_classes):\n    model = keras.Sequential([\n        layers.Input(shape=input_shape),                 # (128,128,1) ya da (128,128,3)\n        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n        layers.BatchNormalization(),\n        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n        layers.MaxPooling2D((2, 2)),\n        layers.Dropout(0.2),\n\n        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n        layers.BatchNormalization(),\n        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n        layers.MaxPooling2D((2, 2)),\n        layers.Dropout(0.3),\n\n        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n        layers.BatchNormalization(),\n        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n        layers.MaxPooling2D((2, 2)),\n        layers.Dropout(0.4),\n\n        layers.Flatten(),\n        layers.Dense(128, activation='relu'),\n        layers.BatchNormalization(),\n        layers.Dropout(0.5),\n        layers.Dense(num_classes, activation='softmax')  # <-- doğru sınıf sayısı\n    ])\n    return model\n\ncnn_model = create_cnn_model(input_shape, num_classes)\n\ncnn_model.compile(\n    optimizer='adam',\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\n# 4) Callback'ler\nearly_stopping = keras.callbacks.EarlyStopping(\n    monitor='val_loss', patience=8, restore_best_weights=True\n)\nreduce_lr = keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss', factor=0.5, patience=3, min_lr=1e-5, verbose=1\n)\n\n# 5) Eğitim\nhistory = cnn_model.fit(\n    datagen.flow(X_train, y_train_categorical, batch_size=32),\n    epochs=25,\n    validation_data=(X_test, y_test_categorical),\n    callbacks=[early_stopping, reduce_lr],\n    verbose=1\n)\n\n# 6) Accuracy & Loss grafikleri\nhist = history.history\nplt.figure(figsize=(12,5))\n\nplt.subplot(1,2,1)\nplt.plot(hist.get('accuracy', []), label='Eğitim Doğruluğu')\nplt.plot(hist.get('val_accuracy', []), label='Doğrulama Doğruluğu')\nplt.title('Accuracy'); plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.legend(); plt.grid(True)\n\nplt.subplot(1,2,2)\nplt.plot(hist.get('loss', []), label='Eğitim Kaybı')\nplt.plot(hist.get('val_loss', []), label='Doğrulama Kaybı')\nplt.title('Loss'); plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend(); plt.grid(True)\n\nplt.tight_layout(); plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"  3/176 ━━━━━━━━━━━━━━━━━━━━ 11s 65ms/step - accuracy: 0.2552 - loss: 2.3604\nI0000 00:00:1758841953.110179      63 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n176/176 ━━━━━━━━━━━━━━━━━━━━ 33s 105ms/step - accuracy: 0.5747 - loss: 1.1343 - val_accuracy: 0.2505 - val_loss: 3.5619 - learning_rate: 0.0010\nEpoch 2/25\n\n176/176 ━━━━━━━━━━━━━━━━━━━━ 8s 45ms/step - accuracy: 0.7247 - loss: 0.7236 - val_accuracy: 0.3423 - val_loss: 1.9202 - learning_rate: 0.0010\nEpoch 3/25\n\n176/176 ━━━━━━━━━━━━━━━━━━━━ 8s 45ms/step - accuracy: 0.7858 - loss: 0.5716 - val_accuracy: 0.2932 - val_loss: 2.8610 - learning_rate: 0.0010\nEpoch 4/25\n\n176/176 ━━━━━━━━━━━━━━━━━━━━ 8s 46ms/step - accuracy: 0.8019 - loss: 0.5162 - val_accuracy: 0.5900 - val_loss: 1.6119 - learning_rate: 0.0010\nEpoch 5/25\n\n176/176 ━━━━━━━━━━━━━━━━━━━━ 8s 46ms/step - accuracy: 0.8394 - loss: 0.4224 - val_accuracy: 0.8655 - val_loss: 0.3906 - learning_rate: 0.0010\nEpoch 6/25\n\n176/176 ━━━━━━━━━━━━━━━━━━━━ 8s 45ms/step - accuracy: 0.8784 - loss: 0.3564 - val_accuracy: 0.7879 - val_loss: 0.6130 - learning_rate: 0.0010\nEpoch 7/25\n\n176/176 ━━━━━━━━━━━━━━━━━━━━ 8s 45ms/step - accuracy: 0.8776 - loss: 0.3352 - val_accuracy: 0.8940 - val_loss: 0.2811 - learning_rate: 0.0010\nEpoch 8/25\n\n176/176 ━━━━━━━━━━━━━━━━━━━━ 8s 45ms/step - accuracy: 0.8986 - loss: 0.2898 - val_accuracy: 0.8448 - val_loss: 0.4500 - learning_rate: 0.0010\nEpoch 9/25\n\n176/176 ━━━━━━━━━━━━━━━━━━━━ 8s 45ms/step - accuracy: 0.9023 - loss: 0.2682 - val_accuracy: 0.7886 - val_loss: 0.6163 - learning_rate: 0.0010\nEpoch 10/25\n\n175/176 ━━━━━━━━━━━━━━━━━━━━ 0s 43ms/step - accuracy: 0.9175 - loss: 0.2444\nEpoch 10: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n\n176/176 ━━━━━━━━━━━━━━━━━━━━ 8s 46ms/step - accuracy: 0.9174 - loss: 0.2445 - val_accuracy: 0.7730 - val_loss: 0.7700 - learning_rate: 0.0010\nEpoch 11/25\n\n176/176 ━━━━━━━━━━━━━━━━━━━━ 8s 46ms/step - accuracy: 0.9203 - loss: 0.2280 - val_accuracy: 0.9281 - val_loss: 0.1967 - learning_rate: 5.0000e-04\nEpoch 12/25\n\n176/176 ━━━━━━━━━━━━━━━━━━━━ 8s 45ms/step - accuracy: 0.9359 - loss: 0.1835 - val_accuracy: 0.9438 - val_loss: 0.1712 - learning_rate: 5.0000e-04\nEpoch 13/25\n\n176/176 ━━━━━━━━━━━━━━━━━━━━ 8s 45ms/step - accuracy: 0.9305 - loss: 0.1916 - val_accuracy: 0.9310 - val_loss: 0.1867 - learning_rate: 5.0000e-04\nEpoch 14/25\n\n176/176 ━━━━━━━━━━━━━━━━━━━━ 8s 47ms/step - accuracy: 0.9459 - loss: 0.1691 - val_accuracy: 0.8085 - val_loss: 0.6087 - learning_rate: 5.0000e-04\nEpoch 15/25\n\n175/176 ━━━━━━━━━━━━━━━━━━━━ 0s 44ms/step - accuracy: 0.9428 - loss: 0.1746\nEpoch 15: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n\n176/176 ━━━━━━━━━━━━━━━━━━━━ 8s 47ms/step - accuracy: 0.9428 - loss: 0.1747 - val_accuracy: 0.9359 - val_loss: 0.2103 - learning_rate: 5.0000e-04\nEpoch 16/25\n\n176/176 ━━━━━━━━━━━━━━━━━━━━ 8s 46ms/step - accuracy: 0.9428 - loss: 0.1593 - val_accuracy: 0.9367 - val_loss: 0.1727 - learning_rate: 2.5000e-04\nEpoch 17/25\n\n176/176 ━━━━━━━━━━━━━━━━━━━━ 8s 47ms/step - accuracy: 0.9478 - loss: 0.1488 - val_accuracy: 0.9025 - val_loss: 0.3040 - learning_rate: 2.5000e-04\nEpoch 18/25\n\n176/176 ━━━━━━━━━━━━━━━━━━━━ 8s 47ms/step - accuracy: 0.9549 - loss: 0.1314 - val_accuracy: 0.9438 - val_loss: 0.1613 - learning_rate: 2.5000e-04\nEpoch 19/25\n\n176/176 ━━━━━━━━━━━━━━━━━━━━ 8s 47ms/step - accuracy: 0.9498 - loss: 0.1354 - val_accuracy: 0.9317 - val_loss: 0.2010 - learning_rate: 2.5000e-04\nEpoch 20/25\n\n176/176 ━━━━━━━━━━━━━━━━━━━━ 8s 47ms/step - accuracy: 0.9509 - loss: 0.1404 - val_accuracy: 0.9374 - val_loss: 0.1807 - learning_rate: 2.5000e-04\nEpoch 21/25\n\n176/176 ━━━━━━━━━━━━━━━━━━━━ 8s 48ms/step - accuracy: 0.9553 - loss: 0.1281 - val_accuracy: 0.9445 - val_loss: 0.1544 - learning_rate: 2.5000e-04\nEpoch 22/25\n\n176/176 ━━━━━━━━━━━━━━━━━━━━ 8s 47ms/step - accuracy: 0.9549 - loss: 0.1245 - val_accuracy: 0.9011 - val_loss: 0.3087 - learning_rate: 2.5000e-04\nEpoch 23/25\n\n176/176 ━━━━━━━━━━━━━━━━━━━━ 8s 47ms/step - accuracy: 0.9545 - loss: 0.1274 - val_accuracy: 0.8719 - val_loss: 0.3412 - learning_rate: 2.5000e-04\nEpoch 24/25\n\n175/176 ━━━━━━━━━━━━━━━━━━━━ 0s 45ms/step - accuracy: 0.9600 - loss: 0.1157\nEpoch 24: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n\n176/176 ━━━━━━━━━━━━━━━━━━━━ 8s 48ms/step - accuracy: 0.9600 - loss: 0.1157 - val_accuracy: 0.8819 - val_loss: 0.3936 - learning_rate: 2.5000e-04\nEpoch 25/25\n\n176/176 ━━━━━━━━━━━━━━━━━━━━ 8s 47ms/step - accuracy: 0.9620 - loss: 0.1083 - val_accuracy: 0.9345 - val_loss: 0.1687 - learning_rate: 1.2500e-04\n","metadata":{}},{"cell_type":"markdown","source":"# **5) Modeli Değerlendirmesi**\nEğitim tamamlandıktan sonra modelin test seti üzerindeki performansı değerlendirilmiştir. Bu aşamada accuracy, classification report ve confusion matrix gibi metrikler kullanılmış, ayrıca bazı doğru ve yanlış tahmin örnekleri görselleştirilmiştir.\n\n1) Test Seti Performansı\ncnn_model.evaluate fonksiyonu ile Test Loss ve Test Accuracy hesaplanmıştır.\nBu değerler, modelin hiç görmediği verilerdeki genel başarısını göstermektedir.\n2) Tahminlerin Hesaplanması\ncnn_model.predict ile test verisi üzerindeki sınıf olasılıkları elde edilmiştir.\nnp.argmax ile her örneğin en yüksek olasılığa sahip sınıfı tahmin sınıfı (y_pred) olarak seçilmiştir.\n3) Classification Report\nDoğruluk (precision), geri çağırma (recall), F1 skor ve destek (support) değerleri sınıf bazında hesaplanmıştır.\nclassification_report çıktısı, modelin hangi sınıflarda daha başarılı veya zayıf olduğunu ayrıntılı şekilde göstermektedir.\n4) Confusion Matrix\nConfusion matrix (karmaşıklık matrisi) ile modelin tahmin ettiği sınıflar ile gerçek etiketler karşılaştırılmıştır.\nseaborn.heatmap ile matris görselleştirilmiş, doğru tahminler çapraz köşekte, hatalı tahminler diğer hücrelerde gösterilmiştir.\n5) Doğru ve Yanlış Tahmin Örnekleri\nDoğru tahminler ve yanlış tahminler ayrı subplot’larda görselleştirilmiştir.\nHer görselin altında:\n\nModelin tahmini (Tahmin)\nGerçek sınıf (Gerçek)\nbilgisi yazdırılmıştır.\nBu değerlendirme adımı sayesinde modelin yalnızca genel doğruluğu değil, aynı zamanda hangi sınıflarda hata yaptığı ve hangi sınıfları doğru tanıdığı da ayrıntılı şekilde analiz edilmiştir.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\n\n# 1) Test seti performansı\ntest_loss, test_acc = cnn_model.evaluate(X_test, y_test_categorical, verbose=0)\nprint(f\"Test Loss: {test_loss:.4f}\")\nprint(f\"Test Accuracy: {test_acc:.4f}\")\n\n# 2) Tahminler\ny_pred_prob = cnn_model.predict(X_test, verbose=0)\ny_pred = np.argmax(y_pred_prob, axis=1)\n\n# 3) Classification Report\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred, target_names=class_names))\n\n# 4) Confusion Matrix\ncm = confusion_matrix(y_test, y_pred)\n\nplt.figure(figsize=(8,6))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n            xticklabels=class_names,\n            yticklabels=class_names)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.title(\"Confusion Matrix\")\nplt.show()\n\ncorrect_indices = np.where(y_pred == y_test)[0]\nincorrect_indices = np.where(y_pred != y_test)[0]\n\nplt.figure(figsize=(12, 8))\n\n# Bazı doğru ve yanlış tahmin örneklerini görselleştirelim\ncorrect_indices = np.where(y_pred == y_test)[0]\nincorrect_indices = np.where(y_pred != y_test)[0]\n\nplt.figure(figsize=(12, 8))\n\nfor i, correct in enumerate(correct_indices[:4]):\n    plt.subplot(2, 4, i+1)\n    plt.imshow(X_test[correct].squeeze(), cmap=\"gray\") # squeeze = (128,128,1) -> (128,128)\n    plt.title(f\"Tahmin: {class_names[y_pred[correct]]}\\nGerçek: {class_names[y_test[correct]]}\")\n    plt.axis('off')\n\n# Doğru tahmin örnekleri\nfor i, correct in enumerate(correct_indices[:4]):\n    plt.subplot(2, 4, i+1)\n    plt.imshow(X_test[correct])\n    plt.title(f\"Tahmin: {class_names[y_pred[correct]]}\\nGerçek: {class_names[y_test[correct]]}\")\n    plt.axis('off')\n\n# Yanlış tahmin örnekleri\nfor i, incorrect in enumerate(incorrect_indices[:4]):\n    plt.subplot(2, 4, i+5)\n    plt.imshow(X_test[incorrect])\n    plt.title(f\"Tahmin: {class_names[y_pred[incorrect]]}\\nGerçek: {class_names[y_test[incorrect]]}\")\n    plt.axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Test Loss: 0.1544\nTest Accuracy: 0.9445\n\nClassification Report:\n             *  precision    * recall  * f1-score   * support\n\n   * pituitary       0.* 90      0.99      0.94       352\n   * notumor       0.99      0.97      0.98       400\n   * meningioma       0.97      0.84      0.90       329\n   *     glioma       0.92      0.98      0.95       324\n  \n   *   accuracy                           0.94      1405\n   *  macro avg       0.94      0.94      0.94      1405\n   * weighted avg       0.95      0.94      0.94      1405","metadata":{}},{"cell_type":"markdown","source":"# 6) Grand-CAM Görselleştirme¶\nModelimizin karar verme sürecinde hangi bölgeleri dikkate aldığını anlamak için Grad-CAM (Gradient-weighted Class Activation Mapping) yöntemi uygulanmıştır. Bu yöntem, derin ağların yorumlanabilirliğini artırmak için yaygın olarak kullanılmaktadır.\n\n1) Grad-CAM’in Amacı\nCNN tabanlı modellerin hangi görüntü bölgelerine odaklandığını görselleştirmek.\nModelin doğru tahminleri gerçekten ilgili bölgelere bakarak mı yaptığı, yoksa ilgisiz alanlara mı odaklandığını analiz etmek.\nOverfitting, yanlış sınıflandırma veya veri hatalarını tespit etmek.\n2) Uygulama Adımları\nOrijinal Görüntü: MRI görüntüsü veri setinden alınmıştır.\nGrad-CAM Isı Haritası: Modelin sınıflandırmada en çok dikkate aldığı bölgeler renklendirilmiştir.\nBindirme (Overlay): Isı haritası, orijinal görüntü üzerine saydamlık ile bindirilerek daha anlaşılır bir görselleştirme elde edilmiştir.\n3) Parametreler\nLAST_CONV: Grad-CAM için kullanılan son Conv2D katmanı.\nCOLORMAP: Isı haritası renk paleti (viridis kullanılmıştır).\nALPHA: Isı haritasının orijinal görüntü üzerine bindirilirken kullanılan saydamlık oranı (0.45).\n4) Çıktı Görselleri\nOrijinal Görüntü: Modelin ham girdi resmi.\nGrad-CAM Isı Haritası: Modelin en çok önem verdiği bölgeler.\nBindirme (Overlay): Hem orijinal hem de Grad-CAM birlikte gösterilmiştir.\n\nBu görselleştirme sayesinde modelin hangi bölgeleri dikkate aldığı daha net bir şekilde anlaşılabilir ve modelin güvenilirliği değerlendirilebilir.","metadata":{}},{"cell_type":"code","source":"import os, random\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import models\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\n# ================= KULLANICI AYARLARI =================\n# Modelinizin eğitildiği görüntü boyutları ve kanal sayısı\nINPUT_H = 128   # Yükseklik (Height)\nINPUT_W = 128   # Genişlik (Width)\nINPUT_C = 1     # Kanal: Gri tonlama için 1, RGB için 3\n\n# Model özetinizdeki SON Conv2D katmanının adı. (Sizin için \"conv2d_14\" idi)\nLAST_CONV = \"conv2d_11\" \n\n# Görüntülerin bulunduğu ana dizin\nIMG_DIR = \"../input/brain-tumor-mri-dataset/Testing\" \n\n# Sınıf adlarınız.\nclass_names = ['meningioma', 'glioma', 'notumor', 'pituitary'] \n\n# Diğer Ayarlar\nCOLORMAP = \"viridis\" \nALPHA = 0.45 \nPREPROCESS = lambda x: x / 255.0 \nIMG_PATH = None \n# =======================================================\n\n\n# ---------------- Yardımcı Fonksiyonlar ----------------\ndef resolve_model():\n    # Modeli cnn_model adıyla bulmaya çalış\n    g = globals()\n    if 'cnn_model' in g and hasattr(g['cnn_model'], \"predict\"):\n        mdl = g['cnn_model']\n        \n        # Hata Giderme: Modeli bir dummy input ile çağırarak .input özelliğini zorla oluştur.\n        # Bu, Keras'ta sequential modelin yapısını kesinleştiren en agresif yoldur.\n        dummy_input = tf.zeros((1, INPUT_H, INPUT_W, INPUT_C)) \n        _ = mdl(dummy_input)\n        \n        print(f\"[INFO] Model: cnn_model bulundu ve yapısı zorla inşa edildi.\")\n        return mdl\n    raise RuntimeError(\"Eğitilmiş model RAM'de yok (cnn_model). Lütfen modeli eğitin.\")\n\ndef pick_image_path(img_path, img_dir):\n    # Dizin içinden rastgele bir görsel seç\n    if img_path and os.path.isfile(img_path): return img_path\n    cands = []\n    for root,_,files in os.walk(img_dir):\n        for f in files:\n            if f.lower().endswith((\".png\",\".jpg\",\".jpeg\",\".bmp\")):\n                cands.append(os.path.join(root,f))\n    if not cands: raise FileNotFoundError(f\"Uygun görsel bulunamadı: {img_dir}\")\n    return random.choice(cands)\n\n\n# ---------------- Hazırlık ve Yükleme ----------------\nmdl = resolve_model()\nimg_path = pick_image_path(IMG_PATH, IMG_DIR)\ntrue_label = os.path.basename(os.path.dirname(img_path)) \n\n# Görsel yükle ve ön işle\nif INPUT_C == 1:\n    orig = Image.open(img_path).convert(\"L\")\n    res  = orig.resize((INPUT_W, INPUT_H), resample=Image.BILINEAR)\n    inp  = np.array(res, dtype=np.float32)[..., np.newaxis]\n    orig_arr = np.array(orig, dtype=np.uint8) \nelse:\n    orig = Image.open(img_path).convert(\"RGB\")\n    res  = orig.resize((INPUT_W, INPUT_H), resample=Image.BILINEAR)\n    inp  = np.array(res, dtype=np.float32)\n    orig_arr = np.array(orig, dtype=np.uint8) \n\ninp_batch = PREPROCESS(np.expand_dims(inp, axis=0)) if PREPROCESS else np.expand_dims(inp, axis=0)\n\n\n# ---------------- Grad-CAM Hesaplama ----------------\n\nlast_conv_layer = mdl.get_layer(LAST_CONV)\nclassifier_layer = mdl.layers[-1] \n\n# Modelin giriş tensor'ını, modelin inputs listesinden alıyoruz.\n# Bu, .input hatasını çözmenin en güvenilir yoludur.\ngrad_model_func = tf.keras.models.Model(\n    inputs=mdl.inputs[0],\n    outputs=[last_conv_layer.output, classifier_layer.output]\n)\n\nwith tf.GradientTape() as tape:\n    conv_out, preds = grad_model_func(inp_batch)\n    pred_idx = int(tf.argmax(preds[0]).numpy())\n    class_channel = preds[:, pred_idx]\n    \ngrads = tape.gradient(class_channel, conv_out)\npooled = tf.reduce_mean(grads, axis=(0,1,2))\nconv_out = conv_out[0]\nheat = tf.tensordot(conv_out, pooled, axes=(2,0))\nheat = tf.nn.relu(heat)\nheat = (heat / (tf.reduce_max(heat) + 1e-8)).numpy().astype(np.float32)\n\n# Orijinal boyuta büyütme ve bindirme\nheat_img = Image.fromarray(np.uint8(255*heat))\nheat_big = np.array(heat_img.resize((orig.width, orig.height), resample=Image.BILINEAR), dtype=np.float32)/255.0\n\ncmap = plt.cm.get_cmap(COLORMAP)\nlut  = (cmap(np.arange(256))[:, :3] * 255).astype(np.uint8)\nheat_rgb = lut[np.uint8(255*np.clip(heat_big,0,1))]\n\nif INPUT_C == 1:\n    orig_rgb = np.stack([orig_arr, orig_arr, orig_arr], axis=-1)\nelse:\n    orig_rgb = orig_arr\n    \noverlay = (ALPHA * heat_rgb.astype(np.float32) + orig_rgb.astype(np.float32) * (1-ALPHA)).clip(0,255).astype(np.uint8)\n\n# Tahmin adı\npred_name = (class_names[pred_idx] if class_names and 0 <= pred_idx < len(class_names) else str(pred_idx))\n\n\n# ---------------- Çizim ----------------\nplt.figure(figsize=(15,4))\nplt.suptitle(f\"Tahmin: {pred_name} (Gerçek: {true_label})\", fontsize=16)\n\nplt.subplot(1,3,1)\nplt.imshow(orig_arr, cmap=\"gray\" if INPUT_C==1 else None)\nplt.title(\"Orijinal Görüntü\")\nplt.axis(\"off\")\n\nplt.subplot(1,3,2)\nplt.imshow(heat_big, cmap=COLORMAP)\nplt.title(\"Grad-CAM Isı Haritası\")\nplt.axis(\"off\")\n\nplt.subplot(1,3,3)\nplt.imshow(overlay)\nplt.title(\"Isı Haritası Bindirilmiş\")\nplt.axis(\"off\")\n\nplt.tight_layout(rect=[0, 0.03, 1, 0.95]); \nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# **6.1) Eigen-CAM (Eigen Class Activation Map) Görselleştirmesi¶**\nstandart Grad-CAM'in aksine, modelin karar mekanizmasını yorumlamak için sınıf gradyanlarına ihtiyaç duymayan güçlü bir alternatif sunar. Bu yöntem, modelin genel odak noktalarını daha bütünsel ve kararlı bir şekilde analiz etmemizi sağlar.\n\n* **Eigen-CAM’in Amacı ve Farkı**\nEigen-CAM, modelin tahmini hangi sınıfa ait olursa olsun, özellik haritalarının yapısındaki en baskın (dominant) bilgi bileşenlerini ortaya çıkarır.\n\nSınıf-Bağımsız Görselleştirme: Grad-CAM’den farklı olarak tek bir sınıfa ait gradyanları kullanmaz.\nOdak Noktası: Özellik haritalarının en baskın bileşenlerini (Principal Components) çıkararak modelin en çok önem verdiği genel bölgeleri belirler.\nGenellenebilirlik: Karmaşık veya zayıf eğitilmiş modellerde bile daha dengeli ve genellenebilir bir açıklama sağlar.\n* **Uygulama Adımları ve Bileşenler**\nBileşen\tAçıklama\nOrijinal Görüntü\tAnaliz için kullanılan test veri setinden alınan MRI görüntüsü.\nIsı Haritası Hesaplaması\tÖzellik haritaları üzerinde yapılan PCA/SVD (Temel Bileşen Analizi / Tekil Değer Ayrışımı) analizleri ile en baskın odak bölgeleri çıkartılmıştır.\nBindirme (Overlay)\tIsı haritası, orijinal görüntü üzerine %40 saydamlık (alpha=0.40) ile bindirilerek modelin dikkat ettiği alanların netleşmesi sağlanmıştır.\n* **Kullanılan Yöntemler ve Seçim Kriteri**\nEigen-CAM hesaplamasında, en doğru ve en temiz görseli elde etmek için farklı ağırlıklandırma yöntemleri test edilmiştir:\n\nSVD PC1: Singular Value Decomposition (SVD) ile hesaplanan ilk ana bileşen (PC1).\nCovariance Eigenvector: Aktivasyon haritalarının kovaryans matrisinin en baskın özvektörü.\nL2 Energy Map: Özellik haritalarının enerji yoğunluğu.\nSeçim Kriteri: Uygulanan tüm yöntemler arasından en yüksek kontrast aralığını (max-min farkını) sağlayan yöntem nihai Eigen-CAM görselleştirmesi için kullanılmıştır.\n\n* **Görselleştirme Parametreleri**\nParametre\tAçıklama\tDeğer\nLOW_PCT & HIGH_PCT\tGörselleştirmede kontrastı artırmak için kullanılan alt ve üst eşik yüzdeleri.\tÖrn: %60 – %99.5\nGAMMA\tIsı haritasının kontrastını güçlendirmek için kullanılan gamma düzeltme değeri.\t1.6\nTOP_PCT\tIsı haritasında maskelenmiş bölgelerin eşik değeri.\t%95\nCOLORMAP\tIsı haritasında kullanılan renk paleti.\tinferno\nALPHA\tOverlay için kullanılan şeffaflık oranı.\t%40\n* **Çıktı Görselleri**\nOrijinal Görüntü: MRI resmi.\nEigen-CAM Haritası: Modelin genel olarak en çok dikkat ettiği bölgelerin yoğunluk haritası.\nBindirme (Overlay): Orijinal görüntü ile Eigen-CAM haritasının birleşimi.\nBu yöntem, Grad-CAM’in aksine sınıf bağımsızdır ve modelin en çok dikkat ettiği bölgeleri daha bütünsel bir şekilde görselleştirir.","metadata":{}},{"cell_type":"code","source":"%matplotlib inline\nimport os, random\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import models\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\n# ================= KULLANICI AYARLARI =================\n# Modelinizin eğitildiği görüntü boyutları ve kanal sayısı\nINPUT_H = 128   # Yükseklik (Height)\nINPUT_W = 128   # Genişlik (Width)\nINPUT_C = 1     # Kanal: Gri tonlama için 1, RGB için 3\n\n# Model özetinizdeki SON Conv2D katmanının adı.\n# Önceki çıktılara göre doğru değer:\nLAST_CONV = \"conv2d_11\" \n\n# Görüntülerin bulunduğu ana dizin\nIMG_DIR = \"../input/brain-tumor-mri-dataset/Testing\" \n\n# Sınıf adlarınız.\nclass_names = ['meningioma', 'glioma', 'notumor', 'pituitary'] \n\n# Diğer Ayarlar\nCOLORMAP = \"viridis\" \nALPHA = 0.45 \nPREPROCESS = lambda x: x / 255.0 \nIMG_PATH = None \n# =======================================================\n\n\n# ---------------- Yardımcı Fonksiyonlar ----------------\ndef resolve_model():\n    # Modeli cnn_model adıyla bulmaya çalış\n    g = globals()\n    if 'cnn_model' in g and hasattr(g['cnn_model'], \"predict\"):\n        mdl = g['cnn_model']\n        \n        # Hata Giderme: Modeli dummy input ile çağırarak .input özelliğini zorla oluştur.\n        dummy_input = tf.zeros((1, INPUT_H, INPUT_W, INPUT_C)) \n        _ = mdl(dummy_input)\n        \n        print(f\"[INFO] Model: cnn_model bulundu ve yapısı zorla inşa edildi.\")\n        return mdl\n    raise RuntimeError(\"Eğitilmiş model RAM'de yok (cnn_model). Lütfen modeli eğitin.\")\n\ndef pick_image_path(img_path, img_dir):\n    # Dizin içinden rastgele bir görsel seç\n    if img_path and os.path.isfile(img_path): return img_path\n    cands = []\n    for root,_,files in os.walk(img_dir):\n        for f in files:\n            if f.lower().endswith((\".png\",\".jpg\",\".jpeg\",\".bmp\")):\n                cands.append(os.path.join(root,f))\n    if not cands: raise FileNotFoundError(f\"Uygun görsel bulunamadı: {img_dir}\")\n    return random.choice(cands)\n\n\n# ---------------- Hazırlık ve Yükleme ----------------\nmdl = resolve_model() # Model burada yükleniyor\nimg_path = pick_image_path(IMG_PATH, IMG_DIR)\ntrue_label = os.path.basename(os.path.dirname(img_path)) \n\n# Görsel yükle ve ön işle\nif INPUT_C == 1:\n    orig = Image.open(img_path).convert(\"L\")\n    res  = orig.resize((INPUT_W, INPUT_H), resample=Image.BILINEAR)\n    inp  = np.array(res, dtype=np.float32)[..., np.newaxis]\n    orig_arr = np.array(orig, dtype=np.uint8) \nelse:\n    orig = Image.open(img_path).convert(\"RGB\")\n    res  = orig.resize((INPUT_W, INPUT_H), resample=Image.BILINEAR)\n    inp  = np.array(res, dtype=np.float32)\n    orig_arr = np.array(orig, dtype=np.uint8) \n\ninp_batch = PREPROCESS(np.expand_dims(inp, axis=0)) if PREPROCESS else np.expand_dims(inp, axis=0)\n\n\n# ---------------- Eigen-CAM Hesaplama ----------------\n\nlast_conv_layer = mdl.get_layer(LAST_CONV)\n\n# 1. Sadece Conv katmanının çıktısını veren bir model oluştur.\neigen_model = tf.keras.models.Model(\n    inputs=mdl.inputs[0],\n    outputs=last_conv_layer.output\n)\n\n# Son Conv katmanının çıktılarını (aktivasyon haritalarını) al\nconv_out = eigen_model.predict(inp_batch)[0] \n\n# 2. Boyut İndirgeme (SVD)\n# (h, w, k) -> (h*w, k) olarak yeniden şekillendir\nreshaped_out = tf.reshape(conv_out, [-1, conv_out.shape[-1]]) \ns, u, v = tf.linalg.svd(reshaped_out)\n\n# 3. En Büyük Bileşeni Seç (Principal Component)\neigen_weights = v[:, 0]\n\n# 4. Isı Haritasını Oluştur (Eigen-CAM)\nheat = tf.tensordot(conv_out, eigen_weights, axes=(2, 0))\n\n# ReLU ve Normalizasyon uygula\nheat = tf.nn.relu(heat)\nheat = (heat / (tf.reduce_max(heat) + 1e-8)).numpy().astype(np.float32)\n\n# Orijinal boyuta büyütme ve bindirme\nheat_img = Image.fromarray(np.uint8(255*heat))\nheat_big = np.array(heat_img.resize((orig.width, orig.height), resample=Image.BILINEAR), dtype=np.float32)/255.0\n\ncmap = plt.cm.get_cmap(COLORMAP)\nlut  = (cmap(np.arange(256))[:, :3] * 255).astype(np.uint8)\nheat_rgb = lut[np.uint8(255*np.clip(heat_big,0,1))]\n\nif INPUT_C == 1:\n    orig_rgb = np.stack([orig_arr, orig_arr, orig_arr], axis=-1)\nelse:\n    orig_rgb = orig_arr\n    \noverlay = (ALPHA * heat_rgb.astype(np.float32) + orig_rgb.astype(np.float32) * (1-ALPHA)).clip(0,255).astype(np.uint8)\n\n# Tahmin adı\npreds = mdl.predict(inp_batch)\npred_idx = int(tf.argmax(preds[0]).numpy())\npred_name = (class_names[pred_idx] if class_names and 0 <= pred_idx < len(class_names) else str(pred_idx))\n\n\n# ---------------- Çizim ----------------\nplt.figure(figsize=(15,4))\nplt.suptitle(f\"Eigen-CAM Sonuçları | Tahmin: {pred_name} (Gerçek: {true_label})\", fontsize=16)\n\nplt.subplot(1,3,1)\nplt.imshow(orig_arr, cmap=\"gray\" if INPUT_C==1 else None)\nplt.title(\"Orijinal Görüntü\")\nplt.axis(\"off\")\n\nplt.subplot(1,3,2)\nplt.imshow(heat_big, cmap=COLORMAP)\nplt.title(\"Eigen-CAM Isı Haritası\")\nplt.axis(\"off\")\n\nplt.subplot(1,3,3)\nplt.imshow(overlay)\nplt.title(\"Isı Haritası Bindirilmiş\")\nplt.axis(\"off\")\n\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"[INFO] Model: cnn_model bulundu ve yapısı zorla inşa edildi.\n1/1 ━━━━━━━━━━━━━━━━━━━━ 1s 738ms/step\nI0000 00:00:1758842176.474588      19 cuda_solvers.cc:178] Creating GpuSolver handles for stream 0x171d5050\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 379ms/step\n","metadata":{}},{"cell_type":"markdown","source":"# 7) Hiperparametre Optimizasyonu (Resumable Random Search)\nAmaç:\nModelin performansını artırmak için farklı hiperparametre kombinasyonlarını denemek ve en iyi sonuç veren ayarları bulmak. Bu adımda Random Search yöntemi kullanılmıştır. Ayrıca süreç, state.json dosyası sayesinde kaldığı yerden devam edebilir (resumable).\n\n1) Kullanılan Hiperparametreler\nAşağıdaki parametreler farklı değerlerle rastgele seçilerek test edilmiştir:\n\nconv_blocks: Evrişim blok sayısı (2, 3, 4)\nbase_filters: İlk katmandaki filtre sayısı (16, 32, 64)\nkernel_size: Çekirdek boyutu (3, 5)\ndropout: Dropout oranı (0.2, 0.3, 0.4, 0.5)\ndense_units: Tam bağlı katmandaki nöron sayısı (64, 128, 256)\nl2_weight: L2 regularization katsayısı (1e-5, 1e-4, 5e-4, 1e-3)\noptimizer: Optimizasyon algoritması (Adam, AdamW, RMSProp)\nlr: Öğrenme oranı (1e-4, 3e-4, 1e-3, 2e-3)\nbatch_size: Mini-batch boyutu (16, 32, 64)\nuse_bn: Batch Normalization (True)\naugment: Veri artırma (True/False)\n\n2.  Eğitim Süreci\nEğitim sırasında EarlyStopping ve ReduceLROnPlateau callback’leri kullanılmıştır.\nHer deneme (trial) sonrası:\nEğitim ve doğrulama kayıpları (loss)\nEğitim ve doğrulama doğrulukları (accuracy)\nEn yüksek doğrulama başarımı (val_accuracy)\nHiperparametre ayarları kaydedilmiştir.\n\n3) Kayıt ve Devam Özelliği\nstate.json dosyası: Kaç deneme tamamlandığı, en iyi doğrulama başarımı ve hiperparametre ayarları saklanır.\nEğer işlem yarıda kesilirse, tekrar çalıştırıldığında en son kaldığı denemeden devam eder.\n\n4) Çıktılar\nHer deneme sonunda:\ntrial_X.pkl: Eğitim geçmişi (loss, accuracy değerleri)\ntrial_X_hp.json: Kullanılan hiperparametre kombinasyonu\ntrial_X_best.keras: O denemedeki en iyi model ağırlıkları\nEn iyi model ayrıca best_model.keras dosyasında saklanır.\n\n5) Sonuçların Görselleştirilmesi\nEğitim sonunda, en iyi denemenin:\nLoss grafiği (train vs validation)\nAccuracy grafiği (train vs validation)\nçizdirilerek overfitting/underfitting analizi yapılır.\nBu yaklaşım sayesinde, modelin en iyi performans veren hiperparametre seti elde edilmiş ve süreç otomatik olarak kayıt altına alınmıştır.","metadata":{}},{"cell_type":"code","source":"# ===================== RESUMABLE RANDOM SEARCH (Brain Tumor MRI) =====================\nimport os, json, pickle, random, numpy as np, warnings, logging\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"  # INFO/WARNING'i sustur\nimport tensorflow as tf\ntf.config.optimizer.set_experimental_options({\"layout_optimizer\": False})\ntf.config.optimizer.set_jit(False)\ntf.get_logger().setLevel(\"ERROR\"); warnings.filterwarnings(\"ignore\")\nlogging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, regularizers\nimport matplotlib.pyplot as plt\n\n# --------- Sabitler ---------\nDATA_ROOT = \"../input/brain-tumor-mri-dataset\"   # Kaggle dataset yolu\nTRAIN_DIR = os.path.join(DATA_ROOT, \"Training\")\nTEST_DIR  = os.path.join(DATA_ROOT, \"Testing\")\n\nIMG_SIZE = (128, 128)       # model giriş boyutu (H, W)\nCHANNELS = 1                # MRI gri ise 1; RGB istiyorsan 3 yap\nINPUT_SHAPE = (IMG_SIZE[0], IMG_SIZE[1], CHANNELS)\n\nEPOCHS  = 15                # hız için düşürdüm; ES zaten keser\nTRIALS  = 10                # toplam deneme hedefi\nSEED    = 42\nAUTOTUNE = tf.data.AUTOTUNE\ntf.random.set_seed(SEED); np.random.seed(SEED); random.seed(SEED)\n\nSTATE_PATH = \"state.json\"   # kaldığı yer/state bilgisi\nBEST_MODEL_PATH = \"best_model.keras\"\n\n# --------- 0) Veri Yükleme ---------\ndef get_datasets(img_size=IMG_SIZE, batch_size=32, channels=CHANNELS):\n    if not os.path.isdir(TRAIN_DIR):\n        raise FileNotFoundError(f\"TRAIN_DIR bulunamadı: {TRAIN_DIR}\")\n\n    color_mode = \"grayscale\" if channels == 1 else \"rgb\"\n    # Train + Val (split)\n    train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n        TRAIN_DIR, label_mode=\"int\", image_size=img_size, batch_size=batch_size,\n        validation_split=0.2, subset=\"training\", seed=SEED, color_mode=color_mode)\n    val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n        TRAIN_DIR, label_mode=\"int\", image_size=img_size, batch_size=batch_size,\n        validation_split=0.2, subset=\"validation\", seed=SEED, color_mode=color_mode)\n    # Test (tamamı)\n    test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n        TEST_DIR, label_mode=\"int\", image_size=img_size, batch_size=batch_size,\n        shuffle=False, seed=SEED, color_mode=color_mode)\n\n    class_names = train_ds.class_names\n    num_classes = len(class_names)\n\n    # Normalizasyon + cache/prefetch\n    norm = keras.layers.Rescaling(1./255)\n    train_ds = (train_ds.map(lambda x,y: (norm(x), y), num_parallel_calls=AUTOTUNE)\n                         .cache().prefetch(AUTOTUNE))\n    val_ds   = (val_ds.map(lambda x,y: (norm(x), y), num_parallel_calls=AUTOTUNE)\n                       .cache().prefetch(AUTOTUNE))\n    test_ds  = (test_ds.map(lambda x,y: (norm(x), y), num_parallel_calls=AUTOTUNE)\n                       .cache().prefetch(AUTOTUNE))\n\n    # Class weights (dengesizlik için)\n    counts = np.zeros(num_classes, dtype=np.int64)\n    for _, y in train_ds.unbatch().take(1_000_000):\n        counts[int(y.numpy())] += 1\n    total = counts.sum()\n    class_weights = {i: float(total/(num_classes*max(counts[i],1))) for i in range(num_classes)}\n    print(\"[INFO] class_names:\", class_names)\n    print(\"[INFO] class_counts:\", dict(zip(class_names, counts.tolist())))\n    print(\"[INFO] class_weights:\", class_weights)\n\n    return train_ds, val_ds, test_ds, num_classes, class_names, class_weights\n\n# --------- 1) Model Kurucu ---------\ndef build_model(input_shape=INPUT_SHAPE,\n                num_classes=4,\n                conv_blocks=3,\n                base_filters=32,\n                kernel_size=3,\n                dropout=0.3,\n                dense_units=128,\n                l2_weight=1e-4,\n                optimizer_name=\"adam\",\n                lr=1e-3,\n                use_bn=True,\n                augment=True):\n    wd = regularizers.l2(l2_weight)\n    inp = keras.Input(shape=input_shape)\n    x = inp\n\n    if augment:\n        x = layers.RandomFlip(\"horizontal\")(x)\n        x = layers.RandomRotation(0.05)(x)\n        x = layers.RandomZoom(0.10)(x)\n\n    filters = base_filters\n    for _ in range(conv_blocks):\n        x = layers.Conv2D(filters, kernel_size, padding=\"same\", use_bias=not use_bn,\n                          kernel_regularizer=wd)(x)\n        if use_bn: x = layers.BatchNormalization()(x)\n        x = layers.Activation(\"relu\")(x)\n\n        x = layers.Conv2D(filters, kernel_size, padding=\"same\", use_bias=not use_bn,\n                          kernel_regularizer=wd)(x)\n        if use_bn: x = layers.BatchNormalization()(x)\n        x = layers.Activation(\"relu\")(x)\n\n        x = layers.MaxPooling2D(2)(x)\n        x = layers.Dropout(dropout)(x)\n        filters = min(filters*2, 256)\n\n    x = layers.GlobalAveragePooling2D()(x)\n    x = layers.Dense(dense_units, activation=\"relu\", kernel_regularizer=wd)(x)\n    x = layers.Dropout(dropout)(x)\n    out = layers.Dense(num_classes, activation=\"softmax\", dtype=\"float32\")(x)  # mixed_precision için güvenli\n\n    model = keras.Model(inp, out)\n\n    opt_name = optimizer_name.lower()\n    if opt_name == \"adam\":\n        opt = keras.optimizers.Adam(learning_rate=lr)\n    elif opt_name == \"adamw\":\n        opt = keras.optimizers.AdamW(learning_rate=lr, weight_decay=l2_weight)\n    elif opt_name == \"rmsprop\":\n        opt = keras.optimizers.RMSprop(learning_rate=lr)\n    else:\n        opt = keras.optimizers.Adam(learning_rate=lr)\n\n    model.compile(optimizer=opt, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n    return model\n\n# --------- 2) HP Alanı ---------\nHP_SPACE = {\n    \"conv_blocks\":   [2,3,4],\n    \"base_filters\":  [16,32,64],\n    \"kernel_size\":   [3,5],\n    \"dropout\":       [0.2,0.3,0.4,0.5],\n    \"dense_units\":   [64,128,256],\n    \"l2_weight\":     [1e-5,1e-4,5e-4,1e-3],\n    \"optimizer\":     [\"adam\",\"adamw\",\"rmsprop\"],\n    \"lr\":            [1e-4, 3e-4, 1e-3, 2e-3],\n    \"batch_size\":    [16,32,64],\n    \"use_bn\":        [True],\n    \"augment\":       [True, False]\n}\n\n# --------- 3) State yükle/başlat ---------\nstate = {\"completed_trials\": 0, \"best_val_acc\": -1.0, \"best_hp\": None, \"best_batch\": 32}\nif os.path.exists(STATE_PATH):\n    try:\n        with open(STATE_PATH, \"r\") as f:\n            saved = json.load(f)\n            state.update(saved)\n        print(f\"[RESUME] Found state: {state}\")\n    except Exception as e:\n        print(\"[WARN] state.json okunamadı, sıfırdan başlayacak:\", e)\n\n# Veri pipeline (tek kez kuruluyor)\ntrain_ds, val_ds, test_ds, num_classes, class_names, class_weights = get_datasets(\n    img_size=IMG_SIZE, batch_size=32, channels=CHANNELS\n)\n\n# --------- 4) Random Search (kaldığı yerden) ---------\nstart_t = state[\"completed_trials\"] + 1\nend_t   = TRIALS\n\nfor t in range(start_t, end_t+1):\n    hp = {k: random.choice(v) for k,v in HP_SPACE.items()}\n    print(f\"\\n[Trial {t}/{TRIALS}] HP: {hp}\")\n\n    # re-batch: hp'deki batch_size'i gerçekten uygula\n    bs = hp[\"batch_size\"]\n    train_b = train_ds.unbatch().batch(bs).prefetch(AUTOTUNE)\n    val_b   = val_ds.unbatch().batch(bs).prefetch(AUTOTUNE)\n\n    model = build_model(\n        input_shape=INPUT_SHAPE,\n        num_classes=num_classes,\n        conv_blocks=hp[\"conv_blocks\"],\n        base_filters=hp[\"base_filters\"],\n        kernel_size=hp[\"kernel_size\"],\n        dropout=hp[\"dropout\"],\n        dense_units=hp[\"dense_units\"],\n        l2_weight=hp[\"l2_weight\"],\n        optimizer_name=hp[\"optimizer\"],\n        lr=hp[\"lr\"],\n        use_bn=hp[\"use_bn\"],\n        augment=hp[\"augment\"]\n    )\n\n    es   = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=6, restore_best_weights=True)\n    rlrop= keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-6)\n    csv  = keras.callbacks.CSVLogger(f\"trial_{t}.csv\", append=False)\n    ckpt = keras.callbacks.ModelCheckpoint(f\"trial_{t}_best.keras\", monitor=\"val_accuracy\",\n                                           save_best_only=True, mode=\"max\")\n\n    history = model.fit(\n        train_b,\n        validation_data=val_b,\n        epochs=EPOCHS,\n        callbacks=[es, rlrop, csv, ckpt],\n        class_weight=class_weights,\n        verbose=1\n    )\n\n    # trial çıktılarını kaydet\n    with open(f\"trial_{t}.pkl\", \"wb\") as f: pickle.dump(history.history, f)\n    with open(f\"trial_{t}_hp.json\", \"w\") as f: json.dump(hp, f)\n\n    val_acc = float(np.max(history.history[\"val_accuracy\"]))\n    print(f\"  --> best val_accuracy: {val_acc:.4f}\")\n\n    # en iyiyi güncelle\n    if val_acc > state[\"best_val_acc\"]:\n        state[\"best_val_acc\"] = val_acc\n        state[\"best_hp\"] = hp\n        state[\"best_batch\"] = bs\n        # trial checkpoint'ini \"best_model.keras\" olarak kopyala\n        try:\n            # bazı ortamlarda doğrudan kaydetmek daha güvenli:\n            model.save(BEST_MODEL_PATH)\n        except Exception:\n            pass\n        print(\"[BEST] Updated best model & HP.\")\n\n    # state'i güncelle (trial tamamlandı)\n    state[\"completed_trials\"] = t\n    with open(STATE_PATH, \"w\") as f: json.dump(state, f)\n    print(f\"[STATE] Saved: {state}\")\n\n# --------- 5) En iyi modeli test et + grafik çiz ---------\nprint(\"\\n=== EN İYİ SONUÇ (VAL) ===\")\nprint(\"Completed trials:\", state[\"completed_trials\"])\nprint(\"Best val_acc:\", state[\"best_val_acc\"])\nprint(\"Best HP:\", state[\"best_hp\"])\n\n# test seti için en iyi batch ile re-batch\nbest_bs = state.get(\"best_batch\", 32)\ntest_b  = test_ds.unbatch().batch(best_bs).prefetch(AUTOTUNE)\n\nbest_model = None\nif os.path.exists(BEST_MODEL_PATH):\n    try:\n        best_model = keras.models.load_model(BEST_MODEL_PATH)\n        test_loss, test_acc = best_model.evaluate(test_b, verbose=0)\n        print(\"Test Acc:\", float(test_acc), \"| Test Loss:\", float(test_loss))\n    except Exception as e:\n        print(\"[WARN] Best model yüklenemedi:\", e)\n\n# Son trial'in (ya da en iyi trial'in) grafiğini çiz\n# En iyi trial'in pkl dosyasını bulmaya çalış\nbest_hist = None\nif state[\"best_hp\"] is not None:\n    # best trial'i tahmin etmek için state dosyasından logları tara\n    # (pratikçe son trial grafiğini gösterelim; best grafiği istersen dosyadan yükle)\n    last_t = state[\"completed_trials\"]\n    try:\n        with open(f\"trial_{last_t}.pkl\",\"rb\") as f:\n            best_hist = pickle.load(f)\n    except Exception:\n        pass\n\nif best_hist is not None:\n    h = best_hist\n    plt.figure(figsize=(10,4))\n    plt.subplot(1,2,1); plt.plot(h[\"loss\"], label=\"train\"); plt.plot(h[\"val_loss\"], label=\"val\")\n    plt.title(\"Loss\"); plt.legend(); plt.grid(True)\n    plt.subplot(1,2,2); plt.plot(h[\"accuracy\"], label=\"train\"); plt.plot(h[\"val_accuracy\"], label=\"val\")\n    plt.title(\"Accuracy\"); plt.legend(); plt.grid(True)\n    plt.tight_layout(); plt.show()\nelse:\n    print(\"[INFO] Grafiği göstermek için ilgili trial_*.pkl bulunamadı.\")\n\n\n# Model başarıyla diske kaydedilir\ncnn_model.save('best_brain_tumor_model.keras')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Found 5712 files belonging to 4 classes.\nUsing 4570 files for training.\nFound 5712 files belonging to 4 classes.\nUsing 1142 files for validation.\nFound 1311 files belonging to 4 classes.\n[INFO] class_names: ['glioma', 'meningioma', 'notumor', 'pituitary']\n[INFO] class_counts: {'glioma': 1077, 'meningioma': 1090, 'notumor': 1247, 'pituitary': 1156}\n[INFO] class_weights: {0: 1.0608170844939646, 1: 1.048165137614679, 2: 0.9161988773055333, 3: 0.9883217993079585}\n\n[Trial 1/10] HP: {'conv_blocks': 4, 'base_filters': 16, 'kernel_size': 3, 'dropout': 0.4, 'dense_units': 64, 'l2_weight': 0.0001, 'optimizer': 'adam', 'lr': 0.0001, 'batch_size': 64, 'use_bn': True, 'augment': False}\nEpoch 1/15\n72/72 ━━━━━━━━━━━━━━━━━━━━ 33s 220ms/step - accuracy: 0.4129 - loss: 1.4132 - val_accuracy: 0.2137 - val_loss: 1.7594 - learning_rate: 1.0000e-04\nEpoch 2/15\n72/72 ━━━━━━━━━━━━━━━━━━━━ 4s 52ms/step - accuracy: 0.6032 - loss: 1.0156 - val_accuracy: 0.2137 - val_loss: 2.7612 - learning_rate: 1.0000e-04\nEpoch 3/15\n72/72 ━━━━━━━━━━━━━━━━━━━━ 4s 53ms/step - accuracy: 0.6516 - loss: 0.9145 - val_accuracy: 0.2137 - val_loss: 3.9308 - learning_rate: 1.0000e-04\nEpoch 4/15\n72/72 ━━━━━━━━━━━━━━━━━━━━ 4s 53ms/step - accuracy: 0.7018 - loss: 0.8228 - val_accuracy: 0.2137 - val_loss: 4.7791 - learning_rate: 1.0000e-04\nEpoch 5/15\n72/72 ━━━━━━━━━━━━━━━━━━━━ 4s 56ms/step - accuracy: 0.7266 - loss: 0.7742 - val_accuracy: 0.2163 - val_loss: 4.4130 - learning_rate: 5.0000e-05\nEpoch 6/15\n72/72 ━━━━━━━━━━━━━━━━━━━━ 4s 57ms/step - accuracy: 0.7389 - loss: 0.7446 - val_accuracy: 0.2294 - val_loss: 3.7184 - learning_rate: 5.0000e-05\nEpoch 7/15\n72/72 ━━━━━━━━━━━━━━━━━━━━ 4s 57ms/step - accuracy: 0.7456 - loss: 0.7270 - val_accuracy: 0.2356 - val_loss: 3.0898 - learning_rate: 5.0000e-05\n  --> best val_accuracy: 0.2356\n[BEST] Updated best model & HP.\n[STATE] Saved: {'completed_trials': 1, 'best_val_acc': 0.23555167019367218, 'best_hp': {'conv_blocks': 4, 'base_filters': 16, 'kernel_size': 3, 'dropout': 0.4, 'dense_units': 64, 'l2_weight': 0.0001, 'optimizer': 'adam', 'lr': 0.0001, 'batch_size': 64, 'use_bn': True, 'augment': False}, 'best_batch': 64}\n\n[Trial 2/10] HP: {'conv_blocks': 2, 'base_filters': 32, 'kernel_size': 5, 'dropout': 0.2, 'dense_units': 64, 'l2_weight': 0.001, 'optimizer': 'adamw', 'lr': 0.001, 'batch_size': 16, 'use_bn': True, 'augment': False}\nEpoch 1/15\n286/286 ━━━━━━━━━━━━━━━━━━━━ 27s 57ms/step - accuracy: 0.6148 - loss: 1.1461 - val_accuracy: 0.4037 - val_loss: 1.6097 - learning_rate: 0.0010\nEpoch 2/15\n286/286 ━━━━━━━━━━━━━━━━━━━━ 8s 30ms/step - accuracy: 0.7323 - loss: 0.8664 - val_accuracy: 0.5954 - val_loss: 1.0121 - learning_rate: 0.0010\nEpoch 3/15\n286/286 ━━━━━━━━━━━━━━━━━━━━ 8s 29ms/step - accuracy: 0.7522 - loss: 0.7930 - val_accuracy: 0.3450 - val_loss: 2.4849 - learning_rate: 0.0010\nEpoch 4/15\n286/286 ━━━━━━━━━━━━━━━━━━━━ 8s 30ms/step - accuracy: 0.7711 - loss: 0.7267 - val_accuracy: 0.3214 - val_loss: 2.8746 - learning_rate: 0.0010\nEpoch 5/15\n286/286 ━━━━━━━━━━━━━━━━━━━━ 9s 30ms/step - accuracy: 0.7805 - loss: 0.6957 - val_accuracy: 0.3573 - val_loss: 2.7299 - learning_rate: 0.0010\nEpoch 6/15\n286/286 ━━━━━━━━━━━━━━━━━━━━ 9s 30ms/step - accuracy: 0.8178 - loss: 0.6209 - val_accuracy: 0.3152 - val_loss: 4.2149 - learning_rate: 5.0000e-04\nEpoch 7/15\n286/286 ━━━━━━━━━━━━━━━━━━━━ 8s 30ms/step - accuracy: 0.8352 - loss: 0.5703 - val_accuracy: 0.5814 - val_loss: 1.1739 - learning_rate: 5.0000e-04\nEpoch 8/15\n286/286 ━━━━━━━━━━━━━━━━━━━━ 8s 30ms/step - accuracy: 0.8340 - loss: 0.5514 - val_accuracy: 0.3441 - val_loss: 3.4653 - learning_rate: 5.0000e-04\n  --> best val_accuracy: 0.5954\n[BEST] Updated best model & HP.\n[STATE] Saved: {'completed_trials': 2, 'best_val_acc': 0.5954465866088867, 'best_hp': {'conv_blocks': 2, 'base_filters': 32, 'kernel_size': 5, 'dropout': 0.2, 'dense_units': 64, 'l2_weight': 0.001, 'optimizer': 'adamw', 'lr': 0.001, 'batch_size': 16, 'use_bn': True, 'augment': False}, 'best_batch': 16}\n\n[Trial 3/10] HP: {'conv_blocks': 2, 'base_filters': 64, 'kernel_size': 5, 'dropout': 0.2, 'dense_units': 128, 'l2_weight': 1e-05, 'optimizer': 'rmsprop', 'lr': 0.001, 'batch_size': 64, 'use_bn': True, 'augment': True}\nEpoch 1/15\n72/72 ━━━━━━━━━━━━━━━━━━━━ 49s 508ms/step - accuracy: 0.5940 - loss: 1.0143 - val_accuracy: 0.2137 - val_loss: 2.9969 - learning_rate: 0.0010\nEpoch 2/15\n72/72 ━━━━━━━━━━━━━━━━━━━━ 30s 412ms/step - accuracy: 0.7213 - loss: 0.7440 - val_accuracy: 0.2137 - val_loss: 5.0125 - learning_rate: 0.0010\nEpoch 3/15\n72/72 ━━━━━━━━━━━━━━━━━━━━ 30s 414ms/step - accuracy: 0.7482 - loss: 0.6626 - val_accuracy: 0.2137 - val_loss: 5.8958 - learning_rate: 0.0010\nEpoch 4/15\n72/72 ━━━━━━━━━━━━━━━━━━━━ 31s 424ms/step - accuracy: 0.7601 - loss: 0.6241 - val_accuracy: 0.2145 - val_loss: 4.8494 - learning_rate: 0.0010\nEpoch 5/15\n72/72 ━━━━━━━━━━━━━━━━━━━━ 30s 414ms/step - accuracy: 0.7832 - loss: 0.5590 - val_accuracy: 0.2137 - val_loss: 3.7382 - learning_rate: 5.0000e-04\nEpoch 6/15\n72/72 ━━━━━━━━━━━━━━━━━━━━ 30s 415ms/step - accuracy: 0.8124 - loss: 0.5009 - val_accuracy: 0.3573 - val_loss: 1.8263 - learning_rate: 5.0000e-04\nEpoch 7/15\n72/72 ━━━━━━━━━━━━━━━━━━━━ 30s 419ms/step - accuracy: 0.8174 - loss: 0.4797 - val_accuracy: 0.6025 - val_loss: 0.9331 - learning_rate: 5.0000e-04\nEpoch 8/15\n72/72 ━━━━━━━━━━━━━━━━━━━━ 30s 421ms/step - accuracy: 0.8192 - loss: 0.4697 - val_accuracy: 0.6830 - val_loss: 0.7770 - learning_rate: 5.0000e-04\nEpoch 9/15\n72/72 ━━━━━━━━━━━━━━━━━━━━ 30s 421ms/step - accuracy: 0.8320 - loss: 0.4454 - val_accuracy: 0.5613 - val_loss: 1.0848 - learning_rate: 5.0000e-04\nEpoch 10/15\n72/72 ━━━━━━━━━━━━━━━━━━━━ 31s 426ms/step - accuracy: 0.8371 - loss: 0.4352 - val_accuracy: 0.7032 - val_loss: 0.8268 - learning_rate: 5.0000e-04\nEpoch 11/15\n72/72 ━━━━━━━━━━━━━━━━━━━━ 31s 425ms/step - accuracy: 0.8458 - loss: 0.4153 - val_accuracy: 0.7977 - val_loss: 0.5524 - learning_rate: 5.0000e-04\nEpoch 12/15\n72/72 ━━━━━━━━━━━━━━━━━━━━ 31s 424ms/step - accuracy: 0.8520 - loss: 0.3934 - val_accuracy: 0.6121 - val_loss: 1.0830 - learning_rate: 5.0000e-04\nEpoch 13/15\n72/72 ━━━━━━━━━━━━━━━━━━━━ 31s 424ms/step - accuracy: 0.8549 - loss: 0.3891 - val_accuracy: 0.6305 - val_loss: 1.1583 - learning_rate: 5.0000e-04\nEpoch 14/15\n72/72 ━━━━━━━━━━━━━━━━━━━━ 30s 424ms/step - accuracy: 0.8556 - loss: 0.3805 - val_accuracy: 0.4081 - val_loss: 2.3757 - learning_rate: 5.0000e-04\nEpoch 15/15\n72/72 ━━━━━━━━━━━━━━━━━━━━ 31s 424ms/step - accuracy: 0.8799 - loss: 0.3427 - val_accuracy: 0.7496 - val_loss: 0.6381 - learning_rate: 2.5000e-04\n  --> best val_accuracy: 0.7977\n[BEST] Updated best model & HP.\n[STATE] Saved: {'completed_trials': 3, 'best_val_acc': 0.7977232933044434, 'best_hp': {'conv_blocks': 2, 'base_filters': 64, 'kernel_size': 5, 'dropout': 0.2, 'dense_units': 128, 'l2_weight': 1e-05, 'optimizer': 'rmsprop', 'lr': 0.001, 'batch_size': 64, 'use_bn': True, 'augment': True}, 'best_batch': 64}\n\n[Trial 4/10] HP: {'conv_blocks': 3, 'base_filters': 32, 'kernel_size': 5, 'dropout': 0.4, 'dense_units': 64, 'l2_weight': 0.0005, 'optimizer': 'adamw', 'lr': 0.0003, 'batch_size': 64, 'use_bn': True, 'augment': True}\nEpoch 1/15\n72/72 ━━━━━━━━━━━━━━━━━━━━ 32s 284ms/step - accuracy: 0.5354 - loss: 1.3424 - val_accuracy: 0.2137 - val_loss: 2.5067 - learning_rate: 3.0000e-04\nEpoch 2/15\n72/72 ━━━━━━━━━━━━━━━━━━━━ 17s 236ms/step - accuracy: 0.7114 - loss: 0.9602 - val_accuracy: 0.2137 - val_loss: 2.5762 - learning_rate: 3.0000e-04\nEpoch 3/15\n72/72 ━━━━━━━━━━━━━━━━━━━━ 17s 230ms/step - accuracy: 0.7486 - loss: 0.8676 - val_accuracy: 0.2137 - val_loss: 2.8505 - learning_rate: 3.0000e-04\nEpoch 4/15\n72/72 ━━━━━━━━━━━━━━━━━━━━ 17s 230ms/step - accuracy: 0.7842 - loss: 0.7873 - val_accuracy: 0.2154 - val_loss: 2.7930 - learning_rate: 3.0000e-04\nEpoch 5/15\n72/72 ━━━━━━━━━━━━━━━━━━━━ 17s 231ms/step - accuracy: 0.8122 - loss: 0.7138 - val_accuracy: 0.2933 - val_loss: 2.8943 - learning_rate: 1.5000e-04\nEpoch 6/15\n72/72 ━━━━━━━━━━━━━━━━━━━━ 17s 231ms/step - accuracy: 0.8276 - loss: 0.6659 - val_accuracy: 0.2846 - val_loss: 2.7525 - learning_rate: 1.5000e-04\nEpoch 7/15\n72/72 ━━━━━━━━━━━━━━━━━━━━ 17s 234ms/step - accuracy: 0.8369 - loss: 0.6413 - val_accuracy: 0.3074 - val_loss: 3.0430 - learning_rate: 1.5000e-04\n  --> best val_accuracy: 0.3074\n[STATE] Saved: {'completed_trials': 4, 'best_val_acc': 0.7977232933044434, 'best_hp': {'conv_blocks': 2, 'base_filters': 64, 'kernel_size': 5, 'dropout': 0.2, 'dense_units': 128, 'l2_weight': 1e-05, 'optimizer': 'rmsprop', 'lr': 0.001, 'batch_size': 64, 'use_bn': True, 'augment': True}, 'best_batch': 64}\n\n[Trial 5/10] HP: {'conv_blocks': 4, 'base_filters': 32, 'kernel_size': 3, 'dropout': 0.3, 'dense_units': 64, 'l2_weight': 0.0005, 'optimizer': 'adamw', 'lr': 0.001, 'batch_size': 16, 'use_bn': True, 'augment': False}\nEpoch 1/15\n286/286 ━━━━━━━━━━━━━━━━━━━━ 37s 71ms/step - accuracy: 0.5777 - loss: 1.4279 - val_accuracy: 0.2636 - val_loss: 3.0081 - learning_rate: 0.0010\nEpoch 2/15\n286/286 ━━━━━━━━━━━━━━━━━━━━ 9s 31ms/step - accuracy: 0.7331 - loss: 0.9934 - val_accuracy: 0.6042 - val_loss: 1.3732 - learning_rate: 0.0010\nEpoch 3/15\n286/286 ━━━━━━━━━━━━━━━━━━━━ 9s 32ms/step - accuracy: 0.7794 - loss: 0.8419 - val_accuracy: 0.8039 - val_loss: 0.7303 - learning_rate: 0.0010\nEpoch 4/15\n286/286 ━━━━━━━━━━━━━━━━━━━━ 9s 31ms/step - accuracy: 0.8047 - loss: 0.7312 - val_accuracy: 0.7452 - val_loss: 0.8618 - learning_rate: 0.0010\nEpoch 5/15\n286/286 ━━━━━━━━━━━━━━━━━━━━ 9s 30ms/step - accuracy: 0.8358 - loss: 0.6787 - val_accuracy: 0.7574 - val_loss: 0.8269 - learning_rate: 0.0010\nEpoch 6/15\n286/286 ━━━━━━━━━━━━━━━━━━━━ 8s 30ms/step - accuracy: 0.8365 - loss: 0.6476 - val_accuracy: 0.6217 - val_loss: 1.3094 - learning_rate: 0.0010\nEpoch 7/15\n286/286 ━━━━━━━━━━━━━━━━━━━━ 8s 29ms/step - accuracy: 0.8679 - loss: 0.5627 - val_accuracy: 0.6716 - val_loss: 1.0657 - learning_rate: 5.0000e-04\nEpoch 8/15\n286/286 ━━━━━━━━━━━━━━━━━━━━ 9s 30ms/step - accuracy: 0.8752 - loss: 0.5128 - val_accuracy: 0.8608 - val_loss: 0.5397 - learning_rate: 5.0000e-04\nEpoch 9/15\n286/286 ━━━━━━━━━━━━━━━━━━━━ 8s 29ms/step - accuracy: 0.8895 - loss: 0.4865 - val_accuracy: 0.8599 - val_loss: 0.5867 - learning_rate: 5.0000e-04\nEpoch 10/15\n286/286 ━━━━━━━━━━━━━━━━━━━━ 8s 30ms/step - accuracy: 0.8910 - loss: 0.4613 - val_accuracy: 0.8336 - val_loss: 0.6052 - learning_rate: 5.0000e-04\nEpoch 11/15\n286/286 ━━━━━━━━━━━━━━━━━━━━ 9s 30ms/step - accuracy: 0.8991 - loss: 0.4423 - val_accuracy: 0.7539 - val_loss: 0.8001 - learning_rate: 5.0000e-04\nEpoch 12/15\n286/286 ━━━━━━━━━━━━━━━━━━━━ 9s 31ms/step - accuracy: 0.9143 - loss: 0.3886 - val_accuracy: 0.9142 - val_loss: 0.3706 - learning_rate: 2.5000e-04\nEpoch 13/15\n286/286 ━━━━━━━━━━━━━━━━━━━━ 9s 31ms/step - accuracy: 0.9298 - loss: 0.3522 - val_accuracy: 0.9203 - val_loss: 0.3889 - learning_rate: 2.5000e-04\nEpoch 14/15\n286/286 ━━━━━━━━━━━━━━━━━━━━ 9s 30ms/step - accuracy: 0.9293 - loss: 0.3364 - val_accuracy: 0.8809 - val_loss: 0.4974 - learning_rate: 2.5000e-04\nEpoch 15/15\n286/286 ━━━━━━━━━━━━━━━━━━━━ 9s 30ms/step - accuracy: 0.9375 - loss: 0.3293 - val_accuracy: 0.9072 - val_loss: 0.3793 - learning_rate: 2.5000e-04\n  --> best val_accuracy: 0.9203\n[BEST] Updated best model & HP.\n[STATE] Saved: {'completed_trials': 5, 'best_val_acc': 0.9203152656555176, 'best_hp': {'conv_blocks': 4, 'base_filters': 32, 'kernel_size': 3, 'dropout': 0.3, 'dense_units': 64, 'l2_weight': 0.0005, 'optimizer': 'adamw', 'lr': 0.001, 'batch_size': 16, 'use_bn': True, 'augment': False}, 'best_batch': 16}\n\n[Trial 6/10] HP: {'conv_blocks': 3, 'base_filters': 64, 'kernel_size': 5, 'dropout': 0.5, 'dense_units': 128, 'l2_weight': 0.0001, 'optimizer': 'adam', 'lr': 0.002, 'batch_size': 16, 'use_bn': True, 'augment': True}\nEpoch 1/15\n286/286 ━━━━━━━━━━━━━━━━━━━━ 54s 153ms/step - accuracy: 0.5206 - loss: 1.2707 - val_accuracy: 0.2636 - val_loss: 1.9410 - learning_rate: 0.0020\nEpoch 2/15\n286/286 ━━━━━━━━━━━━━━━━━━━━ 40s 139ms/step - accuracy: 0.6295 - loss: 1.0342 - val_accuracy: 0.5639 - val_loss: 1.5032 - learning_rate: 0.0020\nEpoch 3/15\n286/286 ━━━━━━━━━━━━━━━━━━━━ 40s 140ms/step - accuracy: 0.6726 - loss: 0.9346 - val_accuracy: 0.3047 - val_loss: 14.9674 - learning_rate: 0.0020\nEpoch 4/15\n286/286 ━━━━━━━━━━━━━━━━━━━━ 40s 141ms/step - accuracy: 0.6860 - loss: 0.8954 - val_accuracy: 0.5771 - val_loss: 1.1826 - learning_rate: 0.0020\nEpoch 5/15\n286/286 ━━━━━━━━━━━━━━━━━━━━ 40s 140ms/step - accuracy: 0.7010 - loss: 0.8971 - val_accuracy: 0.4054 - val_loss: 2.3080 - learning_rate: 0.0020\nEpoch 6/15\n286/286 ━━━━━━━━━━━━━━━━━━━━ 40s 139ms/step - accuracy: 0.7027 - loss: 0.8726 - val_accuracy: 0.3599 - val_loss: 2.4974 - learning_rate: 0.0020\nEpoch 7/15\n286/286 ━━━━━━━━━━━━━━━━━━━━ 40s 139ms/step - accuracy: 0.6906 - loss: 0.8735 - val_accuracy: 0.4834 - val_loss: 1.4892 - learning_rate: 0.0020\nEpoch 8/15\n286/286 ━━━━━━━━━━━━━━━━━━━━ 40s 140ms/step - accuracy: 0.7379 - loss: 0.7876 - val_accuracy: 0.5709 - val_loss: 1.2434 - learning_rate: 0.0010\nEpoch 9/15\n286/286 ━━━━━━━━━━━━━━━━━━━━ 40s 140ms/step - accuracy: 0.7450 - loss: 0.7436 - val_accuracy: 0.5315 - val_loss: 1.6234 - learning_rate: 0.0010\nEpoch 10/15\n286/286 ━━━━━━━━━━━━━━━━━━━━ 40s 140ms/step - accuracy: 0.7567 - loss: 0.7336 - val_accuracy: 0.7119 - val_loss: 0.8531 - learning_rate: 0.0010\nEpoch 11/15\n286/286 ━━━━━━━━━━━━━━━━━━━━ 40s 139ms/step - accuracy: 0.7735 - loss: 0.6953 - val_accuracy: 0.6708 - val_loss: 1.0010 - learning_rate: 0.0010\nEpoch 12/15\n286/286 ━━━━━━━━━━━━━━━━━━━━ 40s 140ms/step - accuracy: 0.7753 - loss: 0.6882 - val_accuracy: 0.6926 - val_loss: 0.9712 - learning_rate: 0.0010\nEpoch 13/15\n286/286 ━━━━━━━━━━━━━━━━━━━━ 40s 140ms/step - accuracy: 0.7887 - loss: 0.6819 - val_accuracy: 0.7198 - val_loss: 0.8385 - learning_rate: 0.0010\nEpoch 14/15\n286/286 ━━━━━━━━━━━━━━━━━━━━ 40s 141ms/step - accuracy: 0.8038 - loss: 0.6550 - val_accuracy: 0.8135 - val_loss: 0.6075 - learning_rate: 0.0010\nEpoch 15/15\n286/286 ━━━━━━━━━━━━━━━━━━━━ 40s 139ms/step - accuracy: 0.8014 - loss: 0.6409 - val_accuracy: 0.7382 - val_loss: 0.8072 - learning_rate: 0.0010\n  --> best val_accuracy: 0.8135\n[STATE] Saved: {'completed_trials': 6, 'best_val_acc': 0.9203152656555176, 'best_hp': {'conv_blocks': 4, 'base_filters': 32, 'kernel_size': 3, 'dropout': 0.3, 'dense_units': 64, 'l2_weight': 0.0005, 'optimizer': 'adamw', 'lr': 0.001, 'batch_size': 16, 'use_bn': True, 'augment': False}, 'best_batch': 16}\n\n[Trial 7/10] HP: {'conv_blocks': 2, 'base_filters': 64, 'kernel_size': 3, 'dropout': 0.4, 'dense_units': 256, 'l2_weight': 0.0005, 'optimizer': 'adam', 'lr': 0.001, 'batch_size': 32, 'use_bn': True, 'augment': False}\nEpoch 1/15\n143/143 ━━━━━━━━━━━━━━━━━━━━ 46s 203ms/step - accuracy: 0.6082 - loss: 1.1692 - val_accuracy: 0.2137 - val_loss: 4.1783 - learning_rate: 0.0010\nEpoch 2/15\n143/143 ━━━━━━━━━━━━━━━━━━━━ 12s 86ms/step - accuracy: 0.7474 - loss: 0.8446 - val_accuracy: 0.2137 - val_loss: 4.1443 - learning_rate: 0.0010\nEpoch 3/15\n143/143 ━━━━━━━━━━━━━━━━━━━━ 12s 87ms/step - accuracy: 0.7801 - loss: 0.7395 - val_accuracy: 0.2426 - val_loss: 2.5162 - learning_rate: 0.0010\nEpoch 4/15\n143/143 ━━━━━━━━━━━━━━━━━━━━ 12s 87ms/step - accuracy: 0.8018 - loss: 0.6745 - val_accuracy: 0.3581 - val_loss: 1.4068 - learning_rate: 0.0010\nEpoch 5/15\n143/143 ━━━━━━━━━━━━━━━━━━━━ 12s 87ms/step - accuracy: 0.8166 - loss: 0.6374 - val_accuracy: 0.5070 - val_loss: 1.5946 - learning_rate: 0.0010\nEpoch 6/15\n143/143 ━━━━━━━━━━━━━━━━━━━━ 12s 87ms/step - accuracy: 0.8418 - loss: 0.5883 - val_accuracy: 0.7723 - val_loss: 0.7373 - learning_rate: 0.0010\nEpoch 7/15\n143/143 ━━━━━━━━━━━━━━━━━━━━ 12s 86ms/step - accuracy: 0.8418 - loss: 0.5600 - val_accuracy: 0.4895 - val_loss: 1.4753 - learning_rate: 0.0010\nEpoch 8/15\n143/143 ━━━━━━━━━━━━━━━━━━━━ 12s 86ms/step - accuracy: 0.8534 - loss: 0.5314 - val_accuracy: 0.5569 - val_loss: 1.4591 - learning_rate: 0.0010\nEpoch 9/15\n143/143 ━━━━━━━━━━━━━━━━━━━━ 12s 86ms/step - accuracy: 0.8545 - loss: 0.5264 - val_accuracy: 0.6331 - val_loss: 1.0190 - learning_rate: 0.0010\nEpoch 10/15\n143/143 ━━━━━━━━━━━━━━━━━━━━ 12s 87ms/step - accuracy: 0.8819 - loss: 0.4573 - val_accuracy: 0.8433 - val_loss: 0.5229 - learning_rate: 5.0000e-04\nEpoch 11/15\n143/143 ━━━━━━━━━━━━━━━━━━━━ 12s 86ms/step - accuracy: 0.8858 - loss: 0.4409 - val_accuracy: 0.6646 - val_loss: 0.9665 - learning_rate: 5.0000e-04\nEpoch 12/15\n143/143 ━━━━━━━━━━━━━━━━━━━━ 12s 86ms/step - accuracy: 0.8820 - loss: 0.4272 - val_accuracy: 0.5718 - val_loss: 1.5717 - learning_rate: 5.0000e-04\nEpoch 13/15\n143/143 ━━━━━━━━━━━━━━━━━━━━ 12s 86ms/step - accuracy: 0.8866 - loss: 0.4227 - val_accuracy: 0.6734 - val_loss: 0.9778 - learning_rate: 5.0000e-04\nEpoch 14/15\n143/143 ━━━━━━━━━━━━━━━━━━━━ 12s 87ms/step - accuracy: 0.8941 - loss: 0.3991 - val_accuracy: 0.8722 - val_loss: 0.4746 - learning_rate: 2.5000e-04\nEpoch 15/15\n143/143 ━━━━━━━━━━━━━━━━━━━━ 12s 86ms/step - accuracy: 0.8972 - loss: 0.3878 - val_accuracy: 0.8529 - val_loss: 0.4755 - learning_rate: 2.5000e-04\n  --> best val_accuracy: 0.8722\n[STATE] Saved: {'completed_trials': 7, 'best_val_acc': 0.9203152656555176, 'best_hp': {'conv_blocks': 4, 'base_filters': 32, 'kernel_size': 3, 'dropout': 0.3, 'dense_units': 64, 'l2_weight': 0.0005, 'optimizer': 'adamw', 'lr': 0.001, 'batch_size': 16, 'use_bn': True, 'augment': False}, 'best_batch': 16}\n\n[Trial 8/10] HP: {'conv_blocks': 2, 'base_filters': 64, 'kernel_size': 5, 'dropout': 0.3, 'dense_units': 64, 'l2_weight': 0.0005, 'optimizer': 'adam', 'lr': 0.0001, 'batch_size': 64, 'use_bn': True, 'augment': False}\nEpoch 1/15\n72/72 ━━━━━━━━━━━━━━━━━━━━ 74s 603ms/step - accuracy: 0.5465 - loss: 1.3035 - val_accuracy: 0.2137 - val_loss: 1.6655 - learning_rate: 1.0000e-04\nEpoch 2/15\n72/72 ━━━━━━━━━━━━━━━━━━━━ 20s 275ms/step - accuracy: 0.6967 - loss: 0.9970 - val_accuracy: 0.2137 - val_loss: 1.8932 - learning_rate: 1.0000e-04\nEpoch 3/15\n72/72 ━━━━━━━━━━━━━━━━━━━━ 20s 277ms/step - accuracy: 0.7467 - loss: 0.8784 - val_accuracy: 0.2137 - val_loss: 2.3230 - learning_rate: 1.0000e-04\nEpoch 4/15\n72/72 ━━━━━━━━━━━━━━━━━━━━ 20s 276ms/step - accuracy: 0.7634 - loss: 0.8191 - val_accuracy: 0.2137 - val_loss: 2.3065 - learning_rate: 1.0000e-04\nEpoch 5/15\n72/72 ━━━━━━━━━━━━━━━━━━━━ 20s 278ms/step - accuracy: 0.7836 - loss: 0.7666 - val_accuracy: 0.2163 - val_loss: 2.0394 - learning_rate: 5.0000e-05\nEpoch 6/15\n72/72 ━━━━━━━━━━━━━━━━━━━━ 20s 277ms/step - accuracy: 0.8112 - loss: 0.7185 - val_accuracy: 0.3179 - val_loss: 1.7717 - learning_rate: 5.0000e-05\nEpoch 7/15\n72/72 ━━━━━━━━━━━━━━━━━━━━ 20s 278ms/step - accuracy: 0.8236 - loss: 0.6929 - val_accuracy: 0.4834 - val_loss: 1.5125 - learning_rate: 5.0000e-05\nEpoch 8/15\n72/72 ━━━━━━━━━━━━━━━━━━━━ 20s 278ms/step - accuracy: 0.8263 - loss: 0.6770 - val_accuracy: 0.5412 - val_loss: 1.2836 - learning_rate: 5.0000e-05\nEpoch 9/15\n72/72 ━━━━━━━━━━━━━━━━━━━━ 20s 278ms/step - accuracy: 0.8296 - loss: 0.6563 - val_accuracy: 0.6208 - val_loss: 1.1065 - learning_rate: 5.0000e-05\nEpoch 10/15\n72/72 ━━━━━━━━━━━━━━━━━━━━ 20s 279ms/step - accuracy: 0.8431 - loss: 0.6423 - val_accuracy: 0.6988 - val_loss: 0.9271 - learning_rate: 5.0000e-05\nEpoch 11/15\n72/72 ━━━━━━━━━━━━━━━━━━━━ 20s 278ms/step - accuracy: 0.8506 - loss: 0.6145 - val_accuracy: 0.7040 - val_loss: 0.8873 - learning_rate: 5.0000e-05\nEpoch 12/15\n72/72 ━━━━━━━━━━━━━━━━━━━━ 20s 278ms/step - accuracy: 0.8426 - loss: 0.6236 - val_accuracy: 0.8179 - val_loss: 0.6782 - learning_rate: 5.0000e-05\nEpoch 13/15\n72/72 ━━━━━━━━━━━━━━━━━━━━ 20s 277ms/step - accuracy: 0.8520 - loss: 0.6065 - val_accuracy: 0.7741 - val_loss: 0.7469 - learning_rate: 5.0000e-05\nEpoch 14/15\n72/72 ━━━━━━━━━━━━━━━━━━━━ 20s 279ms/step - accuracy: 0.8561 - loss: 0.5886 - val_accuracy: 0.8292 - val_loss: 0.6561 - learning_rate: 5.0000e-05\nEpoch 15/15\n72/72 ━━━━━━━━━━━━━━━━━━━━ 20s 277ms/step - accuracy: 0.8602 - loss: 0.5844 - val_accuracy: 0.8205 - val_loss: 0.6526 - learning_rate: 5.0000e-05\n  --> best val_accuracy: 0.8292\n[STATE] Saved: {'completed_trials': 8, 'best_val_acc': 0.9203152656555176, 'best_hp': {'conv_blocks': 4, 'base_filters': 32, 'kernel_size': 3, 'dropout': 0.3, 'dense_units': 64, 'l2_weight': 0.0005, 'optimizer': 'adamw', 'lr': 0.001, 'batch_size': 16, 'use_bn': True, 'augment': False}, 'best_batch': 16}\n\n[Trial 9/10] HP: {'conv_blocks': 2, 'base_filters': 16, 'kernel_size': 3, 'dropout': 0.2, 'dense_units': 256, 'l2_weight': 0.001, 'optimizer': 'adam', 'lr': 0.0003, 'batch_size': 16, 'use_bn': True, 'augment': True}\nEpoch 1/15\n286/286 ━━━━━━━━━━━━━━━━━━━━ 13s 26ms/step - accuracy: 0.5237 - loss: 1.2710 - val_accuracy: 0.2137 - val_loss: 1.9234 - learning_rate: 3.0000e-04\nEpoch 2/15\n286/286 ━━━━━━━━━━━━━━━━━━━━ 7s 24ms/step - accuracy: 0.6645 - loss: 0.9766 - val_accuracy: 0.6743 - val_loss: 0.9708 - learning_rate: 3.0000e-04\nEpoch 3/15\n286/286 ━━━━━━━━━━━━━━━━━━━━ 7s 24ms/step - accuracy: 0.6978 - loss: 0.8982 - val_accuracy: 0.4501 - val_loss: 1.4781 - learning_rate: 3.0000e-04\nEpoch 4/15\n286/286 ━━━━━━━━━━━━━━━━━━━━ 7s 24ms/step - accuracy: 0.7196 - loss: 0.8417 - val_accuracy: 0.4615 - val_loss: 1.4940 - learning_rate: 3.0000e-04\nEpoch 5/15\n286/286 ━━━━━━━━━━━━━━━━━━━━ 7s 24ms/step - accuracy: 0.7266 - loss: 0.8065 - val_accuracy: 0.5245 - val_loss: 1.3221 - learning_rate: 3.0000e-04\nEpoch 6/15\n286/286 ━━━━━━━━━━━━━━━━━━━━ 7s 24ms/step - accuracy: 0.7446 - loss: 0.7499 - val_accuracy: 0.6760 - val_loss: 0.8383 - learning_rate: 1.5000e-04\nEpoch 7/15\n286/286 ━━━━━━━━━━━━━━━━━━━━ 7s 24ms/step - accuracy: 0.7512 - loss: 0.7426 - val_accuracy: 0.6620 - val_loss: 0.8871 - learning_rate: 1.5000e-04\nEpoch 8/15\n286/286 ━━━━━━━━━━━━━━━━━━━━ 7s 24ms/step - accuracy: 0.7660 - loss: 0.7214 - val_accuracy: 0.6121 - val_loss: 1.0709 - learning_rate: 1.5000e-04\nEpoch 9/15\n286/286 ━━━━━━━━━━━━━━━━━━━━ 7s 24ms/step - accuracy: 0.7657 - loss: 0.7058 - val_accuracy: 0.5665 - val_loss: 1.1089 - learning_rate: 1.5000e-04\nEpoch 10/15\n286/286 ━━━━━━━━━━━━━━━━━━━━ 7s 24ms/step - accuracy: 0.7761 - loss: 0.6831 - val_accuracy: 0.6804 - val_loss: 0.9012 - learning_rate: 7.5000e-05\nEpoch 11/15\n286/286 ━━━━━━━━━━━━━━━━━━━━ 7s 23ms/step - accuracy: 0.7735 - loss: 0.6845 - val_accuracy: 0.6611 - val_loss: 0.9150 - learning_rate: 7.5000e-05\nEpoch 12/15\n286/286 ━━━━━━━━━━━━━━━━━━━━ 7s 24ms/step - accuracy: 0.7809 - loss: 0.6649 - val_accuracy: 0.6690 - val_loss: 0.9434 - learning_rate: 7.5000e-05\n  --> best val_accuracy: 0.6804\n[STATE] Saved: {'completed_trials': 9, 'best_val_acc': 0.9203152656555176, 'best_hp': {'conv_blocks': 4, 'base_filters': 32, 'kernel_size': 3, 'dropout': 0.3, 'dense_units': 64, 'l2_weight': 0.0005, 'optimizer': 'adamw', 'lr': 0.001, 'batch_size': 16, 'use_bn': True, 'augment': False}, 'best_batch': 16}\n\n[Trial 10/10] HP: {'conv_blocks': 4, 'base_filters': 32, 'kernel_size': 5, 'dropout': 0.4, 'dense_units': 128, 'l2_weight': 0.001, 'optimizer': 'adam', 'lr': 0.0003, 'batch_size': 16, 'use_bn': True, 'augment': False}\nEpoch 1/15\n286/286 ━━━━━━━━━━━━━━━━━━━━ 39s 83ms/step - accuracy: 0.5698 - loss: 1.9390 - val_accuracy: 0.2636 - val_loss: 4.0048 - learning_rate: 3.0000e-04\nEpoch 2/15\n286/286 ━━━━━━━━━━━━━━━━━━━━ 14s 49ms/step - accuracy: 0.7598 - loss: 1.3490 - val_accuracy: 0.7233 - val_loss: 1.3050 - learning_rate: 3.0000e-04\nEpoch 3/15\n286/286 ━━━━━━━━━━━━━━━━━━━━ 14s 50ms/step - accuracy: 0.7905 - loss: 1.0988 - val_accuracy: 0.7592 - val_loss: 1.1686 - learning_rate: 3.0000e-04\nEpoch 4/15\n286/286 ━━━━━━━━━━━━━━━━━━━━ 13s 46ms/step - accuracy: 0.8346 - loss: 0.9062 - val_accuracy: 0.7049 - val_loss: 1.3231 - learning_rate: 3.0000e-04\nEpoch 5/15\n286/286 ━━━━━━━━━━━━━━━━━━━━ 13s 45ms/step - accuracy: 0.8445 - loss: 0.8070 - val_accuracy: 0.7548 - val_loss: 1.0162 - learning_rate: 3.0000e-04\nEpoch 6/15\n286/286 ━━━━━━━━━━━━━━━━━━━━ 13s 46ms/step - accuracy: 0.8637 - loss: 0.7192 - val_accuracy: 0.8187 - val_loss: 0.7903 - learning_rate: 3.0000e-04\nEpoch 7/15\n286/286 ━━━━━━━━━━━━━━━━━━━━ 13s 45ms/step - accuracy: 0.8854 - loss: 0.6435 - val_accuracy: 0.5718 - val_loss: 2.2042 - learning_rate: 3.0000e-04\nEpoch 8/15\n286/286 ━━━━━━━━━━━━━━━━━━━━ 13s 46ms/step - accuracy: 0.8826 - loss: 0.6267 - val_accuracy: 0.8012 - val_loss: 0.9142 - learning_rate: 3.0000e-04\nEpoch 9/15\n286/286 ━━━━━━━━━━━━━━━━━━━━ 13s 47ms/step - accuracy: 0.8949 - loss: 0.5881 - val_accuracy: 0.4256 - val_loss: 3.4703 - learning_rate: 3.0000e-04\nEpoch 10/15\n286/286 ━━━━━━━━━━━━━━━━━━━━ 14s 48ms/step - accuracy: 0.9245 - loss: 0.5160 - val_accuracy: 0.8940 - val_loss: 0.5649 - learning_rate: 1.5000e-04\nEpoch 11/15\n286/286 ━━━━━━━━━━━━━━━━━━━━ 13s 46ms/step - accuracy: 0.9332 - loss: 0.4651 - val_accuracy: 0.8012 - val_loss: 0.8178 - learning_rate: 1.5000e-04\nEpoch 12/15\n286/286 ━━━━━━━━━━━━━━━━━━━━ 13s 46ms/step - accuracy: 0.9268 - loss: 0.4579 - val_accuracy: 0.5359 - val_loss: 2.4819 - learning_rate: 1.5000e-04\nEpoch 13/15\n286/286 ━━━━━━━━━━━━━━━━━━━━ 13s 46ms/step - accuracy: 0.9368 - loss: 0.4439 - val_accuracy: 0.6708 - val_loss: 1.2926 - learning_rate: 1.5000e-04\nEpoch 14/15\n286/286 ━━━━━━━━━━━━━━━━━━━━ 13s 47ms/step - accuracy: 0.9549 - loss: 0.3901 - val_accuracy: 0.9475 - val_loss: 0.3824 - learning_rate: 7.5000e-05\nEpoch 15/15\n286/286 ━━━━━━━━━━━━━━━━━━━━ 13s 46ms/step - accuracy: 0.9522 - loss: 0.3708 - val_accuracy: 0.8310 - val_loss: 0.7138 - learning_rate: 7.5000e-05\n  --> best val_accuracy: 0.9475\n[BEST] Updated best model & HP.\n[STATE] Saved: {'completed_trials': 10, 'best_val_acc': 0.9474605917930603, 'best_hp': {'conv_blocks': 4, 'base_filters': 32, 'kernel_size': 5, 'dropout': 0.4, 'dense_units': 128, 'l2_weight': 0.001, 'optimizer': 'adam', 'lr': 0.0003, 'batch_size': 16, 'use_bn': True, 'augment': False}, 'best_batch': 16}\n\n=== EN İYİ SONUÇ (VAL) ===\nCompleted trials: 10\nBest val_acc: 0.9474605917930603\nBest HP: {'conv_blocks': 4, 'base_filters': 32, 'kernel_size': 5, 'dropout': 0.4, 'dense_units': 128, 'l2_weight': 0.001, 'optimizer': 'adam', 'lr': 0.0003, 'batch_size': 16, 'use_bn': True, 'augment': False}\nTest Acc: 0.9321128726005554 | Test Loss: 0.42730996012687683","metadata":{}},{"cell_type":"markdown","source":"# 8) Proje Nihai Sonuçları\nUzun süren hiperparametre optimizasyonu (Hyperparameter Optimization) sonucunda, Beyin Tümörü Sınıflandırma modelimiz için en yüksek performansı sağlayan konfigürasyon ve başarı metrikleri aşağıdadır.\n\nMetrik Özeti\nModelimiz, belirlenen en iyi parametrelerle eğitildiğinde, test veri setinde oldukça yüksek bir genelleme başarısı göstermiştir.\n\nMetrik Adı\tSonuç\tAçıklama\nTest Verisi Doğruluğu\t%92.14\tModelin daha önce görmediği verideki nihai başarı oranı.\nEn İyi Doğruluk (Validasyon)\t%93.08\tOptimizasyon sırasında kaydedilen en yüksek doğruluk.\nTest Kaybı (Loss)\t0.4514\tModelin tahmin hatalarının ölçüsü.\nDeneme Sayısı\t10\tHyperparametre optimizasyonu için tamamlanan toplam deneme sayısı.\nEn İyi Hiperparametreler\n\nParametre             Değer     Açıklama \n* Katman Sayısı\t       4\t    Kullanılan Evrişim Bloklarının (conv_blocks) toplam sayısı.\n* Temel Filtre Sayısı\t  32\t    Her bir bloktaki başlangıç filtre sayısı (base_filters).\n* Kernel Boyutu\t       5\t    Evrişim katmanlarında kullanılan filtre boyutu.\n* Dropout Oranı\t      0.4\t    Aşırı öğrenmeyi (overfitting) önlemek için kullanılan oran.\n* Yoğun Katman Nöronu\t  128\t    Sınıflandırma katmanından önceki tam bağlantılı (Dense) katmanın nöron sayısı.\n* Optimizer\t          adam\t    Kullanılan optimizasyon algoritması.\n* Öğrenme Oranı (lr)\t0.0003\t    Optimizasyon algoritmasının ağırlıkları güncelleme hızı.\n* Batch Boyutu\t       16\t     Eğitim sırasında aynı anda işlenen görüntü sayısı.\n* Batch Normalizasyon\t  Açık (True)  Eğitim stabilitesini artıran normalizasyon tekniği.\n* Veri Artırımı\t  Kapalı (False)   Veri setinde augmentasyon (çoğaltma) kullanılmamıştır.","metadata":{}}]}